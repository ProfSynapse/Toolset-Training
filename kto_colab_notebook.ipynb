{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/kto_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "Install required packages including unsloth and flash-attention, and import necessary libraries for the KTO finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages - automatic restart on fresh runtime\nimport importlib.metadata\nimport sys\nimport os\n\ndef check_version(package, required_version):\n    \"\"\"Check if package is installed with correct version.\"\"\"\n    try:\n        installed = importlib.metadata.version(package)\n        return installed.startswith(required_version.split('+')[0])\n    except importlib.metadata.PackageNotFoundError:\n        return False\n\n# Check if we're already set up correctly\ntry:\n    import torch\n    import numpy as np\n    packages_ok = (\n        torch.__version__.startswith(\"2.4.1\") and\n        np.__version__.startswith(\"1.26\") and\n        check_version(\"transformers\", \"4.45.2\") and\n        check_version(\"datasets\", \"2.14.0\") and\n        check_version(\"trl\", \"0.11.4\")\n    )\nexcept ImportError:\n    packages_ok = False\n\nif packages_ok:\n    print(\"=\" * 60)\n    print(\"âœ“ All packages already installed correctly!\")\n    print(\"=\" * 60)\n    \n    # Import everything\n    from unsloth import FastLanguageModel, is_bfloat16_supported\n    import os\n    import re\n    from typing import List, Literal, Optional\n    from datasets import load_dataset\n    from trl import KTOConfig, KTOTrainer\n    from transformers import TrainingArguments\n    import transformers\n    import datasets as ds\n    \n    print(f\"âœ“ PyTorch: {torch.__version__}\")\n    print(f\"âœ“ NumPy: {np.__version__}\")\n    print(f\"âœ“ Transformers: {transformers.__version__}\")\n    print(f\"âœ“ TRL: {importlib.metadata.version('trl')}\")\n    print(f\"âœ“ Datasets: {ds.__version__}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"READY TO PROCEED!\")\n    print(\"=\" * 60)\n\nelse:\n    print(\"=\" * 60)\n    print(\"INSTALLING PACKAGES\")\n    print(\"=\" * 60)\n    print(\"Note: Runtime will auto-restart after installation\")\n    print(\"Just re-run this cell after restart - it will skip installation\\n\")\n    \n    # Install PyTorch with numpy constraint\n    print(\"[1/6] Installing PyTorch 2.4.1 + CUDA 12.1 with numpy<2.0...\")\n    !pip install -q torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 \"numpy>=1.24.0,<2.0\" --index-url https://download.pytorch.org/whl/cu121\n    \n    # Install core ML packages\n    print(\"[2/6] Installing core ML libraries...\")\n    !pip install -q transformers==4.45.2 datasets==2.14.0 accelerate==0.27.0 bitsandbytes==0.43.0 peft==0.7.0 trl==0.11.4\n    \n    # Install utilities\n    print(\"[3/6] Installing utilities...\")\n    !pip install -q pandas==2.0.0 tqdm==4.65.0 huggingface-hub==0.20.0\n    \n    # Install unsloth\n    print(\"[4/6] Installing unsloth...\")\n    !pip install -q \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n    \n    # Force numpy back to 1.26.x\n    print(\"[5/6] Ensuring numpy compatibility...\")\n    !pip install -q --force-reinstall \"numpy>=1.24.0,<2.0\"\n    \n    # Install Flash Attention\n    print(\"[6/6] Installing Flash Attention 2 (this may take 2-5 minutes)...\")\n    !pip install -q ninja packaging\n    \n    # Try to import torch to check GPU capability\n    try:\n        import torch\n        device_capability = torch.cuda.get_device_capability()[0]\n        if device_capability >= 8:\n            !pip install -q flash-attn==2.5.9 --no-build-isolation\n    except:\n        print(\"Note: Flash Attention will be installed after restart\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"âœ“ INSTALLATION COMPLETE\")\n    print(\"=\" * 60)\n    print(\"\\nðŸ”„ Auto-restarting runtime in 3 seconds...\")\n    print(\"After restart, just re-run this cell to continue!\\n\")\n    \n    import time\n    time.sleep(3)\n    \n    # Auto-restart the runtime\n    os.kill(os.getpid(), 9)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading and Configuration\n",
    "Load the pre-trained model and tokenizer using FastLanguageModel, and configure basic parameters like sequence length and quantization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading and Configuration\n",
    "\n",
    "# Set basic parameters\n",
    "max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",  # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.3\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token=\"hf_...\",  # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Add proper chat template if missing\n",
    "if tokenizer.chat_template is None:\n",
    "    DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation and Processing\n",
    "Load the combined Claude + Copilot synthetic dataset from Hugging Face Hub. This dataset file (`syngen_tools_11.14.25.jsonl`) contains 4,652 examples combining both Claude (3,214) and Copilot (1,438) tool use conversations. The dataset maintains balanced True/False examples for effective KTO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation and Processing\n",
    "\n",
    "# Load the combined Claude + Copilot dataset from HuggingFace\n",
    "raw_datasets = load_dataset(\n",
    "    \"professorsynapse/claudesidian-synthetic-dataset\",\n",
    "    data_files=\"syngen_tools_11.14.25.jsonl\"\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "# Convert ChatML to KTO format\n",
    "def prepare_kto_format(example):\n",
    "    \"\"\"Convert ChatML format to KTO format.\"\"\"\n",
    "    conversations = example[\"conversations\"]\n",
    "    user_msgs = [msg for msg in conversations if msg[\"role\"] == \"user\"]\n",
    "    assistant_msgs = [msg for msg in conversations if msg[\"role\"] == \"assistant\"]\n",
    "    \n",
    "    if not user_msgs or not assistant_msgs:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": user_msgs[0][\"content\"],\n",
    "        \"completion\": assistant_msgs[0][\"content\"],\n",
    "        \"label\": example[\"label\"]\n",
    "    }\n",
    "\n",
    "# Process dataset\n",
    "processed_dataset = [prepare_kto_format(ex) for ex in train_dataset if prepare_kto_format(ex)]\n",
    "\n",
    "# Verify distribution\n",
    "desirable = sum(1 for ex in processed_dataset if ex[\"label\"])\n",
    "undesirable = len(processed_dataset) - desirable\n",
    "\n",
    "print(f\"Dataset: {len(processed_dataset)} examples ({desirable} desirable, {undesirable} undesirable)\")\n",
    "print(f\"Ratio: {desirable/undesirable:.2f}:1 (desirable:undesirable)\")\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "train_subset = HFDataset.from_dict({\n",
    "    \"prompt\": [ex[\"prompt\"] for ex in processed_dataset],\n",
    "    \"completion\": [ex[\"completion\"] for ex in processed_dataset],\n",
    "    \"label\": [ex[\"label\"] for ex in processed_dataset],\n",
    "})\n",
    "\n",
    "print(f\"Ready for training: {len(train_subset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Setup\n",
    "Configure LoRA adapters and initialize the KTO trainer with optimized hyperparameters for GPT-OSS 20B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Setup - Configure LoRA and KTO Trainer\n",
    "\n",
    "# For Mistral-7B: Use r=64, alpha=128\n",
    "# For GPT-OSS-20B: Use r=128, alpha=256\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,  # LoRA rank - GPT-OSS 20B configuration (48â†’128)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=256,  # LoRA alpha - GPT-OSS 20B configuration (96â†’256)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "# KTO Training Configuration\n",
    "from trl import KTOConfig, KTOTrainer\n",
    "\n",
    "training_args = KTOConfig(\n",
    "    output_dir=\"./kto_output_gpt_oss_20b\",\n",
    "    \n",
    "    # Batch size optimization\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 32\n",
    "    \n",
    "    # KTO-specific parameters\n",
    "    beta=0.05,  # KTO beta parameter for GPT-OSS 20B\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=5.0e-7,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Sequence lengths\n",
    "    max_length=4096,\n",
    "    max_prompt_length=2048,\n",
    "    \n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    \n",
    "    # Training schedule\n",
    "    max_steps=1250,  # 2 epochs for balanced dataset\n",
    "    warmup_steps=125,  # 10% of max_steps (0.10 warmup ratio)\n",
    "    warmup_ratio=0.10,  # Changed from 0.06 to 0.10\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "# Initialize KTO Trainer\n",
    "kto_trainer = KTOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_subset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ“ KTO trainer initialized\")\n",
    "print(f\"Dataset: {len(train_subset)} examples\")\n",
    "print(f\"Max length: 4096 tokens\")\n",
    "print(f\"Batch config: size=4, accumulation=8, effective=32\")\n",
    "print(f\"\\nGPT-OSS 20B Parameters:\")\n",
    "print(f\"  Learning rate: 5.0e-7\")\n",
    "print(f\"  Warmup steps: 125 (warmup_ratio: 0.10)\")\n",
    "print(f\"  Max steps: 1250 (2 epochs)\")\n",
    "print(f\"  LoRA: r=128, alpha=256\")\n",
    "print(f\"  Beta: 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Execution\n",
    "Execute the training process with the configured trainer and monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Execution\n",
    "\n",
    "# Enable CUDA error debugging\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Start training\n",
    "print(\"Starting KTO training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    trainer_output = kto_trainer.train()\n",
    "    print(\"\\nâœ“ Training completed successfully!\")\n",
    "    print(f\"Final loss: {trainer_output.training_loss:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Training failed: {type(e).__name__}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nIf CUDA error persists, check:\")\n",
    "    print(\"  1. Dataset has mixed True/False labels\")\n",
    "    print(\"  2. Batch size is compatible with dataset size\")\n",
    "    print(\"  3. GPU memory is sufficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving to Hugging Face\n",
    "Save your trained model directly to Hugging Face Hub in both standard and GGUF formats. Simply provide your account name, model name, and HuggingFace token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving to Hugging Face\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: Update these values with your HuggingFace details\n",
    "# ============================================================================\n",
    "HF_USERNAME = \"your_username\"  # e.g., \"professorsynapse\"\n",
    "MODEL_NAME = \"your_model_name\"  # e.g., \"claudesidian-gpt-oss-20b-kto\"\n",
    "HF_TOKEN = \"hf_...\"  # Your HuggingFace write token from https://huggingface.co/settings/tokens\n",
    "\n",
    "# ============================================================================\n",
    "# Choose save method (recommended: \"merged_16bit\" for GGUF conversion)\n",
    "# ============================================================================\n",
    "# Options:\n",
    "#   - \"merged_16bit\": Full precision merged model (required for GGUF)\n",
    "#   - \"merged_4bit\": Quantized 4-bit merged model (smaller size)\n",
    "#   - \"lora\": Save only LoRA adapters (smallest size, requires base model to use)\n",
    "SAVE_METHOD = \"merged_16bit\"\n",
    "\n",
    "# GGUF Quantization options (multiple can be True)\n",
    "CREATE_GGUF = True  # Set to True to create GGUF versions\n",
    "GGUF_QUANTIZATIONS = [\"Q4_K_M\", \"Q5_K_M\", \"Q8_0\"]  # Recommended quantization levels\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Upload standard model to HuggingFace\n",
    "# ============================================================================\n",
    "print(f\"Uploading model to: {HF_USERNAME}/{MODEL_NAME}\")\n",
    "print(f\"Save method: {SAVE_METHOD}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    model.push_to_hub_merged(\n",
    "        f\"{HF_USERNAME}/{MODEL_NAME}\",\n",
    "        tokenizer,\n",
    "        save_method=SAVE_METHOD,\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    print(\"\\nâœ“ Model successfully uploaded to Hugging Face!\")\n",
    "    print(f\"\\nView your model at: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Upload failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Verify your HF_TOKEN has write permissions\")\n",
    "    print(\"  2. Check that HF_USERNAME is correct\")\n",
    "    print(\"  3. Ensure the model name is valid (alphanumeric and hyphens only)\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Create and upload GGUF versions (for llama.cpp)\n",
    "# ============================================================================\n",
    "if CREATE_GGUF:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Creating GGUF versions for llama.cpp\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save merged model locally for GGUF conversion\n",
    "    print(\"\\n[1/4] Saving merged model locally...\")\n",
    "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
    "    \n",
    "    # Clone llama.cpp if not already present\n",
    "    print(\"\\n[2/4] Setting up llama.cpp...\")\n",
    "    !git clone https://github.com/ggerganov/llama.cpp 2>/dev/null || echo \"llama.cpp already exists\"\n",
    "    !cd llama.cpp && make -j 2>/dev/null || echo \"llama.cpp already built\"\n",
    "    \n",
    "    # Convert to GGUF base format\n",
    "    print(\"\\n[3/4] Converting to GGUF base format...\")\n",
    "    !python llama.cpp/convert_hf_to_gguf.py merged_model/ --outfile model-unsloth.gguf --outtype f16\n",
    "    \n",
    "    # Create quantized versions\n",
    "    print(\"\\n[4/4] Creating quantized versions...\")\n",
    "    for quant in GGUF_QUANTIZATIONS:\n",
    "        output_file = f\"model-unsloth-{quant}.gguf\"\n",
    "        print(f\"  - Creating {quant} quantization...\")\n",
    "        !./llama.cpp/llama-quantize model-unsloth.gguf {output_file} {quant}\n",
    "    \n",
    "    # Upload GGUF files to HuggingFace\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Uploading GGUF files to Hugging Face...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Upload base GGUF\n",
    "    print(\"\\nUploading base GGUF (f16)...\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"model-unsloth.gguf\",\n",
    "        path_in_repo=\"model-unsloth-f16.gguf\",\n",
    "        repo_id=f\"{HF_USERNAME}/{MODEL_NAME}\",\n",
    "        repo_type=\"model\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    \n",
    "    # Upload quantized versions\n",
    "    for quant in GGUF_QUANTIZATIONS:\n",
    "        output_file = f\"model-unsloth-{quant}.gguf\"\n",
    "        print(f\"Uploading {quant} quantization...\")\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=output_file,\n",
    "            path_in_repo=output_file,\n",
    "            repo_id=f\"{HF_USERNAME}/{MODEL_NAME}\",\n",
    "            repo_type=\"model\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "    \n",
    "    print(\"\\nâœ“ All GGUF files uploaded successfully!\")\n",
    "    print(f\"\\nGGUF files available at: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}/tree/main\")\n",
    "    print(f\"\\nQuantization levels uploaded:\")\n",
    "    print(f\"  - f16 (base, highest quality)\")\n",
    "    for quant in GGUF_QUANTIZATIONS:\n",
    "        print(f\"  - {quant}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    print(\"\\nCleaning up temporary files...\")\n",
    "    !rm -rf merged_model model-unsloth*.gguf\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ All uploads complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\",\n",
    "    mapping = {\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_response(message):\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\nQUESTION:\\n\" + \"=\"*60)\n",
    "    print(message + \"\\n\")\n",
    "    print(\"-\"*60 + \"\\nRESPONSE:\\n\" + \"-\"*60)\n",
    "\n",
    "    messages = [{\"content\": message, \"role\": \"user\"}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    from transformers import TextStreamer\n",
    "    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "    outputs = model.generate(\n",
    "        input_ids = inputs,\n",
    "        streamer = text_streamer,\n",
    "        temperature = 0.1,\n",
    "        max_new_tokens = 1024,\n",
    "        use_cache = True\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "# Test questions - Claudesidian vault operations\n",
    "questions = [\n",
    "    # Test 1: Basic content reading scenario\n",
    "    \"I need to review my meeting notes from yesterday. Can you help me find and read the notes?\",\n",
    "    \n",
    "    # Test 2: Multi-step workflow with workspace context\n",
    "    \"I'm switching to my 'Q4-Planning' workspace. Once switched, create a summary document that lists all my project notes and their status.\",\n",
    "    \n",
    "    # Test 3: Folder operations and organization\n",
    "    \"My notes are getting disorganized. Rename the 'old-drafts' folder to 'archive-2024' and then create a README.md file inside it explaining its purpose.\",\n",
    "    \n",
    "    # Test 4: Search and cross-workspace coordination\n",
    "    \"Search across all my workspaces for notes containing 'roadmap' or 'strategy'. After finding them, create a unified index file that links to all results.\",\n",
    "    \n",
    "    # Test 5: Error handling and recovery\n",
    "    \"I want to create a backup of an important note, but I'm not sure what the exact file path is. Help me find it and then create a backup copy.\",\n",
    "]\n",
    "\n",
    "# Generate responses\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n\\n{'='*60}\\nTEST CASE {i}: Claudesidian Tool Use\\n{'='*60}\")\n",
    "    generate_response(question)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}