{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/kto_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fju0_fZ9M5Is"
   },
   "source": [
    "# Package Installation and Imports\n",
    "Install required packages including unsloth and flash-attention, and import necessary libraries for the KTO finetuning process."
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SESrOQj1M5Is"
   },
   "outputs": [],
   "source": "# Install required packages\n%%capture\n!pip install unsloth\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n\n# Install Flash Attention 2 for softcapping support\nimport torch\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n\n# Import necessary libraries\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nimport torch\nimport os\nimport re\nfrom typing import List, Literal, Optional\nfrom datasets import load_dataset\nfrom trl import KTOConfig, KTOTrainer\nfrom transformers import TrainingArguments",
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWsQtPsoM5It"
   },
   "source": [
    "# Model Loading and Configuration\n",
    "Load the pre-trained model and tokenizer using FastLanguageModel, and configure basic parameters like sequence length and quantization settings."
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKeX_vuYM5It",
    "outputId": "eceaed54-f97a-42aa-e454-f121fc5f0122"
   },
   "outputs": [],
   "source": "# Model Loading and Configuration\n\n# Set basic parameters\nmax_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n\n\n# Load the pre-trained model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",  # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    # token=\"hf_...\",  # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\n# Add proper chat template if missing\nif tokenizer.chat_template is None:\n    DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE",
   "id": "cell-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfOvZ0XXM5It"
   },
   "source": "# Dataset Preparation and Processing\nLoad the interleaved balanced Claudesidian synthetic dataset from Hugging Face Hub. This dataset file (`syngen_toolset_v1.0.0_claude_balanced_interleaved.jsonl`) contains 508 examples in a strict True/False/True/False alternating pattern (254 desirable : 254 undesirable). The interleaved structure guarantees that sequential batches will always contain mixed labels, preventing the TRL KTO trainer bug that occurs with homogeneous batches.",
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRAwRajtM5It"
   },
   "outputs": [],
   "source": "# Dataset Preparation and Processing\n\n# Load the INTERLEAVED balanced dataset from HuggingFace\nraw_datasets = load_dataset(\n    \"professorsynapse/claudesidian-synthetic-dataset\",\n    data_files=\"syngen_toolset_v1.0.0_claude_balanced_interleaved.jsonl\"\n)\ntrain_dataset = raw_datasets[\"train\"]\n\n# Convert ChatML to KTO format\ndef prepare_kto_format(example):\n    \"\"\"Convert ChatML format to KTO format.\"\"\"\n    conversations = example[\"conversations\"]\n    user_msgs = [msg for msg in conversations if msg[\"role\"] == \"user\"]\n    assistant_msgs = [msg for msg in conversations if msg[\"role\"] == \"assistant\"]\n    \n    if not user_msgs or not assistant_msgs:\n        return None\n    \n    return {\n        \"prompt\": user_msgs[0][\"content\"],\n        \"completion\": assistant_msgs[0][\"content\"],\n        \"label\": example[\"label\"]\n    }\n\n# Process dataset\nprocessed_dataset = [prepare_kto_format(ex) for ex in train_dataset if prepare_kto_format(ex)]\n\n# Verify distribution\ndesirable = sum(1 for ex in processed_dataset if ex[\"label\"])\nundesirable = len(processed_dataset) - desirable\n\nprint(f\"Dataset: {len(processed_dataset)} examples ({desirable} desirable, {undesirable} undesirable)\")\n\n# Verify interleaving (first 10 should alternate)\nfirst_10 = [ex[\"label\"] for ex in processed_dataset[:10]]\nprint(f\"First 10 labels: {first_10}\")\nprint(f\"Interleaved: {'✓' if all(first_10[i] != first_10[i+1] for i in range(9)) else '✗'}\")\n\n# Create HuggingFace dataset\nfrom datasets import Dataset as HFDataset\ntrain_subset = HFDataset.from_dict({\n    \"prompt\": [ex[\"prompt\"] for ex in processed_dataset],\n    \"completion\": [ex[\"completion\"] for ex in processed_dataset],\n    \"label\": [ex[\"label\"] for ex in processed_dataset],\n})\n\nprint(f\"Ready for training: {len(train_subset)} examples\")",
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcAEPbyjM5It"
   },
   "source": "# Model Training Setup\nConfigure the LoRA adapters and set up the KTO trainer with appropriate training arguments. The trainer uses a balanced subset of the Claudesidian dataset (254 desirable / 254 undesirable = 1:1 ratio) to test KTO training and avoid batches with homogeneous labels that cause CUDA indexing errors in TRL's KTO implementation.",
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubLaM74dM5It"
   },
   "outputs": [],
   "source": "print(\"✓ KTO trainer initialized\")\nprint(f\"Dataset: {len(train_subset)} examples (balanced)\")\nprint(f\"Max length: 4096 tokens\")\nprint(f\"Batch config: size=4, accumulation=8, effective=32\")\nprint(f\"\\nGPT-OSS 20B Parameters:\")\nprint(f\"  Learning rate: 5.0e-7\")\nprint(f\"  Warmup steps: 150 (warmup_ratio: 0.06)\")\nprint(f\"  Max steps: 1250 (2 epochs)\")\nprint(f\"  LoRA: r=48, alpha=96\")\nprint(f\"  Beta: 0.05\")",
   "id": "cell-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IMWJLEdM5Iu"
   },
   "source": [
    "# Training Execution\n",
    "Execute the training process with the configured trainer and monitor the training progress."
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "k76j5NP_M5Iu",
    "outputId": "24ed86e7-4a9b-4d99-c828-71c672afb1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "1.709 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "t_IDtt91ZUli",
    "outputId": "362d55f2-72ba-4965-93cd-6259dd12bf8e"
   },
   "outputs": [],
   "source": "# Training Execution\n\n# Enable CUDA error debugging\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Start training\nprint(\"Starting KTO training...\")\nprint(\"=\"*50)\n\ntry:\n    trainer_output = kto_trainer.train()\n    print(\"\\n✓ Training completed successfully!\")\n    print(f\"Final loss: {trainer_output.training_loss:.4f}\")\nexcept Exception as e:\n    print(f\"\\n✗ Training failed: {type(e).__name__}\")\n    print(f\"Error: {e}\")\n    print(\"\\nIf CUDA error persists, check:\")\n    print(\"  1. Dataset has alternating True/False labels\")\n    print(\"  2. Batch size is compatible with dataset size\")\n    print(\"  3. GPU memory is sufficient\")",
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obai83YGM5Iu"
   },
   "source": [
    "# Model Saving and Export\n",
    "Save the trained model in different formats including LoRA adapters and merged model."
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaA0FZkKM5Iu"
   },
   "outputs": [],
   "source": [
    "# Model Saving and Export\n",
    "\n",
    "# Local saving\n",
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# Save merged model as float16 or int4\n",
    "if False: # Set to True to save\n",
    "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "    # model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_4bit\")\n",
    "    # model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"lora\")\n",
    "\n",
    "# Save to HuggingFace Hub\n",
    "if False: # Set to True to save\n",
    "    model.push_to_hub_merged(\"your_name/model\", tokenizer, save_method = \"merged_16bit\", token = \"...\")\n",
    "    # save_method can be \"merged_16bit\", \"merged_4bit\", or \"lora\"\n",
    "\n",
    "# Save to GGUF format (for llama.cpp)\n",
    "if False: # Set to True to save\n",
    "    from transformers import AutoTokenizer\n",
    "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "    !git clone https://github.com/ggerganov/llama.cpp\n",
    "    !cd llama.cpp && make\n",
    "    !python3 llama.cpp/convert.py merged_model/ --outfile model-unsloth.gguf\n",
    "    # Also supports quantization\n",
    "    !./llama.cpp/quantize model-unsloth.gguf model-unsloth-Q4_K_M.gguf Q4_K_M"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8QF4Bt9xSJ6"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMagn9dzWqgi"
   },
   "outputs": [],
   "source": "from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"chatml\",\n    mapping = {\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n)\n\nFastLanguageModel.for_inference(model)\n\ndef generate_response(message):\n    print(\"\\n\" + \"=\"*60 + \"\\nQUESTION:\\n\" + \"=\"*60)\n    print(message + \"\\n\")\n    print(\"-\"*60 + \"\\nRESPONSE:\\n\" + \"-\"*60)\n\n    messages = [{\"content\": message, \"role\": \"user\"}]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True,\n        return_tensors = \"pt\"\n    ).to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n    outputs = model.generate(\n        input_ids = inputs,\n        streamer = text_streamer,\n        temperature = 0.1,\n        max_new_tokens = 1024,\n        use_cache = True\n    )\n    return outputs\n\n# Test questions - Claudesidian vault operations\nquestions = [\n    # Test 1: Basic content reading scenario\n    \"I need to review my meeting notes from yesterday. Can you help me find and read the notes?\",\n    \n    # Test 2: Multi-step workflow with workspace context\n    \"I'm switching to my 'Q4-Planning' workspace. Once switched, create a summary document that lists all my project notes and their status.\",\n    \n    # Test 3: Folder operations and organization\n    \"My notes are getting disorganized. Rename the 'old-drafts' folder to 'archive-2024' and then create a README.md file inside it explaining its purpose.\",\n    \n    # Test 4: Search and cross-workspace coordination\n    \"Search across all my workspaces for notes containing 'roadmap' or 'strategy'. After finding them, create a unified index file that links to all results.\",\n    \n    # Test 5: Error handling and recovery\n    \"I want to create a backup of an important note, but I'm not sure what the exact file path is. Help me find it and then create a backup copy.\",\n]\n\n# Generate responses\nfor i, question in enumerate(questions, 1):\n    print(f\"\\n\\n{'='*60}\\nTEST CASE {i}: Claudesidian Tool Use\\n{'='*60}\")\n    generate_response(question)",
   "id": "cell-15"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}