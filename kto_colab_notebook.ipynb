{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/kto_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fju0_fZ9M5Is"
   },
   "source": [
    "# Package Installation and Imports\n",
    "Install required packages including unsloth and flash-attention, and import necessary libraries for the KTO finetuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SESrOQj1M5Is"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "# Install Flash Attention 2 for softcapping support\n",
    "import torch\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n",
    "\n",
    "# Import necessary libraries\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from typing import List, Literal, Optional\n",
    "from datasets import load_dataset\n",
    "from trl import KTOConfig, KTOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWsQtPsoM5It"
   },
   "source": [
    "# Model Loading and Configuration\n",
    "Load the pre-trained model and tokenizer using FastLanguageModel, and configure basic parameters like sequence length and quantization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKeX_vuYM5It",
    "outputId": "eceaed54-f97a-42aa-e454-f121fc5f0122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.1: Fast Qwen2 patching. Transformers:4.46.2.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Model Loading and Configuration\n",
    "\n",
    "# Set basic parameters\n",
    "max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",  # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token=\"hf_...\",  # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Add proper chat template if missing\n",
    "if tokenizer.chat_template is None:\n",
    "    DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfOvZ0XXM5It"
   },
   "source": "# Dataset Preparation and Processing\nLoad and prepare the Claudesidian synthetic dataset from Hugging Face Hub. The dataset contains 1,000 ChatML-formatted examples with boolean labels (true=desirable, false=undesirable). These are converted to KTO's chosen/rejected format and paired for contrastive learning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRAwRajtM5It"
   },
   "outputs": [],
   "source": "# Dataset Preparation and Processing\n\n# Function to apply chat template\ndef apply_chat_template(\n    example, tokenizer, task: Literal[\"sft\", \"generation\", \"rm\", \"kto\"] = \"sft\", assistant_prefix=\"<|assistant|>\\n\"\n):\n    def _strip_prefix(s, pattern):\n        # Use re.escape to escape any special characters in the pattern\n        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n\n    if task in [\"sft\", \"generation\"]:\n        messages = example[\"messages\"]\n        # We add an empty system message if there is none\n        if messages[0][\"role\"] != \"system\":\n            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n        example[\"text\"] = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False\n        )\n    elif task == \"rm\":\n        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n            chosen_messages = example[\"chosen\"]\n            rejected_messages = example[\"rejected\"]\n            # We add an empty system message if there is none\n            if chosen_messages[0][\"role\"] != \"system\":\n                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n            if rejected_messages[0][\"role\"] != \"system\":\n                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n        else:\n            raise ValueError(\n                f\"Could not format example as dialogue for `rm` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n            )\n    elif task == \"dpo\":\n        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n            # Compared to reward modeling, we filter out the prompt, so the text is everything after the last assistant token\n            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n            # Insert system message\n            if example[\"chosen\"][0][\"role\"] != \"system\":\n                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n            else:\n                prompt_messages.insert(0, example[\"chosen\"][0])\n            # TODO: handle case where chosen/rejected also have system messages\n            chosen_messages = example[\"chosen\"][1:]\n            rejected_messages = example[\"rejected\"][1:]\n            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n                prompt_messages, tokenize=False, add_generation_prompt=True\n            )\n            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n        else:\n            raise ValueError(\n                f\"Could not format example as dialogue for `dpo` task! Require `[chosen, rejected]` keys but found {list(example.keys())}\"\n            )\n    elif task == \"kto\":\n        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n            chosen_messages = prompt_messages + [msg for msg in example[\"chosen\"] if msg[\"role\"] == \"assistant\"]\n            rejected_messages = prompt_messages + [msg for msg in example[\"rejected\"] if msg[\"role\"] == \"assistant\"]\n            if \"system\" in example:\n                chosen_messages.insert(0, {\"role\": \"system\", \"content\": example[\"system\"]})\n                rejected_messages.insert(0, {\"role\": \"system\", \"content\": example[\"system\"]})\n            example[\"text_chosen\"] = _strip_prefix(tokenizer.apply_chat_template(chosen_messages, tokenize=False), assistant_prefix)\n            example[\"text_rejected\"] = _strip_prefix(tokenizer.apply_chat_template(rejected_messages, tokenize=False), assistant_prefix)\n        else:\n            raise ValueError(f\"Could not format example as dialogue for `kto` task!\")\n    else:\n        raise ValueError(\n            f\"Task {task} not supported, please ensure that the provided task is one of {['sft', 'generation', 'rm', 'dpo', 'kto']}\"\n        )\n    return example\n\n# Load the Claudesidian synthetic dataset from Hugging Face\nraw_datasets = load_dataset(\n    \"professorsynapse/claudesidian-synthetic-dataset\",\n    data_files=\"syngen_toolset_v1.0.0_claude.jsonl\"\n)\ntrain_dataset = raw_datasets[\"train\"]\n\n# Convert the dataset format from conversations/label to chosen/rejected for KTO training\ndef convert_to_kto_format(example):\n    \"\"\"\n    Convert from ChatML conversations format with boolean label to KTO's chosen/rejected format.\n    Extract the prompt (user message) to provide explicit structure for KTO trainer.\n    \n    Input format:\n    {\n        \"conversations\": [\n            {\"role\": \"user\", \"content\": \"...\"},\n            {\"role\": \"assistant\", \"content\": \"...\"}\n        ],\n        \"label\": true/false\n    }\n    \n    Output format for KTO:\n    {\n        \"prompt\": [{\"role\": \"user\", \"content\": \"...\"}],  # Just the user message\n        \"chosen\": [messages],   # Full conversation with this response\n        \"rejected\": [messages]  # Full conversation with alternative response\n    }\n    \"\"\"\n    conversations = example[\"conversations\"]\n    label = example[\"label\"]\n    \n    # Extract the prompt (user message) - it's always the first message\n    prompt = [msg for msg in conversations if msg[\"role\"] == \"user\"]\n    \n    # label=true means this is a desirable example, so it goes to \"chosen\"\n    # label=false means this is an undesirable example, so it goes to \"rejected\"\n    if label:\n        return {\n            \"prompt\": prompt,\n            \"chosen\": conversations,\n            \"rejected\": None  # Will be paired later\n        }\n    else:\n        return {\n            \"prompt\": prompt,\n            \"chosen\": None,  # Will be paired later\n            \"rejected\": conversations\n        }\n\n# Apply conversion to all examples\nconverted_dataset = train_dataset.map(convert_to_kto_format)\n\n# Pair desirable and undesirable examples for KTO training\n# KTO requires pairs of chosen/rejected examples\ndesirable_examples = converted_dataset.filter(lambda x: x[\"chosen\"] is not None)\nundesirable_examples = converted_dataset.filter(lambda x: x[\"rejected\"] is not None)\n\n# Create pairs by zipping desirable with undesirable\ndef create_paired_examples(desirable_list, undesirable_list):\n    \"\"\"Create paired examples for KTO training with matching prompts.\"\"\"\n    pairs = []\n    # Repeat undesirable examples if fewer than desirable (common case: more desirable than undesirable)\n    max_len = max(len(desirable_list), len(undesirable_list))\n    \n    for i in range(max_len):\n        desirable = desirable_list[i % len(desirable_list)]\n        undesirable = undesirable_list[i % len(undesirable_list)]\n        \n        # Use the prompt from the desirable example (they should be semantically similar)\n        # Both chosen and rejected should have the same prompt\n        pairs.append({\n            \"prompt\": desirable[\"prompt\"],\n            \"chosen\": desirable[\"chosen\"],\n            \"rejected\": undesirable[\"rejected\"]\n        })\n    \n    return pairs\n\n# Get the paired dataset\ndesirable_list = [example for example in desirable_examples]\nundesirable_list = [example for example in undesirable_examples]\npaired_examples = create_paired_examples(desirable_list, undesirable_list)\n\n# Convert to HuggingFace dataset\nfrom datasets import Dataset as HFDataset\ntrain_subset = HFDataset.from_dict({\n    \"prompt\": [ex[\"prompt\"] for ex in paired_examples],\n    \"chosen\": [ex[\"chosen\"] for ex in paired_examples],\n    \"rejected\": [ex[\"rejected\"] for ex in paired_examples]\n})"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcAEPbyjM5It"
   },
   "source": "# Model Training Setup\nConfigure the LoRA adapters and set up the KTO trainer with appropriate training arguments. The trainer uses the paired Claudesidian dataset (742 desirable/254 undesirable examples) for contrastive learning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubLaM74dM5It"
   },
   "outputs": [],
   "source": "# Model Training Setup\n\n# Configure the LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\n\n# Set up the KTO trainer with appropriate training arguments\n# Note: We're using the full paired dataset from the Claudesidian synthetic dataset\n# Dataset format: {prompt: [user_msg], chosen: [conversation], rejected: [conversation]}\nkto_trainer = KTOTrainer(\n    model=model,\n    args=KTOConfig(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=2,\n        num_train_epochs=1,\n        learning_rate=5e-7,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        output_dir=\"outputs\",\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.1,\n        seed=42,\n        report_to=\"none\",  # Use this for WandB etc\n        remove_unused_columns=False,  # Keep all columns since we use them\n    ),\n    train_dataset=train_subset,\n    processing_class=tokenizer,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IMWJLEdM5Iu"
   },
   "source": [
    "# Training Execution\n",
    "Execute the training process with the configured trainer and monitor the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "k76j5NP_M5Iu",
    "outputId": "24ed86e7-4a9b-4d99-c828-71c672afb1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "1.709 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "t_IDtt91ZUli",
    "outputId": "362d55f2-72ba-4965-93cd-6259dd12bf8e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 58/125 04:52 < 05:49, 0.19 it/s, Epoch 0.46/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.499600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.502200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.499700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.499200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kto_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Obai83YGM5Iu"
   },
   "source": [
    "# Model Saving and Export\n",
    "Save the trained model in different formats including LoRA adapters and merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaA0FZkKM5Iu"
   },
   "outputs": [],
   "source": [
    "# Model Saving and Export\n",
    "\n",
    "# Local saving\n",
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "\n",
    "# Save merged model as float16 or int4\n",
    "if False: # Set to True to save\n",
    "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "    # model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_4bit\")\n",
    "    # model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"lora\")\n",
    "\n",
    "# Save to HuggingFace Hub\n",
    "if False: # Set to True to save\n",
    "    model.push_to_hub_merged(\"your_name/model\", tokenizer, save_method = \"merged_16bit\", token = \"...\")\n",
    "    # save_method can be \"merged_16bit\", \"merged_4bit\", or \"lora\"\n",
    "\n",
    "# Save to GGUF format (for llama.cpp)\n",
    "if False: # Set to True to save\n",
    "    from transformers import AutoTokenizer\n",
    "    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\")\n",
    "    !git clone https://github.com/ggerganov/llama.cpp\n",
    "    !cd llama.cpp && make\n",
    "    !python3 llama.cpp/convert.py merged_model/ --outfile model-unsloth.gguf\n",
    "    # Also supports quantization\n",
    "    !./llama.cpp/quantize model-unsloth.gguf model-unsloth-Q4_K_M.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8QF4Bt9xSJ6"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMagn9dzWqgi"
   },
   "outputs": [],
   "source": "from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"chatml\",\n    mapping = {\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n)\n\nFastLanguageModel.for_inference(model)\n\ndef generate_response(message):\n    print(\"\\n\" + \"=\"*60 + \"\\nQUESTION:\\n\" + \"=\"*60)\n    print(message + \"\\n\")\n    print(\"-\"*60 + \"\\nRESPONSE:\\n\" + \"-\"*60)\n\n    messages = [{\"content\": message, \"role\": \"user\"}]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True,\n        return_tensors = \"pt\"\n    ).to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n    outputs = model.generate(\n        input_ids = inputs,\n        streamer = text_streamer,\n        temperature = 0.1,\n        max_new_tokens = 1024,\n        use_cache = True\n    )\n    return outputs\n\n# Test questions - Claudesidian vault operations\nquestions = [\n    # Test 1: Basic content reading scenario\n    \"I need to review my meeting notes from yesterday. Can you help me find and read the notes?\",\n    \n    # Test 2: Multi-step workflow with workspace context\n    \"I'm switching to my 'Q4-Planning' workspace. Once switched, create a summary document that lists all my project notes and their status.\",\n    \n    # Test 3: Folder operations and organization\n    \"My notes are getting disorganized. Rename the 'old-drafts' folder to 'archive-2024' and then create a README.md file inside it explaining its purpose.\",\n    \n    # Test 4: Search and cross-workspace coordination\n    \"Search across all my workspaces for notes containing 'roadmap' or 'strategy'. After finding them, create a unified index file that links to all results.\",\n    \n    # Test 5: Error handling and recovery\n    \"I want to create a backup of an important note, but I'm not sure what the exact file path is. Help me find it and then create a backup copy.\",\n]\n\n# Generate responses\nfor i, question in enumerate(questions, 1):\n    print(f\"\\n\\n{'='*60}\\nTEST CASE {i}: Claudesidian Tool Use\\n{'='*60}\")\n    generate_response(question)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}