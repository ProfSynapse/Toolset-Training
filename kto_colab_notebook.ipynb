{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/kto_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\nInstall required packages including unsloth and flash-attention, and import necessary libraries for the KTO finetuning process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages\n%%capture\n!pip install unsloth\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n\n# Install Flash Attention 2 for softcapping support\nimport torch\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n\n# Import necessary libraries\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nimport torch\nimport os\nimport re\nfrom typing import List, Literal, Optional\nfrom datasets import load_dataset\nfrom trl import KTOConfig, KTOTrainer\nfrom transformers import TrainingArguments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Loading and Configuration\nLoad the pre-trained model and tokenizer using FastLanguageModel, and configure basic parameters like sequence length and quantization settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model Loading and Configuration\n\n# Set basic parameters\nmax_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n\n\n# Load the pre-trained model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gpt-oss-20b-unsloth-bnb-4bit\",  # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    # token=\"hf_...\",  # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\n\n# Add proper chat template if missing\nif tokenizer.chat_template is None:\n    DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Preparation and Processing\nLoad the combined Claude + Copilot synthetic dataset from Hugging Face Hub. This dataset file (`syngen_tools_11.14.25.jsonl`) contains 4,652 examples combining both Claude (3,214) and Copilot (1,438) tool use conversations. The dataset maintains balanced True/False examples for effective KTO training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dataset Preparation and Processing\n\n# Load the combined Claude + Copilot dataset from HuggingFace\nraw_datasets = load_dataset(\n    \"professorsynapse/claudesidian-synthetic-dataset\",\n    data_files=\"syngen_tools_11.14.25.jsonl\"\n)\ntrain_dataset = raw_datasets[\"train\"]\n\n# Convert ChatML to KTO format\ndef prepare_kto_format(example):\n    \"\"\"Convert ChatML format to KTO format.\"\"\"\n    conversations = example[\"conversations\"]\n    user_msgs = [msg for msg in conversations if msg[\"role\"] == \"user\"]\n    assistant_msgs = [msg for msg in conversations if msg[\"role\"] == \"assistant\"]\n    \n    if not user_msgs or not assistant_msgs:\n        return None\n    \n    return {\n        \"prompt\": user_msgs[0][\"content\"],\n        \"completion\": assistant_msgs[0][\"content\"],\n        \"label\": example[\"label\"]\n    }\n\n# Process dataset\nprocessed_dataset = [prepare_kto_format(ex) for ex in train_dataset if prepare_kto_format(ex)]\n\n# Verify distribution\ndesirable = sum(1 for ex in processed_dataset if ex[\"label\"])\nundesirable = len(processed_dataset) - desirable\n\nprint(f\"Dataset: {len(processed_dataset)} examples ({desirable} desirable, {undesirable} undesirable)\")\nprint(f\"Ratio: {desirable/undesirable:.2f}:1 (desirable:undesirable)\")\n\n# Create HuggingFace dataset\nfrom datasets import Dataset as HFDataset\ntrain_subset = HFDataset.from_dict({\n    \"prompt\": [ex[\"prompt\"] for ex in processed_dataset],\n    \"completion\": [ex[\"completion\"] for ex in processed_dataset],\n    \"label\": [ex[\"label\"] for ex in processed_dataset],\n})\n\nprint(f\"Ready for training: {len(train_subset)} examples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training Setup\nConfigure LoRA adapters and initialize the KTO trainer with optimized hyperparameters for GPT-OSS 20B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model Training Setup - Configure LoRA and KTO Trainer\n\n# For Mistral-7B: Use r=64, alpha=128\n# For GPT-OSS-20B: Use r=128, alpha=256\n\n# Apply LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=128,  # LoRA rank - GPT-OSS 20B configuration (48\u2192128)\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=256,  # LoRA alpha - GPT-OSS 20B configuration (96\u2192256)\n    lora_dropout=0.05,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\n\n# KTO Training Configuration\nfrom trl import KTOConfig, KTOTrainer\n\ntraining_args = KTOConfig(\n    output_dir=\"./kto_output_gpt_oss_20b\",\n    \n    # Batch size optimization\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,  # Effective batch size = 32\n    \n    # KTO-specific parameters\n    beta=0.05,  # KTO beta parameter for GPT-OSS 20B\n    desirable_weight=1.0,\n    undesirable_weight=1.0,\n    \n    # Learning rate\n    learning_rate=5.0e-7,\n    max_grad_norm=1.0,\n    \n    # Sequence lengths\n    max_length=4096,\n    max_prompt_length=2048,\n    \n    # Memory optimizations\n    gradient_checkpointing=True,\n    optim=\"adamw_8bit\",\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    \n    # Training schedule\n    max_steps=1250,  # 2 epochs for balanced dataset\n    warmup_steps=125,  # 10% of max_steps (0.10 warmup ratio)\n    warmup_ratio=0.10,  # Changed from 0.06 to 0.10\n    \n    # Logging and saving\n    logging_steps=10,\n    save_steps=250,\n    save_total_limit=2,\n    \n    # Performance\n    dataloader_num_workers=2,\n    group_by_length=False,\n)\n\n# Initialize KTO Trainer\nkto_trainer = KTOTrainer(\n    model=model,\n    args=training_args,\n    processing_class=tokenizer,\n    train_dataset=train_subset,\n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\u2713 KTO trainer initialized\")\nprint(f\"Dataset: {len(train_subset)} examples\")\nprint(f\"Max length: 4096 tokens\")\nprint(f\"Batch config: size=4, accumulation=8, effective=32\")\nprint(f\"\\nGPT-OSS 20B Parameters:\")\nprint(f\"  Learning rate: 5.0e-7\")\nprint(f\"  Warmup steps: 125 (warmup_ratio: 0.10)\")\nprint(f\"  Max steps: 1250 (2 epochs)\")\nprint(f\"  LoRA: r=128, alpha=256\")\nprint(f\"  Beta: 0.05\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Execution\nExecute the training process with the configured trainer and monitor the training progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training Execution\n\n# Enable CUDA error debugging\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n# Start training\nprint(\"Starting KTO training...\")\nprint(\"=\"*50)\n\ntry:\n    trainer_output = kto_trainer.train()\n    print(\"\\n\u2713 Training completed successfully!\")\n    print(f\"Final loss: {trainer_output.training_loss:.4f}\")\nexcept Exception as e:\n    print(f\"\\n\u2717 Training failed: {type(e).__name__}\")\n    print(f\"Error: {e}\")\n    print(\"\\nIf CUDA error persists, check:\")\n    print(\"  1. Dataset has mixed True/False labels\")\n    print(\"  2. Batch size is compatible with dataset size\")\n    print(\"  3. GPU memory is sufficient\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Saving to Hugging Face\nSave your trained model directly to Hugging Face Hub in both standard and GGUF formats. Simply provide your account name, model name, and HuggingFace token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Model Saving to Hugging Face\n\n# ============================================================================\n# CONFIGURATION: Update these values with your HuggingFace details\n# ============================================================================\nHF_USERNAME = \"your_username\"  # e.g., \"professorsynapse\"\nMODEL_NAME = \"your_model_name\"  # e.g., \"claudesidian-gpt-oss-20b-kto\"\nHF_TOKEN = \"hf_...\"  # Your HuggingFace write token from https://huggingface.co/settings/tokens\n\n# ============================================================================\n# Choose save method (recommended: \"merged_16bit\" for GGUF conversion)\n# ============================================================================\n# Options:\n#   - \"merged_16bit\": Full precision merged model (required for GGUF)\n#   - \"merged_4bit\": Quantized 4-bit merged model (smaller size)\n#   - \"lora\": Save only LoRA adapters (smallest size, requires base model to use)\nSAVE_METHOD = \"merged_16bit\"\n\n# GGUF Quantization options (multiple can be True)\nCREATE_GGUF = True  # Set to True to create GGUF versions\nGGUF_QUANTIZATIONS = [\"Q4_K_M\", \"Q5_K_M\", \"Q8_0\"]  # Recommended quantization levels\n\n# ============================================================================\n# Step 1: Upload standard model to HuggingFace\n# ============================================================================\nprint(f\"Uploading model to: {HF_USERNAME}/{MODEL_NAME}\")\nprint(f\"Save method: {SAVE_METHOD}\")\nprint(\"=\" * 60)\n\ntry:\n    model.push_to_hub_merged(\n        f\"{HF_USERNAME}/{MODEL_NAME}\",\n        tokenizer,\n        save_method=SAVE_METHOD,\n        token=HF_TOKEN\n    )\n    print(\"\\n\u2713 Model successfully uploaded to Hugging Face!\")\n    print(f\"\\nView your model at: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}\")\nexcept Exception as e:\n    print(f\"\\n\u2717 Upload failed: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"  1. Verify your HF_TOKEN has write permissions\")\n    print(\"  2. Check that HF_USERNAME is correct\")\n    print(\"  3. Ensure the model name is valid (alphanumeric and hyphens only)\")\n    raise\n\n# ============================================================================\n# Step 2: Create and upload GGUF versions (for llama.cpp)\n# ============================================================================\nif CREATE_GGUF:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Creating GGUF versions for llama.cpp\")\n    print(\"=\" * 60)\n    \n    # Save merged model locally for GGUF conversion\n    print(\"\\n[1/4] Saving merged model locally...\")\n    model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n    \n    # Clone llama.cpp if not already present\n    print(\"\\n[2/4] Setting up llama.cpp...\")\n    !git clone https://github.com/ggerganov/llama.cpp 2>/dev/null || echo \"llama.cpp already exists\"\n    !cd llama.cpp && make -j 2>/dev/null || echo \"llama.cpp already built\"\n    \n    # Convert to GGUF base format\n    print(\"\\n[3/4] Converting to GGUF base format...\")\n    !python llama.cpp/convert_hf_to_gguf.py merged_model/ --outfile model-unsloth.gguf --outtype f16\n    \n    # Create quantized versions\n    print(\"\\n[4/4] Creating quantized versions...\")\n    for quant in GGUF_QUANTIZATIONS:\n        output_file = f\"model-unsloth-{quant}.gguf\"\n        print(f\"  - Creating {quant} quantization...\")\n        !./llama.cpp/llama-quantize model-unsloth.gguf {output_file} {quant}\n    \n    # Upload GGUF files to HuggingFace\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Uploading GGUF files to Hugging Face...\")\n    print(\"=\" * 60)\n    \n    from huggingface_hub import HfApi\n    api = HfApi()\n    \n    # Upload base GGUF\n    print(\"\\nUploading base GGUF (f16)...\")\n    api.upload_file(\n        path_or_fileobj=\"model-unsloth.gguf\",\n        path_in_repo=\"model-unsloth-f16.gguf\",\n        repo_id=f\"{HF_USERNAME}/{MODEL_NAME}\",\n        repo_type=\"model\",\n        token=HF_TOKEN\n    )\n    \n    # Upload quantized versions\n    for quant in GGUF_QUANTIZATIONS:\n        output_file = f\"model-unsloth-{quant}.gguf\"\n        print(f\"Uploading {quant} quantization...\")\n        api.upload_file(\n            path_or_fileobj=output_file,\n            path_in_repo=output_file,\n            repo_id=f\"{HF_USERNAME}/{MODEL_NAME}\",\n            repo_type=\"model\",\n            token=HF_TOKEN\n        )\n    \n    print(\"\\n\u2713 All GGUF files uploaded successfully!\")\n    print(f\"\\nGGUF files available at: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}/tree/main\")\n    print(f\"\\nQuantization levels uploaded:\")\n    print(f\"  - f16 (base, highest quality)\")\n    for quant in GGUF_QUANTIZATIONS:\n        print(f\"  - {quant}\")\n    \n    # Cleanup\n    print(\"\\nCleaning up temporary files...\")\n    !rm -rf merged_model model-unsloth*.gguf\n    \nprint(\"\\n\" + \"=\" * 60)\nprint(\"\u2713 All uploads complete!\")\nprint(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"chatml\",\n    mapping = {\"role\": \"role\", \"content\": \"content\", \"user\": \"user\", \"assistant\": \"assistant\"},\n)\n\nFastLanguageModel.for_inference(model)\n\ndef generate_response(message):\n    print(\"\\n\" + \"=\"*60 + \"\\nQUESTION:\\n\" + \"=\"*60)\n    print(message + \"\\n\")\n    print(\"-\"*60 + \"\\nRESPONSE:\\n\" + \"-\"*60)\n\n    messages = [{\"content\": message, \"role\": \"user\"}]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize = True,\n        add_generation_prompt = True,\n        return_tensors = \"pt\"\n    ).to(\"cuda\")\n\n    from transformers import TextStreamer\n    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n    outputs = model.generate(\n        input_ids = inputs,\n        streamer = text_streamer,\n        temperature = 0.1,\n        max_new_tokens = 1024,\n        use_cache = True\n    )\n    return outputs\n\n# Test questions - Claudesidian vault operations\nquestions = [\n    # Test 1: Basic content reading scenario\n    \"I need to review my meeting notes from yesterday. Can you help me find and read the notes?\",\n    \n    # Test 2: Multi-step workflow with workspace context\n    \"I'm switching to my 'Q4-Planning' workspace. Once switched, create a summary document that lists all my project notes and their status.\",\n    \n    # Test 3: Folder operations and organization\n    \"My notes are getting disorganized. Rename the 'old-drafts' folder to 'archive-2024' and then create a README.md file inside it explaining its purpose.\",\n    \n    # Test 4: Search and cross-workspace coordination\n    \"Search across all my workspaces for notes containing 'roadmap' or 'strategy'. After finding them, create a unified index file that links to all results.\",\n    \n    # Test 5: Error handling and recovery\n    \"I want to create a backup of an important note, but I'm not sure what the exact file path is. Help me find it and then create a backup copy.\",\n]\n\n# Generate responses\nfor i, question in enumerate(questions, 1):\n    print(f\"\\n\\n{'='*60}\\nTEST CASE {i}: Claudesidian Tool Use\\n{'='*60}\")\n    generate_response(question)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}