{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocGQlIoGRtIL"
   },
   "source": "# Tool-Calling Fine-Tuning on Nebius AI Cloud\n\n[![Nebius AI](https://img.shields.io/badge/Nebius-AI%20Cloud-blue)](https://nebius.com/)\n\n## ğŸ“ What You'll Learn\n\nThis notebook teaches you how to fine-tune a language model to use the **Claudesidian-MCP toolset** for Obsidian vault operations **on Nebius JupyterHub**. By the end, you'll have:\n\n- **A custom AI model** that can call tools to manage Obsidian vaults\n- **Hands-on experience** with supervised fine-tuning (SFT) on cloud GPUs\n- **Understanding** of hyperparameters and how they affect training\n- **Published model** on HuggingFace ready to use\n\n## ğŸ”¬ What is SFT?\n\n**SFT (Supervised Fine-Tuning)** is like teaching through examples:\n- You show the model examples of correct tool-calling behavior\n- The model learns to replicate those patterns\n- Use SFT when teaching a model a **new skill** (like using tools)\n\n**When to use SFT:**\n- âœ… Teaching tool-calling from scratch\n- âœ… Learning new task formats\n- âœ… Initial training with positive examples\n\n**Not for:**\n- âŒ Refining existing behavior (use KTO instead)\n- âŒ Teaching preferences between good/bad outputs (use preference learning)\n\n## ğŸ’» Hardware (Nebius JupyterHub)\n\n**Recommended GPU:**\n- 7B models: H100 (80GB VRAM) - âœ… **3x faster than RTX 3090!**\n- 13B models: H100 (80GB VRAM) - Easy fit with room to spare\n- 20B+ models: H100 (80GB VRAM) - Plenty of headroom\n\n**Training time:**\n- 7B model on H100: ~15 minutes (vs 45 min on RTX 3090)\n- Cost: ~$0.38 with Explorer tier ($1.50/hour)\n\n**Cost estimate:**\n- Explorer tier: $1.50/GPU-hour (first 1,000 hours/month)\n- This notebook: ~$0.38-0.50 for full training run\n- 100+ experiments possible for under $100/month"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG-NCBytRtIM"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-_FO9oORtIM"
   },
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0di49m8RtIM"
   },
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets==4.3.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZd9FBPeRtIM"
   },
   "source": "## 2. Setup Workspace Directories\n\nSave checkpoints to Nebius workspace storage so they persist across sessions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMgubaVhRtIM"
   },
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# Setup workspace directories\n# On Nebius JupyterHub, /workspace is persistent storage\nWORKSPACE_OUTPUT_DIR = \"/workspace/sft_training_output\"\nos.makedirs(WORKSPACE_OUTPUT_DIR, exist_ok=True)\n\nprint(f\"âœ“ Workspace directory ready\")\nprint(f\"âœ“ Checkpoints will be saved to: {WORKSPACE_OUTPUT_DIR}\")\nprint(f\"âœ“ Files persist on Nebius storage\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30bXXvR_RtIM"
   },
   "source": "## 3. HuggingFace Credentials\n\n**Setup your HF token:**\n\n1. Get token from https://huggingface.co/settings/tokens (create with \"write\" access)\n2. Enter it in the cell below when prompted\n\n**Why you need this:**\n- Download the base model from HuggingFace\n- Upload your trained model when done"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TtcHRkzRtIN"
   },
   "outputs": [],
   "source": "import os\nimport getpass\nfrom huggingface_hub import HfApi\n\n# Get HuggingFace token (you'll be prompted to enter it)\nHF_TOKEN = getpass.getpass(\"Enter your HuggingFace token: \")\nos.environ['HF_TOKEN'] = HF_TOKEN\n\n# Get your HuggingFace username automatically\napi = HfApi()\nhf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n\nprint(f\"\\nâœ“ HuggingFace token loaded\")\nprint(f\"âœ“ Username: {hf_user}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8Y2ETsnRtIN"
   },
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "**What this does:** Choose the base model you want to fine-tune and configure basic settings.\n",
    "\n",
    "Think of this like choosing which \"brain\" you want to teach tool-calling skills to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEhWlq5wRtIN"
   },
   "outputs": [],
   "source": "# @title âš™ï¸ Model & Dataset Configuration\n# @markdown Use the dropdowns to select your model and configure your dataset.\n\n# @markdown ### ğŸ§  Base Model Selection\n# @markdown Choose a model based on your needs. On Nebius H100 (80GB VRAM), you can run any of these comfortably!\n# @markdown * **1B-3B:** Fast experimentation, ~5-10 min training\n# @markdown * **7B-9B:** Standard balance of speed/intelligence (~15 min) **[RECOMMENDED]**\n# @markdown * **12B-24B:** High intelligence, ~25-40 min training\nMODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # @param [\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\", \"unsloth/gemma-2-2b-it-bnb-4bit\", \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", \"unsloth/Phi-3.5-mini-instruct\", \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\", \"unsloth/mistral-7b-v0.3-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \"unsloth/gemma-2-9b-it-bnb-4bit\", \"unsloth/Mistral-Nemo-Instruct-v1-bnb-4bit\", \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\", \"unsloth/Phi-4\", \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", \"unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit\"]\n\n# @markdown ### ğŸ“ Max Output Length\n# @markdown  2048 is standard. Higher values work fine on H100!\nMAX_SEQ_LENGTH = 2048 # @param [1024, 2048, 4096, 8192] {type:\"raw\"}\n\n# @markdown ### ğŸ“š Dataset Configuration\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\" # @param {type:\"string\"}\nDATASET_FILE = \"syngen_tools_sft_11.22.25.jsonl\" # @param {type:\"string\"}\n\n# @markdown ### ğŸ·ï¸ Output Model Name\n# @markdown Name your fine-tuned model (e.g., `my-tool-model-v1`).\nOUTPUT_MODEL_NAME = \"claudesidian-tools-sft-7b-nebius\" # @param {type:\"string\"}\n\nprint(f\"âœ“ Configuration set:\")\nprint(f\"  â€¢ Model: {MODEL_NAME}\")\nprint(f\"  â€¢ Context: {MAX_SEQ_LENGTH} tokens\")\nprint(f\"  â€¢ Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"  â€¢ Output: {OUTPUT_MODEL_NAME}\")\nprint(f\"\\nğŸ’¡ On Nebius H100 (80GB), you can increase batch sizes for faster training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoPq_st8RtIN"
   },
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "**What this does:** Downloads the base model and prepares it for training.\n",
    "\n",
    "The model is the \"brain\" that will learn tool-calling. The tokenizer converts text into numbers the model can process. We use 4-bit quantization to fit large models into limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bbAGd7TRtIN"
   },
   "outputs": [],
   "source": "from unsloth import FastLanguageModel\nimport torch\n\n# Check GPU\nprint(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nprint()\n\nif \"H100\" in torch.cuda.get_device_name(0):\n    print(\"ğŸš€ You're running on NVIDIA H100!\")\n    print(\"   This is 3x faster than RTX 3090 for training\")\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQXkG71jRtIN"
   },
   "outputs": [],
   "source": [
    "# Load the base model and tokenizer from HuggingFace\n",
    "# This downloads the model weights (~7GB for 7B models)\n",
    "#\n",
    "# Parameters explained:\n",
    "#   model_name: Which model to download\n",
    "#   max_seq_length: Max tokens model can process at once\n",
    "#   dtype=None: Auto-detect best precision for your GPU\n",
    "#   load_in_4bit=True: Use 4-bit quantization to save memory\n",
    "#   token: Your HF token for accessing the model\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect (usually bfloat16 or float16)\n",
    "    load_in_4bit=True,  # Reduces memory usage by ~75%\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "print(f\"  Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLeRYX9ORtIN"
   },
   "source": [
    "## 6. Apply LoRA Adapters\n",
    "\n",
    "**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n",
    "\n",
    "Think of LoRA like teaching a new skill through muscle memory - we add small specialized layers that learn the new behavior, while keeping the main \"brain\" frozen. This is way faster and uses less memory than retraining everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-Y8IQtdRtIN"
   },
   "outputs": [],
   "source": [
    "# @title ğŸ”§ LoRA Adapter Configuration\n",
    "# @markdown Configure the size and strength of the fine-tuning adapters.\n",
    "\n",
    "# @markdown ### ğŸ›ï¸ LoRA Parameters\n",
    "# @markdown **Rank (r):** Higher = smarter but slower/more memory (Standard: 16-64).\n",
    "# @markdown Alpha will be automatically set to 2 * r.\n",
    "r = 32 # @param [8, 16, 32, 64, 128] {type:\"raw\"}\n",
    "\n",
    "lora_alpha = r * 2\n",
    "\n",
    "# @markdown **Dropout:** Helps prevent overfitting (Standard: 0.05).\n",
    "lora_dropout = 0.05 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown **Random Seed:** Change this for different initialization.\n",
    "random_state = 3407 # @param {type:\"integer\"}\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters applied:\")\n",
    "print(f\"  â€¢ Rank: {r}\")\n",
    "print(f\"  â€¢ Alpha: {lora_alpha}\")\n",
    "print(f\"  â€¢ Dropout: {lora_dropout}\")\n",
    "print(f\"  â€¢ Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRo35-UkRtIN"
   },
   "source": [
    "## 7. Load and Prepare Dataset\n",
    "\n",
    "**What this does:** Downloads training examples and formats them for the model.\n",
    "\n",
    "The dataset contains ~5,500 examples of correct tool-calling behavior. Think of it like a textbook full of solved problems that the model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiAxKyv1RtIN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "LOAD DATASET FROM HUGGINGFACE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "This downloads pre-made training examples of correct tool-calling behavior.\n",
    "Each example shows: user request â†’ tool call â†’ result â†’ assistant response\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_NAME}\")\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,  # HuggingFace repository containing the dataset\n",
    "    data_files=DATASET_FILE,  # Specific JSONL file to use\n",
    "    split=\"train\"  # Use the training split\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(dataset)} examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(dataset[0])\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SET CHAT TEMPLATE (Model-Specific)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "The chat template defines how conversations are formatted for the model.\n",
    "Different models use different formats - we auto-detect and use the right one!\n",
    "\"\"\"\n",
    "\n",
    "# Mistral-specific template (uses [INST] format)\n",
    "MISTRAL_CHAT_TEMPLATE = \"\"\"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'system' %}{% if loop.index == 1 %}{{ message['content'] + ' ' }}{% endif %}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "# Generic fallback template for other models\n",
    "DEFAULT_CHAT_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|user|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ '<|system|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>\\\\n'  + message['content'] + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    # Detect model type and use appropriate template\n",
    "    is_mistral = 'mistral' in MODEL_NAME.lower()\n",
    "\n",
    "    if is_mistral:\n",
    "        print(\"\\nâœ“ Detected Mistral model - using [INST] format\")\n",
    "        tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE\n",
    "        print(\"   Format: <s>[INST] user [/INST] assistant</s>\")\n",
    "        print(\"   This is the official Mistral chat format!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No chat template found, using default format\")\n",
    "        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "        print(\"   Format: <|user|>\\\\ncontent</s>\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Tokenizer already has chat template\")\n",
    "    if 'mistral' in MODEL_NAME.lower():\n",
    "        print(\"   Using Mistral model - template should use [INST] format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9G_kCjSRtIN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "FORMAT DATASET FOR TRAINING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Convert the conversation format into the exact text format the model expects.\n",
    "This applies the chat template to each example.\n",
    "\"\"\"\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Convert conversations to tokenizer's chat template.\n",
    "\n",
    "    Input: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, ...]}\n",
    "    Output: {\"text\": \"<|im_start|>user\\n...<|im_end|>\\n...\"}\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"conversations\"],  # List of message dicts\n",
    "        tokenize=False,  # Return string, not token IDs\n",
    "        add_generation_prompt=False  # Don't add prompt for model to continue\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting to entire dataset\n",
    "# This creates a new \"text\" field with formatted conversations\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,  # Function to apply\n",
    "    remove_columns=dataset.column_names,  # Remove original columns\n",
    "    desc=\"Formatting dataset\"  # Progress bar description\n",
    ")\n",
    "\n",
    "print(\"âœ“ Dataset formatted for training\")\n",
    "print(f\"\\nFormatted example (first 500 characters):\")\n",
    "print(dataset[0][\"text\"][:500])\n",
    "print(\"\\nğŸ’¡ The full formatted text will be used during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43Kqdd9wRtIO"
   },
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "**What this does:** Set the hyperparameters that control how the model learns.\n",
    "\n",
    "This is the **most important section** - these settings determine how fast the model learns, how much memory it uses, and how good the final result will be. Think of it like configuring a study plan: how many hours per day, how many review sessions, how to handle difficult material, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlOVjlIGRtIO"
   },
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bfloat16_supported\nfrom datetime import datetime\n\n# Create timestamped output directory\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_dir = f\"{WORKSPACE_OUTPUT_DIR}/{timestamp}\"\n\n# @title ğŸƒ Training Hyperparameters - OPTIMIZED FOR NEBIUS H100\n# @markdown Control the speed and quality of training.\n\n# @markdown ### âš¡ Performance Settings\n# @markdown **Batch Size:** Examples per step. On H100 (80GB), we can go higher!\nper_device_train_batch_size = 8 # @param [1, 2, 4, 8, 16] {type:\"raw\"}\n\n# @markdown **Gradient Accumulation:** Simulates larger batches. Lower since we can use larger real batch sizes.\ngradient_accumulation_steps = 3 # @param [1, 2, 3, 4, 8, 16] {type:\"raw\"}\n\n# @markdown ### ğŸ§  Learning Rate Configuration\n# @markdown **Step 1: Choose the Magnitude (Exponent)**\n# @markdown This is the most important setting. It determines the \"speed\" of learning.\n# @markdown * **4** = Standard (1e-4). Recommended for 7B models and SFT.\n# @markdown * **5** = Slow (1e-5). Use if training is unstable or for larger models.\n# @markdown * **6** = Very Slow (1e-6). Precise but takes much longer.\nlearning_rate_exponent = 4 # @param [4, 5, 6] {type:\"raw\"}\n\n# @markdown **Step 2: Choose the Multiplier**\n# @markdown Fine-tunes the rate within that magnitude (e.g., Multiplier 2 + Exponent 4 = 2e-4).\nlearning_rate_multiplier = 2 # @param [1, 2, 3, 4, 5, 6, 7, 8, 9] {type:\"raw\"}\n\nlearning_rate = learning_rate_multiplier * (10 ** -learning_rate_exponent)\n\n# @markdown ### ğŸ”„ Epochs\n# @markdown Number of passes through the dataset.\nnum_train_epochs = 3 # @param {type:\"integer\"}\n\n# @markdown ### ğŸ’¾ Saving & Logging\nsave_steps = 100 # @param {type:\"integer\"}\nlogging_steps = 5 # @param {type:\"integer\"}\n\n# Effective batch size\neffective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n\ntraining_args = SFTConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=1.0,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    num_train_epochs=num_train_epochs,\n    max_seq_length=MAX_SEQ_LENGTH,\n    packing=False,\n    dataset_text_field=\"text\",\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    optim=\"adamw_8bit\",\n    gradient_checkpointing=True,\n    logging_steps=logging_steps,\n    save_steps=save_steps,\n    save_total_limit=3,\n    seed=42,\n    report_to=\"none\",\n)\n\nprint(\"âœ“ Training configuration ready\")\nprint(f\"  â€¢ Batch Size: {per_device_train_batch_size} (higher on H100!)\")\nprint(f\"  â€¢ Gradient Accum.: {gradient_accumulation_steps}\")\nprint(f\"  â€¢ Effective Batch: {effective_batch_size}\")\nprint(f\"  â€¢ Learning Rate: {learning_rate} ({learning_rate_multiplier}e-{learning_rate_exponent})\")\nprint(f\"  â€¢ Epochs: {num_train_epochs}\")\nprint(f\"  â€¢ Output Dir: {output_dir}\")\nprint(f\"\\nğŸš€ Optimized for Nebius H100 - will be ~3x faster than RTX 3090!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b6bueM4RtIO"
   },
   "source": [
    "## 9. Initialize Trainer\n",
    "\n",
    "**What this does:** Creates the training engine that coordinates everything.\n",
    "\n",
    "The SFTTrainer is the orchestrator - it takes the model, dataset, and configuration, then handles all the training mechanics (gradient updates, checkpointing, logging, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xRGa_dXRtIO"
   },
   "outputs": [],
   "source": [
    "# Create the SFTTrainer\n",
    "# This combines the model, dataset, and configuration into one training pipeline\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The model with LoRA adapters\n",
    "    tokenizer=tokenizer,  # For converting text to tokens\n",
    "    train_dataset=dataset,  # Our formatted training examples\n",
    "    args=training_args,  # All the hyperparameters we configured\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(\"  Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5yi6rJ1RtIO"
   },
   "source": "## 10. Train!\n\n**What this does:** The actual learning happens here!\n\nThe model will:\n1. **Read examples** from the dataset\n2. **Predict** what the response should be\n3. **Compare** its prediction to the correct answer\n4. **Update weights** to get closer to the correct answer\n5. **Repeat** this process for 3 epochs (3 full passes through the data)\n\n**What to expect on Nebius H100:**\n- Training takes ~15 minutes for 7B models (3x faster than RTX 3090!)\n- You'll see progress updates every 5 steps\n- Loss should generally decrease over time (learning is working!)\n- Checkpoints are saved every 100 steps to workspace\n- **Cost:** ~$0.38 with Explorer tier ($1.50/hour)\n\n**What the metrics mean:**\n- **Loss:** How \"wrong\" the model is (lower = better, aim for <1.0)\n- **Learning Rate:** Gradually decreases as training progresses\n- **Samples/sec:** Training speed (should be high on H100!)\n\n**ğŸ’¾ Checkpoint Resumption:**\nIf your session disconnects, don't worry! Your checkpoints are saved to workspace. You can resume training by:\n1. Re-running cells 1-9 (setup, model loading, dataset prep, config)\n2. In the training cell below, the code will automatically detect the latest checkpoint and resume from there\n3. Your progress is preserved!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6490TDHRtIO"
   },
   "outputs": [],
   "source": [
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” Check for existing checkpoints (automatic resumption)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import glob\n",
    "import os\n",
    "\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Found checkpoints - get the latest one\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = latest_checkpoint\n",
    "    print(f\"âœ“ Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"  Resuming training from this checkpoint\")\n",
    "    print(f\"  Total checkpoints found: {len(checkpoint_dirs)}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No existing checkpoints found - starting fresh training\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Start training (or resume)\n",
    "print(\"=\" * 60)\n",
    "if resume_from_checkpoint:\n",
    "    print(\"RESUMING TRAINING FROM CHECKPOINT\")\n",
    "else:\n",
    "    print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZwBSPBvRtIO"
   },
   "source": [
    "## 11. Upload to HuggingFace\n",
    "\n",
    "**What this does:** Share your trained model with the world!\n",
    "\n",
    "We'll create **three versions** of your model:\n",
    "\n",
    "1. **LoRA adapters** (~320MB) - Small files that contain just the changes\n",
    "   - Fast to download\n",
    "   - Need to be combined with base model to use\n",
    "   \n",
    "2. **Merged 16-bit model** (~14GB) - Full model with adapters merged in\n",
    "   - High quality, no precision loss\n",
    "   - Large file size\n",
    "   - Best for local deployment (Ollama, LM Studio)\n",
    "   \n",
    "3. **GGUF quantizations** - Optimized versions for CPU/GPU inference\n",
    "   - Q4_K_M (~3.5GB) - Recommended for most users\n",
    "   - Q5_K_M (~4.5GB) - Better quality, larger size\n",
    "   - Q8_0 (~7GB) - Nearly full quality\n",
    "   - These work directly with Ollama and LM Studio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yU-kZcxaRtIO"
   },
   "outputs": [],
   "source": [
    "# Upload LoRA adapters\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN89CamTRtIO"
   },
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Merged model uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K90WE6frRtIO"
   },
   "outputs": [],
   "source": [
    "# Create GGUF quantizations\n",
    "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    "\n",
    "print(\"Creating GGUF quantizations...\")\n",
    "print(f\"This will create {len(quantization_methods)} versions\")\n",
    "print()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_methods,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"âœ“ GGUF quantizations created and uploaded!\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7fuKcZuRtIO"
   },
   "source": [
    "## ğŸ‰ Done!\n",
    "\n",
    "Your model has been trained and uploaded to HuggingFace!\n",
    "\n",
    "## ğŸ“Š What You Accomplished\n",
    "\n",
    "âœ… **Fine-tuned a 7B parameter language model** to use the Claudesidian toolset  \n",
    "âœ… **Learned about SFT hyperparameters** and how they affect training  \n",
    "âœ… **Created multiple model formats** (LoRA, merged, GGUF) for different use cases  \n",
    "âœ… **Published your model** to HuggingFace for others to use  \n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "### 1. Test Your Model\n",
    "\n",
    "**Using LM Studio:**\n",
    "1. Open LM Studio â†’ \"Discover\" tab\n",
    "2. Search for your username: `{hf_user}`\n",
    "3. Download the Q4_K_M or Q5_K_M GGUF version\n",
    "4. Load and test with tool-calling prompts!\n",
    "\n",
    "**Using Ollama:**\n",
    "```bash\n",
    "# Download your model\n",
    "ollama pull {hf_user}/{OUTPUT_MODEL_NAME}\n",
    "\n",
    "# Test it\n",
    "ollama run {hf_user}/{OUTPUT_MODEL_NAME}\n",
    "```\n",
    "\n",
    "### 2. Evaluate Quality\n",
    "\n",
    "Run the Evaluator to test tool-calling accuracy:\n",
    "```bash\n",
    "python -m Evaluator.cli \\\n",
    "  --model {OUTPUT_MODEL_NAME} \\\n",
    "  --prompt-set Evaluator/prompts/full_coverage.json \\\n",
    "  --output results.json\n",
    "```\n",
    "\n",
    "### 3. Refine Further (Optional)\n",
    "\n",
    "If you want to improve your model:\n",
    "- **Collect more data**: Add examples where your model struggles\n",
    "- **Try KTO training**: Refine behavior by showing good vs bad examples\n",
    "- **Adjust hyperparameters**: Experiment with learning rate, batch size, etc.\n",
    "\n",
    "## ğŸ“ Learn More\n",
    "\n",
    "- **KTO Training**: For preference learning (good vs bad outputs)\n",
    "- **Dataset Creation**: Build custom training data for your use case\n",
    "- **Model Evaluation**: Systematic testing of tool-calling accuracy\n",
    "\n",
    "## ğŸ’¡ Tips for Better Models\n",
    "\n",
    "1. **More data is better**: 5,000+ examples produce robust models\n",
    "2. **Quality over quantity**: Clean, accurate examples matter more than volume\n",
    "3. **Test thoroughly**: Use the Evaluator to catch issues early\n",
    "4. **Iterate**: Fine-tuning is iterative - train, test, improve, repeat\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the documentation or open an issue on GitHub!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}