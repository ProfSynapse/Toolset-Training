{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toolset Training on Nebius JupyterHub\n",
        "\n",
        "This notebook demonstrates how to run SFT and KTO training on Nebius AI Cloud using JupyterHub.\n",
        "\n",
        "## Prerequisites\n",
        "- Nebius JupyterHub instance with H100/H200 GPU\n",
        "- Training repository uploaded to `/workspace/`\n",
        "- Datasets uploaded to `/workspace/Datasets/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth and dependencies\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes -q\n",
        "!pip install wandb python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "import unsloth\n",
        "from transformers import __version__ as transformers_version\n",
        "from trl import __version__ as trl_version\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Transformers version: {transformers_version}\")\n",
        "print(f\"TRL version: {trl_version}\")\n",
        "print(f\"Unsloth installed: ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure Paths and Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add training modules to path\n",
        "WORKSPACE = Path(\"/workspace/Toolset-Training\")\n",
        "SFT_TRAINER = WORKSPACE / \"Trainers/rtx3090_sft\"\n",
        "KTO_TRAINER = WORKSPACE / \"Trainers/rtx3090_kto\"\n",
        "DATASETS = WORKSPACE / \"Datasets\"\n",
        "\n",
        "sys.path.insert(0, str(SFT_TRAINER))\n",
        "sys.path.insert(0, str(KTO_TRAINER))\n",
        "\n",
        "# Verify paths\n",
        "print(f\"SFT Trainer: {SFT_TRAINER.exists()}\")\n",
        "print(f\"KTO Trainer: {KTO_TRAINER.exists()}\")\n",
        "print(f\"Datasets: {DATASETS.exists()}\")\n",
        "\n",
        "# List available datasets\n",
        "print(\"\\nAvailable datasets:\")\n",
        "for dataset in DATASETS.glob(\"*.jsonl\"):\n",
        "    print(f\"  - {dataset.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Configure W&B for experiment tracking\n",
        "import wandb\n",
        "\n",
        "# Set your W&B API key\n",
        "os.environ[\"WANDB_API_KEY\"] = \"your-wandb-key-here\"  # Replace with your key\n",
        "\n",
        "# Or login interactively\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. SFT Training (Supervised Fine-Tuning)\n",
        "\n",
        "Train the model to learn tool-calling behavior from positive examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import SFT training modules\n",
        "os.chdir(SFT_TRAINER)\n",
        "\n",
        "from configs.training_config import get_7b_config, ModelConfig, LoRAConfig, SFTTrainingConfig, DatasetConfig\n",
        "from src.model_loader import load_model_and_tokenizer\n",
        "from src.data_loader import prepare_dataset\n",
        "from src.training_callbacks import MetricsTableCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure SFT training\n",
        "sft_config = get_7b_config()\n",
        "\n",
        "# Update dataset path\n",
        "sft_config.dataset_config.local_file = str(DATASETS / \"syngen_tools_sft_11.18.25.jsonl\")\n",
        "\n",
        "# Optional: Adjust for H100 (you can increase batch size!)\n",
        "sft_config.training_config.per_device_train_batch_size = 8  # Up from 6\n",
        "sft_config.training_config.gradient_accumulation_steps = 3  # Effective batch = 24\n",
        "\n",
        "# Optional: Enable W&B\n",
        "# sft_config.training_config.report_to = [\"wandb\"]\n",
        "# sft_config.training_config.run_name = \"nebius-sft-7b\"\n",
        "\n",
        "print(\"SFT Configuration:\")\n",
        "print(f\"  Model: {sft_config.model_config.model_name}\")\n",
        "print(f\"  Dataset: {sft_config.dataset_config.local_file}\")\n",
        "print(f\"  Batch size: {sft_config.training_config.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {sft_config.training_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {sft_config.training_config.per_device_train_batch_size * sft_config.training_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {sft_config.training_config.learning_rate}\")\n",
        "print(f\"  Epochs: {sft_config.training_config.num_train_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "print(\"Loading model and tokenizer...\")\n",
        "model, tokenizer = load_model_and_tokenizer(\n",
        "    sft_config.model_config,\n",
        "    sft_config.lora_config\n",
        ")\n",
        "print(\"✓ Model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "print(\"Preparing dataset...\")\n",
        "train_dataset = prepare_dataset(sft_config.dataset_config, tokenizer)\n",
        "print(f\"✓ Dataset loaded: {len(train_dataset)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup output directory\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = SFT_TRAINER / f\"sft_output_nebius/{timestamp}\"\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sft_config.training_config.output_dir = str(output_dir)\n",
        "sft_config.training_config.logging_dir = str(output_dir / \"logs\")\n",
        "\n",
        "print(f\"Output directory: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train!\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"Starting SFT training...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=sft_config.training_config.output_dir,\n",
        "        per_device_train_batch_size=sft_config.training_config.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=sft_config.training_config.gradient_accumulation_steps,\n",
        "        learning_rate=sft_config.training_config.learning_rate,\n",
        "        num_train_epochs=sft_config.training_config.num_train_epochs,\n",
        "        logging_steps=sft_config.training_config.logging_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        max_grad_norm=sft_config.training_config.max_grad_norm,\n",
        "        warmup_steps=sft_config.training_config.warmup_steps,\n",
        "        fp16=True,\n",
        "        logging_dir=sft_config.training_config.logging_dir,\n",
        "    ),\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=sft_config.training_config.max_seq_length,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n✓ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "final_model_dir = output_dir / \"final_model\"\n",
        "final_model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "model.save_pretrained(str(final_model_dir))\n",
        "tokenizer.save_pretrained(str(final_model_dir))\n",
        "\n",
        "print(f\"✓ Model saved to: {final_model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick inference test\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test prompt\n",
        "test_prompt = \"\"\"Create a new note titled 'Meeting Notes' with content about the quarterly review.\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    test_prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nModel Response:\")\n",
        "print(\"=\"*80)\n",
        "print(response)\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload to HuggingFace (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set HuggingFace token\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_your_token_here\"  # Replace with your token\n",
        "\n",
        "# Or login interactively\n",
        "# from huggingface_hub import login\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload using the existing upload script\n",
        "# Note: Adjust paths based on where you saved the model\n",
        "\n",
        "!python {SFT_TRAINER}/src/upload_to_hf.py \\\n",
        "  {final_model_dir} \\\n",
        "  your-username/toolset-sft-7b-nebius \\\n",
        "  --save-method merged_16bit \\\n",
        "  --create-gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. KTO Training (Optional Refinement)\n",
        "\n",
        "After SFT, you can optionally refine with KTO using preference learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Switch to KTO trainer directory\n",
        "os.chdir(KTO_TRAINER)\n",
        "sys.path.insert(0, str(KTO_TRAINER))\n",
        "\n",
        "from configs.training_config import get_7b_config as get_kto_7b_config\n",
        "# ... (similar setup as SFT, but with KTO dataset and trainer)\n",
        "\n",
        "print(\"KTO training setup would go here...\")\n",
        "print(\"See the full KTO training script in Trainers/rtx3090_kto/train_kto.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Monitor GPU Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU memory usage\n",
        "!nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View training logs\n",
        "!tail -n 50 {output_dir}/logs/training_*.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. ✓ Environment setup on Nebius JupyterHub\n",
        "2. ✓ SFT training with your existing pipeline\n",
        "3. ✓ Model testing and inference\n",
        "4. ✓ Uploading to HuggingFace\n",
        "\n",
        "**Next Steps:**\n",
        "- Run KTO training for refinement\n",
        "- Experiment with different hyperparameters\n",
        "- Use W&B for experiment tracking\n",
        "- Try multi-GPU training (if using multi-node setup)\n",
        "\n",
        "**Estimated Cost (H100 Explorer Tier at $1.50/hour):**\n",
        "- SFT Training (45 min): ~$1.13\n",
        "- KTO Training (15 min): ~$0.38\n",
        "- **Total: ~$1.50 for complete pipeline**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
