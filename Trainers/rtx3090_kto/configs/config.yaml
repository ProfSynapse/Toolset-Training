model:
  model_name: F:\Code\Toolset-Training\Trainers\rtx3090_sft\sft_output_rtx3090\20251123_090722\merged_model
  max_seq_length: 2048
  dtype: null
  load_in_4bit: true
lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  bias: none
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  use_gradient_checkpointing: unsloth
  random_state: 3407
  use_rslora: false
  use_dora: false
training:
  output_dir: ./kto_output_rtx3090
  per_device_train_batch_size: 6
  gradient_accumulation_steps: 4
  beta: 0.2
  desirable_weight: 0.7
  undesirable_weight: 1.0
  learning_rate: 5e-7
  max_grad_norm: 0.5
  lr_scheduler_type: cosine
  use_kto_s: false
  use_two_stage_lr: false
  lr_reduction_step: 50
  lr_reduction_factor: 0.5
  max_length: 2048
  max_prompt_length: 1024
  gradient_checkpointing: true
  optim: adamw_8bit
  fp16: false
  bf16: true
  num_train_epochs: 1
  warmup_ratio: 0.15
  logging_steps: 5
  save_steps: 50
  save_total_limit: 3
  dataloader_num_workers: 0
  dataloader_pin_memory: true
  group_by_length: false
  eval_strategy: 'no'
  eval_steps: 100
dataset:
  dataset_name: professorsynapse/claudesidian-synthetic-dataset
  dataset_file: behavior_merged_kto_v1.1.jsonl
  local_file: ../../Datasets/behavior_merged_kto_v1.1.jsonl
  num_proc: 1
  test_size: 0.1
  chat_template: chatml
wandb:
  enabled: false
  project: kto-finetuning
  run_name: null
  entity: null
seed: 42
