# ============================================================================
# SFT Training Configuration for RTX 3090
# Supervised Fine-Tuning for tool-calling instruction learning
# ============================================================================

# Model Configuration
model:
  # Model selection - Tier 2 (7B) recommended for production
  # Tier 1 (3B): "unsloth/Qwen2.5-3B-Instruct-bnb-4bit", "unsloth/Llama-3.2-3B-Instruct-bnb-4bit"
  # Tier 2 (7B): "unsloth/mistral-7b-v0.3-bnb-4bit", "unsloth/llama-3.1-8b-instruct-bnb-4bit"
  # Tier 3 (13B): "unsloth/llama-2-13b-bnb-4bit"
  model_name: "unsloth/Ministral-3-8B-Instruct-2512"

  # Model parameters
  max_seq_length: 2048 
  dtype: null  # Auto-detection
  load_in_4bit: true  # Essential for memory efficiency

# LoRA Adapter Configuration
lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
  bias: "none"
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  use_gradient_checkpointing: "unsloth"  # Unsloth's optimized version
  random_state: 3407

# SFT Training Configuration
training:
  # Output directory
  output_dir: "./sft_output_rtx3090"

  # Batch configuration
  per_device_train_batch_size: 2  
  gradient_accumulation_steps: 4  

  # Learning rate
  learning_rate: 2e-4  # vs KTO's 2e-7
  max_grad_norm: 1.0  # vs KTO's 0.5
  lr_scheduler_type: "cosine"

  # SFT-specific parameters
  max_seq_length: 2048 
  packing: false  # Pack multiple examples per sequence
  completion_only_loss: true  # Train only on assistant completions
  assistant_only_loss: false  # For multi-turn (not needed here)

  # Memory optimizations
  gradient_checkpointing: true  # Reduces memory usage
  optim: "adamw_8bit"  # 8-bit optimizer saves ~2GB VRAM
  fp16: false  # Set dynamically based on GPU
  bf16: true  # RTX 3090 supports BF16 (Ampere)

  # Training schedule - Multi-epoch for better learning
  num_train_epochs: 2  # vs KTO's 1 (more epochs to internalize patterns)
  warmup_ratio: 0.1  # 10% warmup

  # Logging and saving
  logging_steps: 5  # Log every 5 steps
  save_steps: 50  # Save checkpoint every 50 steps
  save_total_limit: 3  # Keep last 3 checkpoints

  # Performance
  dataloader_num_workers: 0  # MUST be 0 on WSL2 (multiprocessing hangs)
  dataloader_pin_memory: true
  group_by_length: false  # Can cause hangs with multiprocessing

  # Evaluation (optional)
  eval_strategy: "no"  # "steps" or "no"
  eval_steps: 50

# Dataset Configuration
dataset:
  # Dataset source
  dataset_name: "professorsynapse/nexus-synthetic-dataset"
  dataset_file: "tools_sft_v1.5_11.29.25.jsonl"  # 6,794 examples (removed 10 corrupted)

  # Use local file (relative to Trainers/rtx3090_sft/)
  local_file: "../../Datasets/tools_sft_v1.5_11.29.25.jsonl"

  # Dataset processing
  num_proc: 1  # Set to 1 on Windows to avoid multiprocessing issues
  test_size: 0.1  # For train/validation split (optional)
  split_dataset: false  # Enable to create train/val split

  # SFT-specific: filter for desirable examples only
  filter_desirable: false  # False because SFT dataset already filtered

# Weights & Biases (optional)
wandb:
  enabled: false  # Set to true to enable W&B logging
  project: null  # W&B project name
  run_name: null  # Run name (auto-generated if null)
  entity: null  # W&B entity/team

# Random seed
seed: 42

# Notes:
# - This is optimized for RTX 3090 24GB VRAM
# - Expected VRAM usage: ~7-9GB for 7B model
# - Training time: ~60 minutes for 3 epochs with 8,036 examples
# - For 3B models: increase batch_size to 12, reduce to ~12GB VRAM
# - For 13B models: reduce batch_size to 4, expect ~14GB VRAM
