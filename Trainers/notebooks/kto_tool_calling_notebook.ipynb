{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/kto_tool_calling_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# KTO Training for Tool Calling - Claudesidian Vault Tools\n\nThis notebook trains a language model using KTO (Kahneman-Tversky Optimization) to internalize tool calling for the Claudesidian vault application.\n\n**Dataset**: claudesidian-behaviors-merged v1.1 (1,852 examples, 8 behaviors)\n- **Total Examples**: 1,852 (1,085 positive, 767 negative)\n- **Behaviors**: context_continuity, context_efficiency, error_recovery, execute_prompt_usage, intellectual_humility, strategic_tool_selection, verification_before_action, workspace_awareness\n- **Format**: OpenAI-compatible tool calling format\n- **Source**: HuggingFace Hub (professorsynapse/claudesidian-behaviors-merged)\n\n**Goal**: Train the model to recognize and use the correct tools with proper behavioral patterns for vault operations."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Fast installation using --no-deps to avoid dependency resolution delays (2-3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fast installation for Colab - uses --no-deps to avoid dependency resolution delays\n\n# Check if packages are already installed correctly\nimport importlib.metadata\nimport sys\n\ndef check_install_needed():\n    \"\"\"Check if we need to install packages or if they're already correct.\"\"\"\n    try:\n        import torch\n        import numpy as np\n        \n        # Check versions\n        torch_ok = torch.__version__.startswith(\"2.4.1\")\n        numpy_ok = np.__version__.startswith(\"1.26\")\n        transformers_ok = importlib.metadata.version(\"transformers\").startswith(\"4.56\")\n        trl_ok = importlib.metadata.version(\"trl\").startswith(\"0.22\")\n        \n        # Check if unsloth is importable\n        try:\n            from unsloth import FastLanguageModel\n            unsloth_ok = True\n        except:\n            unsloth_ok = False\n        \n        all_ok = torch_ok and numpy_ok and transformers_ok and trl_ok and unsloth_ok\n        \n        if all_ok:\n            print(\"=\" * 60)\n            print(\"‚úì ALL PACKAGES ALREADY INSTALLED CORRECTLY!\")\n            print(\"=\" * 60)\n            print(f\"‚úì PyTorch: {torch.__version__}\")\n            print(f\"‚úì NumPy: {np.__version__}\")\n            print(f\"‚úì Transformers: {importlib.metadata.version('transformers')}\")\n            print(f\"‚úì TRL: {importlib.metadata.version('trl')}\")\n            print(f\"‚úì Unsloth: Ready\")\n            print(\"\\n\" + \"=\" * 60)\n            print(\"SKIPPING INSTALLATION - Proceed to next cell!\")\n            print(\"=\" * 60)\n            return False  # Don't need to install\n        else:\n            print(\"Packages need installation/update:\")\n            print(f\"  PyTorch: {torch.__version__} {'‚úì' if torch_ok else '‚ùå (need 2.4.1)'}\")\n            print(f\"  NumPy: {np.__version__} {'‚úì' if numpy_ok else '‚ùå (need 1.26.x)'}\")\n            print(f\"  Transformers: {importlib.metadata.version('transformers')} {'‚úì' if transformers_ok else '‚ùå (need 4.56.x)'}\")\n            print(f\"  TRL: {importlib.metadata.version('trl')} {'‚úì' if trl_ok else '‚ùå (need 0.22.x)'}\")\n            print(f\"  Unsloth: {'‚úì' if unsloth_ok else '‚ùå (need to install)'}\")\n            return True  # Need to install\n    except Exception as e:\n        print(f\"Initial check failed (first install): {e}\")\n        return True  # Need to install\n\n# Only run installation if needed\nif check_install_needed():\n    print(\"\\n\" + \"=\" * 60)\n    print(\"INSTALLING PACKAGES (3-5 minutes)...\")\n    print(\"=\" * 60)\n    \n    # Step 0: Uninstall packages that pin torch to newer versions\n    print(\"\\n[0/10] Removing xformers and flash-attn (will reinstall later)...\")\n    !pip uninstall -y -q xformers flash-attn 2>/dev/null || echo \"Packages not installed\"\n    \n    # Step 1: Install PyTorch 2.4.1 with CUDA 12.1\n    print(\"\\n[1/10] Installing PyTorch 2.4.1 + CUDA 12.1...\")\n    !pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n    print(\"‚úì PyTorch 2.4.1 installed\")\n    \n    # Step 2: Install core dependencies\n    print(\"\\n[2/10] Installing core dependencies...\")\n    !pip install --no-deps bitsandbytes accelerate peft triton cut_cross_entropy unsloth_zoo\n    print(\"‚úì Core dependencies installed\")\n    \n    # Step 3: Install supporting libraries\n    print(\"\\n[3/10] Installing supporting libraries...\")\n    !pip install sentencepiece protobuf \"datasets>=2.14.0,<4.0.0\" \"huggingface_hub>=0.20.0\"\n    print(\"‚úì Supporting libraries installed\")\n    \n    # Step 4: Install transformers and trl\n    print(\"\\n[4/10] Installing transformers and trl...\")\n    !pip install transformers==4.56.2\n    !pip install --no-deps trl==0.22.2\n    print(\"‚úì Transformers and TRL installed\")\n    \n    # Step 5: Install tyro and msgspec\n    print(\"\\n[5/10] Installing tyro and msgspec...\")\n    !pip install tyro msgspec\n    print(\"‚úì Tyro and msgspec installed\")\n    \n    # Step 6: Install xformers with --no-deps (prevents torch upgrade)\n    print(\"\\n[6/10] Installing xformers...\")\n    !pip install --no-deps xformers\n    print(\"‚úì xformers installed\")\n    \n    # Step 7: Install unsloth\n    print(\"\\n[7/10] Installing unsloth...\")\n    !pip install --no-deps unsloth\n    print(\"‚úì Unsloth installed\")\n    \n    # Step 8: Install numpy\n    print(\"\\n[8/10] Installing numpy...\")\n    !pip install \"numpy>=1.24.0,<2.0\"\n    print(\"‚úì NumPy configured\")\n    \n    # Step 9: Force PyTorch back to 2.4.1\n    print(\"\\n[9/10] Locking PyTorch at 2.4.1...\")\n    !pip install --force-reinstall torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n    print(\"‚úì PyTorch 2.4.1 locked\")\n    \n    # Step 10: Install Flash Attention with --no-deps (prevents torch upgrade)\n    print(\"\\n[10/10] Installing Flash Attention...\")\n    import torch\n    if torch.cuda.get_device_capability()[0] >= 8:\n        !pip install ninja packaging\n        !pip install --no-deps flash-attn\n        print(\"‚úì Flash Attention installed (no-deps)\")\n    else:\n        print(\"‚ö† GPU doesn't support Flash Attention 2 (skipping)\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚úì INSTALLATION COMPLETE!\")\n    print(\"=\" * 60)\n    print(\"\\n‚ö†Ô∏è  RESTART REQUIRED: Runtime ‚Üí Restart runtime\")\n    print(\"Then re-run this cell (will skip installation)!\")\n    print(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nprint(\"\\nImporting libraries...\")\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nimport torch\nimport os\nimport json\nfrom datasets import Dataset\nfrom trl import KTOConfig, KTOTrainer\n\nprint(\"\\n‚úì All imports successful!\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Model Loading\nLoad a pre-trained model suitable for tool calling tasks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Model configuration\nmax_seq_length = 4096\ndtype = None  # Auto-detect: Float16 for older GPUs, Bfloat16 for Ampere+\nload_in_4bit = True  # Use 4-bit quantization for memory efficiency\n\n# Load model and tokenizer\n# Options:\n# - \"unsloth/Qwen2.5-Coder-1.5B-Instruct\" (small, fast)\n# - \"unsloth/Qwen2.5-7B-Instruct\" (medium)\n# - \"unsloth/Llama-3.2-3B-Instruct\" (small, good for tool calling)\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-Coder-1.5B-Instruct\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nprint(f\"‚úì Model loaded: {model.config.model_type}\")\nprint(f\"‚úì Tokenizer vocab size: {len(tokenizer)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Processing\n",
    "Load the tool calling dataset and convert to KTO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the dataset from HuggingFace\nfrom datasets import load_dataset\n\n# Load the merged behavioral dataset (v1.1)\n# Contains 1,852 examples across 8 behavioral patterns\n# All in OpenAI-compatible tool calling format\nraw_dataset = load_dataset(\n    \"professorsynapse/claudesidian-behaviors-merged\"\n)\n\nprint(f\"‚úì Dataset loaded: {len(raw_dataset['train'])} examples\")\nprint(f\"  Dataset: claudesidian-behaviors-merged v1.1\")\nprint(f\"  Behaviors: 8 (context_continuity, context_efficiency, error_recovery,\")\nprint(f\"             execute_prompt_usage, intellectual_humility,\")\nprint(f\"             strategic_tool_selection, verification_before_action,\")\nprint(f\"             workspace_awareness)\")\nprint(f\"  Format: OpenAI-compatible tool calling\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert OpenAI tool calling format to KTO format\ndef convert_to_kto_format(example):\n    \"\"\"\n    Convert tool calling conversations to KTO format.\n    \n    Input format (OpenAI-compatible):\n    {\n      \"conversations\": [\n        {\"role\": \"user\", \"content\": \"...\"},\n        {\n          \"role\": \"assistant\", \n          \"content\": null,\n          \"tool_calls\": [\n            {\n              \"id\": \"abc123\",\n              \"type\": \"function\",\n              \"function\": {\n                \"name\": \"toolName\",\n                \"arguments\": \"{...}\"\n              }\n            }\n          ]\n        }\n      ],\n      \"label\": true/false,\n      \"behavior\": \"verification_before_action\"\n    }\n    \n    Output format:\n    {\n      \"prompt\": \"user message\",\n      \"completion\": \"formatted tool call with behavior context\",\n      \"label\": true/false\n    }\n    \"\"\"\n    conversations = example[\"conversations\"]\n    behavior = example.get(\"behavior\", \"general\")\n    \n    # Extract user and assistant messages\n    user_msg = None\n    assistant_msg = None\n    \n    for msg in conversations:\n        if msg[\"role\"] == \"user\":\n            user_msg = msg[\"content\"]\n        elif msg[\"role\"] == \"assistant\":\n            # Handle OpenAI tool calling format\n            if \"tool_calls\" in msg:\n                # Format tool calls for display\n                tool_calls_str = []\n                for tc in msg[\"tool_calls\"]:\n                    func = tc[\"function\"]\n                    tool_calls_str.append(\n                        f\"tool_call: {func['name']}\\n\"\n                        f\"arguments: {func['arguments']}\"\n                    )\n                assistant_msg = \"\\n\\n\".join(tool_calls_str)\n            else:\n                assistant_msg = msg[\"content\"]\n    \n    if user_msg is None or assistant_msg is None:\n        return None\n    \n    # Add behavior context to prompt\n    prompt_with_behavior = f\"[Behavior: {behavior}]\\n{user_msg}\"\n    \n    return {\n        \"prompt\": prompt_with_behavior,\n        \"completion\": assistant_msg,\n        \"label\": example[\"label\"]\n    }\n\n# Process the dataset\nkto_data = []\nskipped = 0\nfor example in raw_dataset[\"train\"]:\n    processed = convert_to_kto_format(example)\n    if processed:\n        kto_data.append(processed)\n    else:\n        skipped += 1\n\n# Create HuggingFace Dataset\ntrain_dataset = Dataset.from_dict({\n    \"prompt\": [ex[\"prompt\"] for ex in kto_data],\n    \"completion\": [ex[\"completion\"] for ex in kto_data],\n    \"label\": [ex[\"label\"] for ex in kto_data],\n})\n\n# Show statistics\ndesirable = sum(train_dataset[\"label\"])\nundesirable = len(train_dataset) - desirable\n\nprint(f\"\\n‚úì KTO Dataset prepared:\")\nprint(f\"  Total examples: {len(train_dataset)}\")\nprint(f\"  Desirable (good behavior): {desirable}\")\nprint(f\"  Undesirable (bad behavior): {undesirable}\")\nprint(f\"  Ratio: {desirable/undesirable:.2f}:1\")\nif skipped > 0:\n    print(f\"  Skipped: {skipped}\")\n\n# Show example\nprint(f\"\\nüìù Example (desirable):\")\ndesirable_ex = [ex for ex in kto_data if ex[\"label\"]][0]\nprint(f\"Prompt: {desirable_ex['prompt'][:120]}...\")\nprint(f\"Completion: {desirable_ex['completion'][:200]}...\")\n\nprint(f\"\\nüìù Example (undesirable):\")\nundesirable_ex = [ex for ex in kto_data if not ex[\"label\"]][0]\nprint(f\"Prompt: {undesirable_ex['prompt'][:120]}...\")\nprint(f\"Completion: {undesirable_ex['completion'][:200]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "Configure LoRA adapters for efficient fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank (higher = more parameters)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=128,  # LoRA scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"‚úì LoRA adapters configured\")\n",
    "print(f\"  Rank: 64\")\n",
    "print(f\"  Alpha: 128\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTO Training Configuration\n",
    "Set up KTO trainer to learn correct vs incorrect tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KTO Training Arguments\n",
    "training_args = KTOConfig(\n",
    "    output_dir=\"./kto_claudesidian_tools\",\n",
    "    \n",
    "    # Batch size configuration\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 32\n",
    "    \n",
    "    # KTO-specific parameters\n",
    "    beta=0.1,  # KTO beta (controls strength of preference optimization)\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=5e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Sequence lengths\n",
    "    max_length=4096,\n",
    "    max_prompt_length=2048,\n",
    "    \n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",  # Change to \"wandb\" for experiment tracking\n",
    ")\n",
    "\n",
    "# Initialize KTO Trainer\n",
    "kto_trainer = KTOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úì KTO trainer initialized\")\n",
    "print(f\"  Dataset: {len(train_dataset)} examples\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  KTO beta: {training_args.beta}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "Train the model to internalize Claudesidian vault tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show memory stats before training\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU: {gpu_stats.name}\")\n",
    "    print(f\"Max memory: {max_memory} GB\")\n",
    "    print(f\"Memory reserved: {start_gpu_memory} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting KTO training for tool calling...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trainer_output = kto_trainer.train()\n",
    "    print(\"\\n‚úì Training completed successfully!\")\n",
    "    print(f\"Final loss: {trainer_output.training_loss:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Training failed: {type(e).__name__}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Save the trained model and adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(\"claudesidian_tool_lora\")\n",
    "tokenizer.save_pretrained(\"claudesidian_tool_lora\")\n",
    "\n",
    "print(\"‚úì Model saved to ./claudesidian_tool_lora\")\n",
    "\n",
    "# Optional: Upload to HuggingFace Hub\n",
    "# Uncomment and configure:\n",
    "# HF_USERNAME = \"your_username\"\n",
    "# MODEL_NAME = \"claudesidian-tool-calling-qwen-1.5b\"\n",
    "# HF_TOKEN = \"hf_...\"\n",
    "# \n",
    "# model.push_to_hub_merged(\n",
    "#     f\"{HF_USERNAME}/{MODEL_NAME}\",\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\",\n",
    "#     token=HF_TOKEN\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing\n",
    "Test the trained model with tool calling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Set up for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    ")\n",
    "\n",
    "def test_tool_calling(user_message):\n",
    "    \"\"\"Generate tool call for a user request.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USER REQUEST:\")\n",
    "    print(\"=\"*60)\n",
    "    print(user_message)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"MODEL RESPONSE:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        streamer=text_streamer,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"‚úì Inference setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases covering different Claudesidian tools\n",
    "test_cases = [\n",
    "    # Content reading\n",
    "    \"Show me the contents of my project roadmap file.\",\n",
    "    \n",
    "    # Content modification\n",
    "    \"Add a header to my meeting notes saying 'Q1 2025 Planning'.\",\n",
    "    \n",
    "    # File operations\n",
    "    \"Delete the old draft file called 'temp-notes.md'.\",\n",
    "    \n",
    "    # Workspace operations\n",
    "    \"Switch to my 'Personal' workspace.\",\n",
    "    \n",
    "    # Agent operations\n",
    "    \"Turn on my Research Assistant agent.\",\n",
    "    \n",
    "    # Search operations\n",
    "    \"Find all notes that mention 'product launch'.\",\n",
    "    \n",
    "    # Folder operations\n",
    "    \"Create a new folder called 'Archive-2024'.\",\n",
    "]\n",
    "\n",
    "print(\"Testing tool calling with trained model...\\n\")\n",
    "for test_case in test_cases:\n",
    "    test_tool_calling(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trained a model using KTO to internalize Claudesidian vault tools:\n",
    "\n",
    "**Tools covered:**\n",
    "- `contentManager_readContent` - Read file contents\n",
    "- `contentManager_prependContent` - Add content to file start\n",
    "- `contentManager_appendContent` - Add content to file end\n",
    "- `vaultManager_deleteNote` - Delete files\n",
    "- `workspaceManager_switchWorkspace` - Switch workspaces\n",
    "- `agentManager_toggleAgent` - Enable/disable agents\n",
    "- `searchManager_search` - Search for notes\n",
    "- `folderManager_createFolder` - Create folders\n",
    "\n",
    "**Training approach:**\n",
    "- KTO learns from desirable (correct) vs undesirable (incorrect) tool usage\n",
    "- Model learns to use correct parameter names (e.g., `filePath` not `file`)\n",
    "- Model learns to include all required parameters\n",
    "- Model learns when to use which tool\n",
    "\n",
    "**Next steps:**\n",
    "1. Test the model with your actual Claudesidian application\n",
    "2. Collect more examples of edge cases\n",
    "3. Iterate and retrain for better performance\n",
    "4. Consider larger models (7B, 14B) for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}