{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Tool-Calling Refinement with KTO\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/kto_tool_calling_notebook.ipynb)\n\n## üéì What You'll Learn\n\nThis notebook teaches you how to **refine** a language model using KTO (Kahneman-Tversky Optimization) to prefer better tool-calling behaviors. By the end, you'll have:\n\n- **A refined AI model** that prefers correct tool usage over incorrect patterns\n- **Hands-on experience** with preference learning (KTO)\n- **Understanding** of how KTO differs from SFT\n\n## üî¨ What is KTO?\n\n**KTO (Kahneman-Tversky Optimization)** is a preference learning method:\n- You show the model pairs of good vs bad examples\n- The model learns to prefer the good patterns\n- Use KTO to **refine** an already-capable model\n\n**When to use KTO:**\n- ‚úÖ Refining existing tool-calling behavior\n- ‚úÖ Teaching preferences between good/bad outputs\n- ‚úÖ Improving behavioral patterns (error handling, verification, etc.)\n\n**Not for:**\n- ‚ùå Teaching tool-calling from scratch (use SFT instead)\n- ‚ùå Learning entirely new task formats (use SFT instead)\n\n## üíª Hardware Requirements\n\n**Recommended GPU:**\n- 1.5B-3B models: T4 (15GB VRAM) - ‚úÖ **Free Colab tier works!**\n- 7B models: T4 or better\n- 13B+ models: A100 (40GB VRAM) - Colab Pro\n\n**Training time:** ~30-45 minutes for a 1.5B model on T4\n\n## üìä Dataset\n\n- **Dataset**: claudesidian-synthetic-dataset v1.5\n- **File**: behavior_merged_kto_v1.5_11.29.25.jsonl\n- **Total Examples**: 4,268 (2,286 desirable, 1,982 undesirable)\n- **Ratio**: 1.15:1 (well-balanced for KTO)\n- **Behaviors**: 12 categories including error_recovery, intellectual_humility, strategic_tool_selection, verification_before_action, and more\n- **Format**: OpenAI-compatible tool calling with KTO interleaving"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Install Unsloth for faster training\n%%capture\n!pip install unsloth\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Install Transformers 5 from special branch (required for Ministral 3)\n!pip install git+https://github.com/huggingface/transformers.git@bf3f0ae70d0e902efab4b8517fce88f6697636ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Install training dependencies\n%%capture\n# TRL 0.22.2 is required for Transformers 5 + Unsloth compatibility\n!pip install --no-deps trl==0.22.2\n!pip install \"datasets>=2.14.0\"\n!pip install -U accelerate bitsandbytes\n!pip install -U peft xformers triton\n# Reinstall unsloth_zoo for FastVisionModel support (Ministral 3)\n!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to Google Drive so they persist if runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/KTO_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Google Drive mounted\")\n",
    "print(f\"‚úì Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets:\n",
    "1. Click the üîë key icon in the left sidebar\n",
    "2. Add new secret: `HF_TOKEN`\n",
    "3. Get token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Get your HuggingFace username automatically\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"‚úì HuggingFace token loaded\")\n",
    "print(f\"‚úì Username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "**What this does:** Choose the base model you want to refine and configure basic settings.\n",
    "\n",
    "For KTO, you're typically starting with a model that already has some capability‚Äîyou're teaching it to prefer better patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# @title ‚öôÔ∏è Model & Dataset Configuration\n# @markdown Use the dropdowns to select your model and configure your dataset.\n\n# @markdown ### üß† Base Model Selection\n# @markdown Choose a model based on your VRAM availability. Smaller models recommended for KTO.\n# @markdown * **1B-3B:** Fast, runs on any GPU (recommended for free Colab)\n# @markdown * **7B-9B:** Standard balance of speed/intelligence\n# @markdown * **12B+:** High intelligence, requires A100\n# @markdown * **Ministral 3 (NEW):** Latest Mistral model with vision capabilities (3B/8B/14B)\nMODEL_NAME = \"unsloth/Ministral-3-8B-Instruct-2512\" # @param [\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit\", \"unsloth/gemma-2-2b-it-bnb-4bit\", \"unsloth/Ministral-3-3B-Instruct-2512\", \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", \"unsloth/Phi-3.5-mini-instruct\", \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/Ministral-3-8B-Instruct-2512\", \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \"unsloth/Ministral-3-14B-Instruct-2512\"]\n\n# @markdown ### üìè Max Output Length\n# @markdown 4096 recommended for tool calling. Higher values require more VRAM.\nMAX_SEQ_LENGTH = 4096 # @param [1024, 2048, 4096, 8192] {type:\"raw\"}\n\n# @markdown ### üìö Dataset Configuration\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\" # @param {type:\"string\"}\nDATASET_FILE = \"behavior_merged_kto_v1.5_11.29.25.jsonl\" # @param {type:\"string\"}\n\n# @markdown ### üè∑Ô∏è Output Model Name\n# @markdown Name your refined model (e.g., `my-tool-model-kto-v1`).\nOUTPUT_MODEL_NAME = \"nexus-tools-kto\" # @param {type:\"string\"}\n\n# Detect if this is a Ministral 3 model (requires FastVisionModel)\nIS_MINISTRAL = \"ministral\" in MODEL_NAME.lower()\n\nprint(f\"‚úì Configuration set:\")\nprint(f\"  ‚Ä¢ Model: {MODEL_NAME}\")\nprint(f\"  ‚Ä¢ Context: {MAX_SEQ_LENGTH}\")\nprint(f\"  ‚Ä¢ Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"  ‚Ä¢ Output: {OUTPUT_MODEL_NAME}\")\nif IS_MINISTRAL:\n    print(f\"  ‚Ä¢ Note: Ministral 3 detected - will use FastVisionModel\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "**What this does:** Downloads the base model and prepares it for training.\n",
    "\n",
    "The model is the \"brain\" that will learn to prefer correct tool-calling patterns. We use 4-bit quantization to fit large models into limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU and store info for lineage\n",
    "GPU_NAME = torch.cuda.get_device_name(0)\n",
    "CUDA_VERSION = torch.version.cuda\n",
    "GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "print(f\"Using GPU: {GPU_NAME}\")\n",
    "print(f\"CUDA version: {CUDA_VERSION}\")\n",
    "print(f\"Available VRAM: {GPU_MEMORY_GB:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Load the base model and tokenizer from HuggingFace\n# This downloads the model weights\n#\n# Parameters explained:\n#   model_name: Which model to download\n#   max_seq_length: Max tokens model can process at once\n#   dtype=None: Auto-detect best precision for your GPU\n#   load_in_4bit=True: Use 4-bit quantization to save memory\n#   token: Your HF token for accessing the model\n\n# Use FastVisionModel for Ministral 3, FastLanguageModel for others\nif IS_MINISTRAL:\n    from unsloth import FastVisionModel\n    model, tokenizer = FastVisionModel.from_pretrained(\n        model_name=MODEL_NAME,\n        max_seq_length=MAX_SEQ_LENGTH,\n        dtype=None,  # Auto-detect (usually bfloat16 or float16)\n        load_in_4bit=True,  # Reduces memory usage by ~75%\n        token=HF_TOKEN,\n    )\n    ModelClass = FastVisionModel\nelse:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=MODEL_NAME,\n        max_seq_length=MAX_SEQ_LENGTH,\n        dtype=None,  # Auto-detect (usually bfloat16 or float16)\n        load_in_4bit=True,  # Reduces memory usage by ~75%\n        token=HF_TOKEN,\n    )\n    ModelClass = FastLanguageModel\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# CRITICAL: Apply chat template BEFORE training\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# This ensures the special tokens (<|im_start|>, <|im_end|>) are properly\n# handled as single tokens, not as literal text strings.\n# Without this, the model will output \"<|im_start|>assistant\" as text!\n\nfrom unsloth.chat_templates import get_chat_template\n\n# Detect the correct chat template based on model name\nif \"qwen\" in MODEL_NAME.lower():\n    chat_template_name = \"chatml\"  # Qwen uses ChatML format\nelif \"llama\" in MODEL_NAME.lower():\n    chat_template_name = \"llama-3\"\nelif \"mistral\" in MODEL_NAME.lower() or \"ministral\" in MODEL_NAME.lower():\n    chat_template_name = \"mistral\"\nelif \"gemma\" in MODEL_NAME.lower():\n    chat_template_name = \"gemma\"\nelif \"phi\" in MODEL_NAME.lower():\n    chat_template_name = \"phi-3\"\nelse:\n    chat_template_name = \"chatml\"  # Default fallback\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template=chat_template_name,\n)\n\nprint(f\"Applied {chat_template_name} chat template\")\n\n# Store for lineage\nTOTAL_PARAMS = sum(p.numel() for p in model.parameters())\nCHAT_TEMPLATE = chat_template_name\n\nprint(\"Model loaded successfully\")\nprint(f\"  Model has {TOTAL_PARAMS:,} parameters\")\nprint(f\"  Chat template: {chat_template_name}\")\nif IS_MINISTRAL:\n    print(f\"  Using: FastVisionModel (Ministral 3)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA Adapters\n",
    "\n",
    "**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n",
    "\n",
    "Think of LoRA like teaching a new preference through muscle memory‚Äîwe add small specialized layers that learn to prefer better behaviors, while keeping the main \"brain\" frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# @title üîß LoRA Adapter Configuration\n# @markdown Configure the size and strength of the fine-tuning adapters.\n\n# @markdown ### üéõÔ∏è LoRA Parameters\n# @markdown **Rank (r):** Higher = more capacity but slower/more memory (Standard: 32-64 for KTO).\n# @markdown Alpha will be automatically set to 2 * r.\nLORA_R = 64 # @param [16, 32, 64, 128] {type:\"raw\"}\n\nLORA_ALPHA = LORA_R * 2\n\n# @markdown **Dropout:** Helps prevent overfitting (Standard: 0.05).\nLORA_DROPOUT = 0.05 # @param {type:\"number\"}\n\n# @markdown **Random Seed:** Change this for different initialization.\nRANDOM_STATE = 3407 # @param {type:\"integer\"}\n\n# Target modules for LoRA\nTARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n\n# Use the correct model class (FastVisionModel for Ministral, FastLanguageModel for others)\nmodel = ModelClass.get_peft_model(\n    model,\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    target_modules=TARGET_MODULES,\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=RANDOM_STATE,\n)\n\n# Store for lineage\nTRAINABLE_PARAMS = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"‚úì LoRA adapters applied:\")\nprint(f\"  ‚Ä¢ Rank: {LORA_R}\")\nprint(f\"  ‚Ä¢ Alpha: {LORA_ALPHA}\")\nprint(f\"  ‚Ä¢ Dropout: {LORA_DROPOUT}\")\nprint(f\"  ‚Ä¢ Trainable params: {TRAINABLE_PARAMS:,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Dataset\n",
    "\n",
    "**What this does:** Downloads training examples and formats them for KTO.\n",
    "\n",
    "The dataset contains ~1,850 examples of **both good AND bad** tool-calling behavior. KTO learns by comparing:\n",
    "- ‚úÖ **Desirable examples** (label=true): Correct tool usage\n",
    "- ‚ùå **Undesirable examples** (label=false): Common mistakes\n",
    "\n",
    "This is the key difference from SFT‚ÄîKTO learns *preferences*, not just *patterns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset, Dataset\n\n\"\"\"\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nLOAD DATASET FROM HUGGINGFACE\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nThis downloads pre-made training examples with BOTH good and bad tool-calling.\nEach example shows: user request ‚Üí tool call (correct OR incorrect)\n\"\"\"\n\nprint(f\"Loading dataset: {DATASET_NAME}/{DATASET_FILE}\")\nraw_dataset = load_dataset(\n    DATASET_NAME,\n    data_files=DATASET_FILE,\n    split=\"train\"\n)\n\n# Store dataset info for lineage\nDATASET_SIZE = len(raw_dataset)\n\n# Try to get behaviors list from first few examples\nBEHAVIORS = list(set(ex.get('behavior', 'general') for ex in raw_dataset[:100] if 'behavior' in ex))\nif not BEHAVIORS:\n    BEHAVIORS = ['general']\n\nprint(f\"‚úì Loaded {DATASET_SIZE} examples\")\nprint(f\"  Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"  Behaviors: {len(BEHAVIORS)} categories\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nCONVERT TO KTO FORMAT\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nKTO expects:\n  - prompt: The user request\n  - completion: The model's response (tool call)\n  - label: True (desirable) or False (undesirable)\n\"\"\"\n\ndef convert_to_kto_format(example):\n    \"\"\"\n    Convert tool calling conversations to KTO format.\n    \n    KTO learns by comparing desirable vs undesirable completions\n    for the same type of prompt.\n    \"\"\"\n    conversations = example[\"conversations\"]\n    behavior = example.get(\"behavior\", \"general\")\n    \n    # Extract user and assistant messages\n    user_msg = None\n    assistant_msg = None\n    \n    for msg in conversations:\n        if msg[\"role\"] == \"user\":\n            user_msg = msg[\"content\"]\n        elif msg[\"role\"] == \"assistant\":\n            # Handle OpenAI tool calling format\n            if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n                tool_calls_str = []\n                for tc in msg[\"tool_calls\"]:\n                    func = tc[\"function\"]\n                    tool_calls_str.append(\n                        f\"tool_call: {func['name']}\\n\"\n                        f\"arguments: {func['arguments']}\"\n                    )\n                assistant_msg = \"\\n\\n\".join(tool_calls_str)\n            else:\n                assistant_msg = msg.get(\"content\", \"\")\n    \n    if user_msg is None or assistant_msg is None:\n        return None\n    \n    return {\n        \"prompt\": user_msg,\n        \"completion\": assistant_msg,\n        \"label\": example[\"label\"]\n    }\n\n# Process the dataset (raw_dataset is already the train split)\nkto_data = []\nskipped = 0\nfor example in raw_dataset:\n    processed = convert_to_kto_format(example)\n    if processed:\n        kto_data.append(processed)\n    else:\n        skipped += 1\n\n# Create HuggingFace Dataset\ntrain_dataset = Dataset.from_dict({\n    \"prompt\": [ex[\"prompt\"] for ex in kto_data],\n    \"completion\": [ex[\"completion\"] for ex in kto_data],\n    \"label\": [ex[\"label\"] for ex in kto_data],\n})\n\n# Calculate and store statistics for lineage\nDESIRABLE_COUNT = sum(train_dataset[\"label\"])\nUNDESIRABLE_COUNT = len(train_dataset) - DESIRABLE_COUNT\nDATASET_PROCESSED_SIZE = len(train_dataset)\n\nprint(f\"‚úì KTO Dataset prepared:\")\nprint(f\"  ‚Ä¢ Total examples: {DATASET_PROCESSED_SIZE}\")\nprint(f\"  ‚Ä¢ Desirable (good): {DESIRABLE_COUNT} ‚úÖ\")\nprint(f\"  ‚Ä¢ Undesirable (bad): {UNDESIRABLE_COUNT} ‚ùå\")\nprint(f\"  ‚Ä¢ Ratio: {DESIRABLE_COUNT/UNDESIRABLE_COUNT:.2f}:1\")\nif skipped > 0:\n    print(f\"  ‚Ä¢ Skipped: {skipped}\")\n\nprint(f\"\\nüìù Sample desirable example:\")\ndesirable_ex = [ex for ex in kto_data if ex[\"label\"]][0]\nprint(f\"Prompt: {desirable_ex['prompt'][:100]}...\")\nprint(f\"Completion: {desirable_ex['completion'][:150]}...\")\n\nprint(f\"\\nüìù Sample undesirable example:\")\nundesirable_ex = [ex for ex in kto_data if not ex[\"label\"]][0]\nprint(f\"Prompt: {undesirable_ex['prompt'][:100]}...\")\nprint(f\"Completion: {undesirable_ex['completion'][:150]}...\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "**What this does:** Set the hyperparameters that control how the model learns preferences.\n",
    "\n",
    "KTO has a key parameter called **beta** that controls how strongly the model should prefer desirable outputs. Higher beta = stronger preference learning.\n",
    "\n",
    "**Key differences from SFT:**\n",
    "- **Lower learning rate** (5e-6 vs 2e-4): Preference learning is more subtle\n",
    "- **Beta parameter**: Controls KTO's preference strength\n",
    "- **Desirable/Undesirable weights**: Balance the learning signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import KTOConfig, KTOTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped output directory\n",
    "TRAINING_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{DRIVE_OUTPUT_DIR}/{TRAINING_TIMESTAMP}\"\n",
    "\n",
    "# @title üèÉ Training Hyperparameters\n",
    "# @markdown Control the speed and quality of training.\n",
    "\n",
    "# @markdown ### ‚ö° Performance Settings\n",
    "# @markdown **Batch Size:** Examples per step. Lower if you run out of memory.\n",
    "BATCH_SIZE = 4 # @param [1, 2, 4, 6, 8] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Gradient Accumulation:** Simulates larger batches.\n",
    "GRADIENT_ACCUMULATION = 8 # @param [2, 4, 6, 8, 12, 16] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### üß† KTO-Specific Parameters\n",
    "# @markdown **Beta:** Controls preference strength. Higher = stronger preferences.\n",
    "# @markdown * **0.05-0.1:** Gentle preference learning\n",
    "# @markdown * **0.1-0.3:** Standard (recommended)\n",
    "# @markdown * **0.3-0.5:** Strong preference enforcement\n",
    "KTO_BETA = 0.1 # @param [0.05, 0.1, 0.2, 0.3, 0.5] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Desirable Weight:** Weight for positive examples.\n",
    "DESIRABLE_WEIGHT = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown **Undesirable Weight:** Weight for negative examples.\n",
    "UNDESIRABLE_WEIGHT = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown ### üìö Learning Rate Configuration\n",
    "# @markdown KTO uses lower learning rates than SFT (preference learning is subtle).\n",
    "# @markdown * **5e-6:** Standard for KTO (recommended)\n",
    "# @markdown * **1e-6:** Very conservative\n",
    "# @markdown * **1e-5:** Faster but riskier\n",
    "LEARNING_RATE_EXPONENT = 6 # @param [5, 6, 7] {type:\"raw\"}\n",
    "LEARNING_RATE_MULTIPLIER = 5 # @param [1, 2, 3, 5] {type:\"raw\"}\n",
    "\n",
    "LEARNING_RATE = LEARNING_RATE_MULTIPLIER * (10 ** -LEARNING_RATE_EXPONENT)\n",
    "\n",
    "# @markdown ### üîÑ Epochs\n",
    "# @markdown Number of passes through the dataset.\n",
    "NUM_EPOCHS = 3 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### üíæ Saving & Logging\n",
    "SAVE_STEPS = 100 # @param {type:\"integer\"}\n",
    "LOGGING_STEPS = 10 # @param {type:\"integer\"}\n",
    "\n",
    "# Other training settings\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LR_SCHEDULER = \"cosine\"\n",
    "OPTIMIZER = \"adamw_8bit\"\n",
    "USE_BF16 = is_bfloat16_supported()\n",
    "\n",
    "training_args = KTOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    # KTO-specific\n",
    "    beta=KTO_BETA,\n",
    "    desirable_weight=DESIRABLE_WEIGHT,\n",
    "    undesirable_weight=UNDESIRABLE_WEIGHT,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    \n",
    "    # Sequence lengths\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    max_prompt_length=MAX_SEQ_LENGTH // 2,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    \n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    optim=OPTIMIZER,\n",
    "    fp16=not USE_BF16,\n",
    "    bf16=USE_BF16,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    seed=RANDOM_STATE,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Calculate effective batch size\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "\n",
    "print(\"‚úì Training configuration ready\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Gradient Accum.: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  ‚Ä¢ Effective Batch: {EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {LEARNING_RATE} ({LEARNING_RATE_MULTIPLIER}e-{LEARNING_RATE_EXPONENT})\")\n",
    "print(f\"  ‚Ä¢ KTO Beta: {KTO_BETA}\")\n",
    "print(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Output Dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer\n",
    "\n",
    "**What this does:** Creates the KTO training engine that coordinates everything.\n",
    "\n",
    "The KTOTrainer is similar to SFTTrainer but optimizes for *preferences* rather than just *imitation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KTOTrainer\n",
    "# This combines the model, dataset, and configuration into one training pipeline\n",
    "\n",
    "kto_trainer = KTOTrainer(\n",
    "    model=model,  # The model with LoRA adapters\n",
    "    args=training_args,  # All the hyperparameters we configured\n",
    "    processing_class=tokenizer,  # For converting text to tokens\n",
    "    train_dataset=train_dataset,  # Our formatted training examples\n",
    ")\n",
    "\n",
    "print(\"‚úì KTO Trainer initialized\")\n",
    "print(\"  Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 10. Train!\n",
    "\n",
    "**What this does:** The actual preference learning happens here!\n",
    "\n",
    "The model will:\n",
    "1. **Read examples** of good AND bad tool usage\n",
    "2. **Compare** desirable vs undesirable completions\n",
    "3. **Update weights** to prefer the desirable patterns\n",
    "4. **Repeat** this process for 3 epochs\n",
    "\n",
    "**What to expect:**\n",
    "- Training takes ~30-45 minutes for 1.5B models on T4 GPU\n",
    "- You'll see progress updates every 10 steps\n",
    "- Loss should generally decrease over time\n",
    "- Checkpoints are saved every 100 steps to Google Drive\n",
    "\n",
    "**What the metrics mean:**\n",
    "- **Loss:** How well the model separates good from bad (lower = better)\n",
    "- **Learning Rate:** Gradually decreases as training progresses\n",
    "- **Rewards/chosen:** How much the model prefers good examples (higher = better)\n",
    "- **Rewards/rejected:** How much the model avoids bad examples (lower = better)\n",
    "\n",
    "**üíæ Checkpoint Resumption:**\n",
    "If your Colab session disconnects, your checkpoints are saved to Google Drive. Re-run the setup cells and training will automatically resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üîç Check for existing checkpoints (automatic resumption)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Found checkpoints - get the latest one\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = latest_checkpoint\n",
    "    print(f\"‚úì Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"  Resuming training from this checkpoint\")\n",
    "    print(f\"  Total checkpoints found: {len(checkpoint_dirs)}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No existing checkpoints found - starting fresh training\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Start training (or resume)\n",
    "print(\"=\" * 60)\n",
    "if resume_from_checkpoint:\n",
    "    print(\"RESUMING KTO TRAINING FROM CHECKPOINT\")\n",
    "else:\n",
    "    print(\"STARTING KTO TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Track training time\n",
    "training_start_time = time.time()\n",
    "\n",
    "trainer_stats = kto_trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "# Calculate training duration\n",
    "TRAINING_DURATION_SECONDS = time.time() - training_start_time\n",
    "TRAINING_DURATION_MINUTES = TRAINING_DURATION_SECONDS / 60\n",
    "\n",
    "# Store final metrics for lineage\n",
    "FINAL_LOSS = trainer_stats.training_loss\n",
    "TOTAL_STEPS = trainer_stats.global_step\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final loss: {FINAL_LOSS:.4f}\")\n",
    "print(f\"Total steps: {TOTAL_STEPS}\")\n",
    "print(f\"Training time: {TRAINING_DURATION_MINUTES:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 11. Build Training Lineage\n",
    "\n",
    "**What this does:** Captures all training metadata for reproducibility and analysis.\n",
    "\n",
    "This creates a complete record of:\n",
    "- Base model and configuration\n",
    "- Dataset details and statistics\n",
    "- All hyperparameters used\n",
    "- Training results and metrics\n",
    "- Hardware and environment info\n",
    "\n",
    "This information will be automatically added to your HuggingFace model card!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datetime import datetime\n\n\"\"\"\nBUILD COMPLETE TRAINING LINEAGE\n\nThis dictionary captures EVERYTHING about the training run for:\n- Reproducibility\n- Model card generation\n- Experiment tracking\n- Analysis and comparison\n\"\"\"\n\nTRAINING_LINEAGE = {\n    # IDENTIFICATION\n    \"model_name\": OUTPUT_MODEL_NAME,\n    \"training_method\": \"KTO\",\n    \"training_timestamp\": TRAINING_TIMESTAMP,\n    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \n    # BASE MODEL\n    \"base_model\": {\n        \"name\": MODEL_NAME,\n        \"total_parameters\": TOTAL_PARAMS,\n        \"quantization\": \"4-bit\",\n        \"max_seq_length\": MAX_SEQ_LENGTH,\n        \"chat_template\": CHAT_TEMPLATE,  # Added chat template tracking\n    },\n    \n    # LORA CONFIGURATION\n    \"lora_config\": {\n        \"r\": LORA_R,\n        \"alpha\": LORA_ALPHA,\n        \"dropout\": LORA_DROPOUT,\n        \"target_modules\": TARGET_MODULES,\n        \"trainable_parameters\": TRAINABLE_PARAMS,\n        \"trainable_percentage\": round(TRAINABLE_PARAMS / TOTAL_PARAMS * 100, 4),\n    },\n    \n    # DATASET\n    \"dataset\": {\n        \"name\": DATASET_NAME,\n        \"huggingface_url\": f\"https://huggingface.co/datasets/{DATASET_NAME}\",\n        \"total_examples\": DATASET_PROCESSED_SIZE,\n        \"desirable_examples\": DESIRABLE_COUNT,\n        \"undesirable_examples\": UNDESIRABLE_COUNT,\n        \"desirable_ratio\": round(DESIRABLE_COUNT / UNDESIRABLE_COUNT, 2),\n        \"behaviors\": sorted(BEHAVIORS),\n    },\n    \n    # KTO HYPERPARAMETERS\n    \"kto_config\": {\n        \"beta\": KTO_BETA,\n        \"desirable_weight\": DESIRABLE_WEIGHT,\n        \"undesirable_weight\": UNDESIRABLE_WEIGHT,\n    },\n    \n    # TRAINING HYPERPARAMETERS\n    \"training_config\": {\n        \"batch_size\": BATCH_SIZE,\n        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n        \"effective_batch_size\": EFFECTIVE_BATCH_SIZE,\n        \"learning_rate\": LEARNING_RATE,\n        \"learning_rate_scheduler\": LR_SCHEDULER,\n        \"warmup_ratio\": WARMUP_RATIO,\n        \"max_grad_norm\": MAX_GRAD_NORM,\n        \"num_epochs\": NUM_EPOCHS,\n        \"optimizer\": OPTIMIZER,\n        \"precision\": \"bf16\" if USE_BF16 else \"fp16\",\n        \"gradient_checkpointing\": True,\n        \"random_seed\": RANDOM_STATE,\n    },\n    \n    # TRAINING RESULTS\n    \"training_results\": {\n        \"final_loss\": round(FINAL_LOSS, 4),\n        \"total_steps\": TOTAL_STEPS,\n        \"training_duration_minutes\": round(TRAINING_DURATION_MINUTES, 1),\n    },\n    \n    # HARDWARE & ENVIRONMENT\n    \"hardware\": {\n        \"gpu\": GPU_NAME,\n        \"gpu_memory_gb\": round(GPU_MEMORY_GB, 1),\n        \"cuda_version\": CUDA_VERSION,\n        \"platform\": \"Google Colab\",\n    },\n    \n    # FRAMEWORK VERSIONS\n    \"framework_versions\": {\n        \"torch\": torch.__version__,\n        \"transformers\": __import__(\"transformers\").__version__,\n        \"trl\": __import__(\"trl\").__version__,\n        \"peft\": __import__(\"peft\").__version__,\n        \"unsloth\": \"latest\",\n    },\n}\n\n# Save lineage to file\nlineage_path = f\"{output_dir}/training_lineage.json\"\nwith open(lineage_path, \"w\") as f:\n    json.dump(TRAINING_LINEAGE, f, indent=2)\n\nprint(\"Training lineage captured!\")\nprint(f\"  Saved to: {lineage_path}\")\nprint()\nprint(\"Summary:\")\nprint(f\"  Base Model: {MODEL_NAME}\")\nprint(f\"  Chat Template: {CHAT_TEMPLATE}\")\nprint(f\"  Dataset: {DATASET_NAME} ({DATASET_PROCESSED_SIZE} examples)\")\nprint(f\"  Method: KTO (beta={KTO_BETA})\")\nprint(f\"  LoRA: r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"  LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}\")\nprint(f\"  Final Loss: {FINAL_LOSS:.4f}\")\nprint(f\"  Duration: {TRAINING_DURATION_MINUTES:.1f} min on {GPU_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 12. Upload to HuggingFace\n",
    "\n",
    "**What this does:** Share your refined model with the world!\n",
    "\n",
    "The model card will be **automatically generated** with all your training details:\n",
    "- Base model and configuration\n",
    "- Dataset information\n",
    "- All hyperparameters\n",
    "- Training results\n",
    "- Hardware used\n",
    "\n",
    "We'll create **three versions** of your model:\n",
    "\n",
    "1. **LoRA adapters** - Small files that contain just the preference changes\n",
    "2. **Merged 16-bit model** - Full model with adapters merged in\n",
    "3. **GGUF quantizations** - Optimized versions for CPU/GPU inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(lineage: dict, hf_username: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive HuggingFace model card from training lineage.\n",
    "    \n",
    "    This creates a professional README.md with all training details\n",
    "    for reproducibility and transparency.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_model = lineage[\"base_model\"][\"name\"]\n",
    "    dataset = lineage[\"dataset\"]\n",
    "    lora = lineage[\"lora_config\"]\n",
    "    kto = lineage[\"kto_config\"]\n",
    "    training = lineage[\"training_config\"]\n",
    "    results = lineage[\"training_results\"]\n",
    "    hardware = lineage[\"hardware\"]\n",
    "    frameworks = lineage[\"framework_versions\"]\n",
    "    \n",
    "    # Format behaviors list\n",
    "    behaviors_list = \"\\n\".join([f\"  - {b}\" for b in dataset[\"behaviors\"]])\n",
    "    \n",
    "    model_card = f'''---\n",
    "language:\n",
    "- en\n",
    "license: apache-2.0\n",
    "library_name: transformers\n",
    "tags:\n",
    "- tool-calling\n",
    "- kto\n",
    "- preference-learning\n",
    "- claudesidian\n",
    "- obsidian\n",
    "- fine-tuned\n",
    "- unsloth\n",
    "base_model: {base_model}\n",
    "datasets:\n",
    "- {dataset[\"name\"]}\n",
    "pipeline_tag: text-generation\n",
    "model-index:\n",
    "- name: {lineage[\"model_name\"]}\n",
    "  results:\n",
    "  - task:\n",
    "      type: text-generation\n",
    "    metrics:\n",
    "    - name: Final Loss\n",
    "      type: loss\n",
    "      value: {results[\"final_loss\"]}\n",
    "---\n",
    "\n",
    "# {lineage[\"model_name\"]}\n",
    "\n",
    "This model was fine-tuned using **KTO (Kahneman-Tversky Optimization)** to improve tool-calling behavior for the Claudesidian vault application.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Base Model:** [{base_model}](https://huggingface.co/{base_model})\n",
    "- **Training Method:** KTO (Preference Learning)\n",
    "- **Task:** Tool-calling for Obsidian vault operations\n",
    "- **Training Date:** {lineage[\"training_date\"]}\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Dataset\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Dataset | [{dataset[\"name\"]}]({dataset[\"huggingface_url\"]}) |\n",
    "| Total Examples | {dataset[\"total_examples\"]:,} |\n",
    "| Desirable (‚úÖ) | {dataset[\"desirable_examples\"]:,} |\n",
    "| Undesirable (‚ùå) | {dataset[\"undesirable_examples\"]:,} |\n",
    "| Ratio | {dataset[\"desirable_ratio\"]}:1 |\n",
    "\n",
    "**Behaviors trained:**\n",
    "{behaviors_list}\n",
    "\n",
    "### KTO Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Beta (Œ≤) | {kto[\"beta\"]} |\n",
    "| Desirable Weight | {kto[\"desirable_weight\"]} |\n",
    "| Undesirable Weight | {kto[\"undesirable_weight\"]} |\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Rank (r) | {lora[\"r\"]} |\n",
    "| Alpha (Œ±) | {lora[\"alpha\"]} |\n",
    "| Dropout | {lora[\"dropout\"]} |\n",
    "| Target Modules | {', '.join(lora[\"target_modules\"])} |\n",
    "| Trainable Parameters | {lora[\"trainable_parameters\"]:,} ({lora[\"trainable_percentage\"]}%) |\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Batch Size | {training[\"batch_size\"]} |\n",
    "| Gradient Accumulation | {training[\"gradient_accumulation_steps\"]} |\n",
    "| Effective Batch Size | {training[\"effective_batch_size\"]} |\n",
    "| Learning Rate | {training[\"learning_rate\"]} |\n",
    "| LR Scheduler | {training[\"learning_rate_scheduler\"]} |\n",
    "| Warmup Ratio | {training[\"warmup_ratio\"]} |\n",
    "| Max Grad Norm | {training[\"max_grad_norm\"]} |\n",
    "| Epochs | {training[\"num_epochs\"]} |\n",
    "| Optimizer | {training[\"optimizer\"]} |\n",
    "| Precision | {training[\"precision\"]} |\n",
    "| Random Seed | {training[\"random_seed\"]} |\n",
    "\n",
    "### Training Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Final Loss | {results[\"final_loss\"]} |\n",
    "| Total Steps | {results[\"total_steps\"]:,} |\n",
    "| Training Duration | {results[\"training_duration_minutes\"]} minutes |\n",
    "\n",
    "### Hardware\n",
    "\n",
    "| Component | Value |\n",
    "|-----------|-------|\n",
    "| GPU | {hardware[\"gpu\"]} |\n",
    "| GPU Memory | {hardware[\"gpu_memory_gb\"]} GB |\n",
    "| CUDA Version | {hardware[\"cuda_version\"]} |\n",
    "| Platform | {hardware[\"platform\"]} |\n",
    "\n",
    "### Framework Versions\n",
    "\n",
    "| Library | Version |\n",
    "|---------|--------|\n",
    "| PyTorch | {frameworks[\"torch\"]} |\n",
    "| Transformers | {frameworks[\"transformers\"]} |\n",
    "| TRL | {frameworks[\"trl\"]} |\n",
    "| PEFT | {frameworks[\"peft\"]} |\n",
    "| Unsloth | {frameworks[\"unsloth\"]} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "### With Transformers\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{hf_username}/{lineage[\"model_name\"]}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{hf_username}/{lineage[\"model_name\"]}\")\n",
    "\n",
    "# Example tool-calling prompt\n",
    "messages = [{{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Show me the contents of my project roadmap file.\"\n",
    "}}]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "### With Ollama (GGUF)\n",
    "\n",
    "```bash\n",
    "# Download the GGUF version\n",
    "ollama pull {hf_username}/{lineage[\"model_name\"]}\n",
    "\n",
    "# Run inference\n",
    "ollama run {hf_username}/{lineage[\"model_name\"]}\n",
    "```\n",
    "\n",
    "### With LM Studio\n",
    "\n",
    "1. Open LM Studio ‚Üí \"Discover\" tab\n",
    "2. Search for `{hf_username}/{lineage[\"model_name\"]}`\n",
    "3. Download the Q4_K_M or Q5_K_M GGUF version\n",
    "4. Load and test with tool-calling prompts\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is designed for:\n",
    "- Tool-calling in Obsidian vault management applications\n",
    "- Claudesidian MCP integration\n",
    "- Local AI assistants that interact with note-taking systems\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Trained specifically for Claudesidian tool schemas\n",
    "- May not generalize to other tool-calling formats\n",
    "- Best performance with the specific tool set it was trained on\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{{lineage[\"model_name\"].replace(\"-\", \"_\")},\n",
    "  author = {{{hf_username}}},\n",
    "  title = {{{lineage[\"model_name\"]}: KTO Fine-tuned Tool-Calling Model}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{HuggingFace}},\n",
    "  url = {{https://huggingface.co/{hf_username}/{lineage[\"model_name\"]}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Training Lineage\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand full training configuration (JSON)</summary>\n",
    "\n",
    "```json\n",
    "{json.dumps(lineage, indent=2)}\n",
    "```\n",
    "\n",
    "</details>\n",
    "'''\n",
    "    \n",
    "    return model_card\n",
    "\n",
    "# Generate model card\n",
    "MODEL_CARD = generate_model_card(TRAINING_LINEAGE, hf_user)\n",
    "\n",
    "# Save model card locally\n",
    "model_card_path = f\"{output_dir}/README.md\"\n",
    "with open(model_card_path, \"w\") as f:\n",
    "    f.write(MODEL_CARD)\n",
    "\n",
    "print(\"‚úì Model card generated!\")\n",
    "print(f\"  Saved to: {model_card_path}\")\n",
    "print()\n",
    "print(\"Preview (first 50 lines):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\".join(MODEL_CARD.split(\"\\n\")[:50]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "\n",
    "# Upload LoRA adapters with model card\n",
    "print(\"Uploading LoRA adapters...\")\n",
    "\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload model card (README.md)\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card_path,\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Upload training lineage JSON\n",
    "api.upload_file(\n",
    "    path_or_fileobj=lineage_path,\n",
    "    path_in_repo=\"training_lineage.json\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"‚úì Model card with full lineage uploaded\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model with model card\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload model card to merged repo too\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card_path,\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=lineage_path,\n",
    "    path_in_repo=\"training_lineage.json\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Merged model uploaded to HuggingFace\")\n",
    "print(f\"‚úì Model card with full lineage uploaded\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GGUF quantizations\n",
    "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    "\n",
    "print(\"Creating GGUF quantizations...\")\n",
    "print(f\"This will create {len(quantization_methods)} versions\")\n",
    "print()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_methods,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"‚úì GGUF quantizations created and uploaded!\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjtani2aibn",
   "source": "## 13. Evaluate Model (Optional)\n\n**What this does:** Run automated tests to measure your model's tool-calling accuracy.\n\nThis will:\n- Load your trained model with vLLM for fast inference\n- Run test prompts covering all 47 tools\n- Calculate pass rates by category\n- Generate evaluation lineage that can be added to your model card\n\n**Skip this section** if you want to evaluate later using the standalone evaluation notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qldi0qw47lo",
   "source": "# @title Run Evaluation (Optional)\n# @markdown Test your refined model's tool-calling accuracy and behavioral patterns.\n\n# @markdown ### Enable Evaluation\nrun_evaluation = True # @param {type:\"boolean\"}\n\n# @markdown ### Test Suite Selection\n# @markdown * **Tool Coverage (47 tools):** Tests each tool individually\n# @markdown * **Behavioral Patterns (24 tests):** Tests context efficiency, executePrompt delegation, etc.\n# @markdown * **All Suites:** Runs both tool coverage and behavioral patterns\neval_test_suite = \"All Suites\" # @param [\"Tool Coverage (47 tools)\", \"Behavioral Patterns (24 tests)\", \"All Suites\"]\n\nif run_evaluation:\n    print(\"Installing vLLM for evaluation...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"-q\", \"vllm>=0.6.0\"], check=True)\n    \n    # Download evaluator framework\n    import requests\n    from pathlib import Path\n    \n    os.makedirs(\"Evaluator/prompts\", exist_ok=True)\n    os.makedirs(\"Evaluator/results\", exist_ok=True)\n    os.makedirs(\"tools\", exist_ok=True)\n    \n    REPO_BASE = \"https://raw.githubusercontent.com/ProfSynapse/Toolset-Training/main\"\n    \n    eval_files = {\n        \"Evaluator/__init__.py\": \"Evaluator/__init__.py\",\n        \"Evaluator/runner.py\": \"Evaluator/runner.py\",\n        \"Evaluator/schema_validator.py\": \"Evaluator/schema_validator.py\",\n        \"Evaluator/prompt_sets.py\": \"Evaluator/prompt_sets.py\",\n        \"Evaluator/reporting.py\": \"Evaluator/reporting.py\",\n        \"Evaluator/config.py\": \"Evaluator/config.py\",\n        \"Evaluator/prompts/tool_prompts.json\": \"Evaluator/prompts/tool_prompts.json\",\n        \"Evaluator/prompts/baseline.json\": \"Evaluator/prompts/baseline.json\",\n        \"Evaluator/prompts/behavioral_patterns.json\": \"Evaluator/prompts/behavioral_patterns.json\",\n        \"tools/tool_schemas.json\": \"tools/tool_schemas.json\",\n    }\n    \n    print(\"Downloading evaluation framework...\")\n    for remote_path, local_path in eval_files.items():\n        url = f\"{REPO_BASE}/{remote_path}\"\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n            with open(local_path, 'w', encoding='utf-8') as f:\n                f.write(response.text)\n        except Exception as e:\n            print(f\"  Warning: Failed to download {remote_path}\")\n    \n    print(\"Evaluation framework ready\")\nelse:\n    print(\"Evaluation skipped. Set run_evaluation = True to enable.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "69hd2wueogu",
   "source": "if run_evaluation:\n    from vllm import LLM, SamplingParams\n    from dataclasses import dataclass\n    from typing import Any, Dict, Mapping, Sequence\n    import time\n    import sys\n    \n    sys.path.insert(0, '/content')\n    \n    # Use the merged model for evaluation\n    EVAL_MODEL = f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\"\n    \n    print(f\"Loading model for evaluation: {EVAL_MODEL}\")\n    print(\"This may take 1-2 minutes...\")\n    \n    # Initialize vLLM\n    eval_llm = LLM(\n        model=EVAL_MODEL,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n        max_model_len=2048,\n        trust_remote_code=True,\n        dtype=\"auto\",\n        token=HF_TOKEN,\n    )\n    \n    # Get the vLLM tokenizer for proper chat template handling\n    eval_tokenizer = eval_llm.get_tokenizer()\n    \n    # Create vLLM client for evaluator\n    @dataclass\n    class VLLMResponse:\n        message: str\n        raw: Dict[str, Any]\n        latency_s: float\n    \n    class VLLMClient:\n        def __init__(self, llm, llm_tokenizer, temperature=0.2, max_tokens=1024, seed=42):\n            self.llm = llm\n            self.tokenizer = llm_tokenizer\n            self.temperature = temperature\n            self.max_tokens = max_tokens\n            self.seed = seed\n        \n        def chat(self, messages):\n            # Use the tokenizer's chat template instead of manual construction\n            # This ensures special tokens are handled correctly\n            prompt = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            \n            sampling_params = SamplingParams(\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                seed=self.seed,\n            )\n            \n            start = time.perf_counter()\n            outputs = self.llm.generate([prompt], sampling_params)\n            latency_s = time.perf_counter() - start\n            \n            output = outputs[0]\n            message = output.outputs[0].text.strip()\n            \n            return VLLMResponse(\n                message=message,\n                raw={\"output\": message},\n                latency_s=latency_s\n            )\n    \n    eval_client = VLLMClient(eval_llm, eval_tokenizer)\n    print(\"Model loaded for evaluation\")\n    \n    # Run evaluation\n    from Evaluator.prompt_sets import load_prompt_cases, filter_prompts\n    from Evaluator.runner import evaluate_cases\n    from Evaluator.reporting import build_evaluation_lineage, generate_evaluation_model_card_section\n    from Evaluator.config import PromptFilter\n    \n    # Map test suite to files - updated with behavioral patterns\n    eval_suite_map = {\n        \"Tool Coverage (47 tools)\": [\"Evaluator/prompts/tool_prompts.json\"],\n        \"Behavioral Patterns (24 tests)\": [\"Evaluator/prompts/behavioral_patterns.json\"],\n        \"All Suites\": [\n            \"Evaluator/prompts/tool_prompts.json\",\n            \"Evaluator/prompts/behavioral_patterns.json\"\n        ],\n    }\n    \n    eval_prompt_files = eval_suite_map[eval_test_suite]\n    all_eval_records = []\n    suite_results = {}  # Track results per suite\n    \n    print()\n    print(\"=\" * 60)\n    print(\"RUNNING EVALUATION\")\n    print(f\"Test Suite: {eval_test_suite}\")\n    print(\"=\" * 60)\n    \n    for prompt_file in eval_prompt_files:\n        suite_name = prompt_file.split(\"/\")[-1].replace(\".json\", \"\")\n        cases = load_prompt_cases(prompt_file)\n        print(f\"\\nRunning {len(cases)} tests from {suite_name}\")\n        \n        records = evaluate_cases(\n            cases=cases,\n            client=eval_client,\n            dry_run=False,\n        )\n        all_eval_records.extend(records)\n        \n        passed = sum(1 for r in records if r.passed)\n        suite_results[suite_name] = {\n            \"passed\": passed,\n            \"total\": len(records),\n            \"rate\": passed/len(records)*100\n        }\n        print(f\"   Results: {passed}/{len(records)} passed ({passed/len(records)*100:.1f}%)\")\n    \n    # Calculate overall results\n    eval_passed = sum(1 for r in all_eval_records if r.passed)\n    eval_total = len(all_eval_records)\n    EVAL_PASS_RATE = round(eval_passed / eval_total * 100, 1)\n    \n    print()\n    print(\"=\" * 60)\n    print(\"EVALUATION COMPLETE\")\n    print(\"=\" * 60)\n    print(f\"\\nResults by Suite:\")\n    for suite_name, results in suite_results.items():\n        print(f\"  {suite_name}: {results['passed']}/{results['total']} ({results['rate']:.1f}%)\")\n    print(f\"\\nOverall: {eval_passed}/{eval_total} passed ({EVAL_PASS_RATE}%)\")\n    \n    # Build evaluation lineage\n    eval_config = {\"temperature\": 0.2, \"max_tokens\": 1024, \"seed\": 42}\n    eval_hardware = {\n        \"gpu\": GPU_NAME,\n        \"gpu_memory_gb\": round(GPU_MEMORY_GB, 1),\n        \"platform\": \"Google Colab\",\n    }\n    \n    EVALUATION_LINEAGE = build_evaluation_lineage(\n        records=all_eval_records,\n        model_name=EVAL_MODEL,\n        test_suites=eval_prompt_files,\n        eval_config=eval_config,\n        hardware_info=eval_hardware,\n    )\n    \n    # Add suite-level breakdown to lineage\n    EVALUATION_LINEAGE[\"suite_results\"] = suite_results\n    \n    MODEL_CARD_EVAL_SECTION = generate_evaluation_model_card_section(EVALUATION_LINEAGE)\n    \n    # Save evaluation lineage\n    eval_lineage_path = f\"{output_dir}/evaluation_lineage.json\"\n    with open(eval_lineage_path, \"w\") as f:\n        json.dump(EVALUATION_LINEAGE, f, indent=2)\n    \n    print()\n    print(f\"Evaluation lineage saved: {eval_lineage_path}\")\n    print(f\"  Overall Pass Rate: {EVAL_PASS_RATE}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ygqfdu3p4zp",
   "source": "if run_evaluation:\n    import re\n    import tempfile\n    from huggingface_hub import hf_hub_download\n    \n    print(\"Uploading evaluation results to HuggingFace...\")\n    \n    # Upload to both LoRA and merged repos\n    repos_to_update = [\n        f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n        f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n    ]\n    \n    for repo_id in repos_to_update:\n        try:\n            # Upload evaluation lineage JSON\n            api.upload_file(\n                path_or_fileobj=eval_lineage_path,\n                path_in_repo=\"evaluation_lineage.json\",\n                repo_id=repo_id,\n                token=HF_TOKEN,\n            )\n            \n            # Download and update README with evaluation section\n            try:\n                readme_path = hf_hub_download(repo_id=repo_id, filename=\"README.md\", token=HF_TOKEN)\n                with open(readme_path, 'r') as f:\n                    existing_readme = f.read()\n                \n                if \"## Evaluation Results\" in existing_readme:\n                    pattern = r'## Evaluation Results.*?(?=\\n## |\\Z)'\n                    updated_readme = re.sub(pattern, MODEL_CARD_EVAL_SECTION, existing_readme, flags=re.DOTALL)\n                else:\n                    updated_readme = existing_readme.rstrip() + \"\\n\\n\" + MODEL_CARD_EVAL_SECTION\n                \n                with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:\n                    f.write(updated_readme)\n                    temp_readme = f.name\n                \n                api.upload_file(\n                    path_or_fileobj=temp_readme,\n                    path_in_repo=\"README.md\",\n                    repo_id=repo_id,\n                    token=HF_TOKEN,\n                )\n                print(f\"  ‚úì Updated: {repo_id}\")\n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è  Could not update README for {repo_id}: {e}\")\n        \n        except Exception as e:\n            print(f\"  ‚ö†Ô∏è  Failed to update {repo_id}: {e}\")\n    \n    print()\n    print(\"=\" * 60)\n    print(\"‚úì EVALUATION RESULTS UPLOADED\")\n    print(\"=\" * 60)\n    print(f\"Pass Rate: {EVAL_PASS_RATE}%\")\n    print(f\"View model card: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": "## 14. Test Your Model (Optional)\n\n**What this does:** Quick inference test to see how your refined model behaves."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "from transformers import TextStreamer\n\n# Set up for inference\nModelClass.for_inference(model)\n\n# Note: Chat template was already applied in cell-12 (before training)\n# No need to apply it again here\n\ndef test_tool_calling(user_message):\n    \"\"\"Generate tool call for a user request.\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"USER REQUEST:\")\n    print(\"=\"*60)\n    print(user_message)\n    print(\"\\n\" + \"-\"*60)\n    print(\"MODEL RESPONSE:\")\n    print(\"-\"*60)\n    \n    messages = [{\"role\": \"user\", \"content\": user_message}]\n    inputs = tokenizer.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    ).to(\"cuda\")\n    \n    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n    outputs = model.generate(\n        input_ids=inputs,\n        streamer=text_streamer,\n        temperature=0.1,\n        max_new_tokens=512,\n        use_cache=True\n    )\n    print(\"\\n\")\n\nprint(\"Inference setup complete\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases covering different Claudesidian behaviors\n",
    "test_cases = [\n",
    "    # Content reading\n",
    "    \"Show me the contents of my project roadmap file.\",\n",
    "    \n",
    "    # Content modification  \n",
    "    \"Add a header to my meeting notes saying 'Q1 2025 Planning'.\",\n",
    "    \n",
    "    # File operations\n",
    "    \"Delete the old draft file called 'temp-notes.md'.\",\n",
    "    \n",
    "    # Search operations\n",
    "    \"Find all notes that mention 'product launch'.\",\n",
    "]\n",
    "\n",
    "print(\"Testing tool calling with refined model...\\n\")\n",
    "for test_case in test_cases:\n",
    "    test_tool_calling(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": "## Done!\n\nYour model has been refined with KTO, evaluated, and uploaded to HuggingFace!\n\n### What You Accomplished\n\n| Step | Description |\n|------|-------------|\n| ‚úÖ Refined | Trained with KTO to prefer correct tool-calling patterns |\n| ‚úÖ Lineage | Captured complete training metadata for reproducibility |\n| ‚úÖ Model Card | Auto-generated with all hyperparameters |\n| ‚úÖ Formats | Created LoRA, merged 16-bit, and GGUF versions |\n| ‚úÖ Evaluation | Tested tool-calling accuracy (if enabled) |\n| ‚úÖ Published | Model card includes training AND evaluation results |\n\n### Your Lineage Files\n\n| File | Location | Description |\n|------|----------|-------------|\n| `training_lineage.json` | HuggingFace + Google Drive | All KTO training parameters |\n| `evaluation_lineage.json` | HuggingFace + Google Drive | Test results by category |\n| `README.md` | HuggingFace | Auto-generated model card |\n\n### KTO vs SFT: When to Use Each\n\n| Method | Use Case | Learning Type |\n|--------|----------|---------------|\n| **SFT** | Teaching NEW skills (tool-calling syntax) | Imitation learning |\n| **KTO** | Refining EXISTING behavior (prefer better patterns) | Preference learning |\n\n**Typical pipeline:**\n1. SFT first ‚Üí Teach the model *what* tool calling is\n2. KTO second ‚Üí Refine *which* patterns are preferred\n\n### Next Steps\n\n**Test locally with LM Studio:**\n1. Open LM Studio ‚Üí \"Discover\" tab\n2. Search for your model name\n3. Download the GGUF version\n4. Test with tool-calling prompts\n\n**Test with Ollama:**\n```bash\nollama pull {your-username}/{model-name}\nollama run {your-username}/{model-name}\n```\n\n---\n\n**Questions?** Check the [Evaluator README](https://github.com/ProfSynapse/Toolset-Training/blob/main/Evaluator/README.md) or open an issue on GitHub."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}