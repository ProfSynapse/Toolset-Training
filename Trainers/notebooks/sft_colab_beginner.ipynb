{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tool-Calling Fine-Tuning with SFT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_beginner.ipynb)\n",
    "\n",
    "## ğŸ“ What You'll Learn\n",
    "\n",
    "This notebook teaches you how to fine-tune a language model to use the **Claudesidian-MCP toolset** for Obsidian vault operations. By the end, you'll have:\n",
    "\n",
    "- **A custom AI model** that can call tools to manage Obsidian vaults\n",
    "- **Hands-on experience** with supervised fine-tuning (SFT)\n",
    "- **Understanding** of hyperparameters and how they affect training\n",
    "\n",
    "## ğŸ”¬ What is SFT?\n",
    "\n",
    "**SFT (Supervised Fine-Tuning)** is like teaching through examples:\n",
    "- You show the model examples of correct tool-calling behavior\n",
    "- The model learns to replicate those patterns\n",
    "- Use SFT when teaching a model a **new skill** (like using tools)\n",
    "\n",
    "**When to use SFT:**\n",
    "- âœ… Teaching tool-calling from scratch\n",
    "- âœ… Learning new task formats\n",
    "- âœ… Initial training with positive examples\n",
    "\n",
    "**Not for:**\n",
    "- âŒ Refining existing behavior (use KTO instead)\n",
    "- âŒ Teaching preferences between good/bad outputs (use preference learning)\n",
    "\n",
    "## ğŸ’» Hardware Requirements\n",
    "\n",
    "**Recommended GPU:**\n",
    "- 7B models: T4 (15GB VRAM) - âœ… **Free Colab tier works!**\n",
    "- 13B models: A100 (40GB VRAM) - Colab Pro\n",
    "- 70B models: A100 (80GB VRAM) - Colab Pro+\n",
    "\n",
    "**Training time:** ~45 minutes for a 7B model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets>=2.14.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to Google Drive so they persist if runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets:\n",
    "1. Click the ğŸ”‘ key icon in the left sidebar\n",
    "2. Add new secret: `HF_TOKEN`\n",
    "3. Get token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Get your HuggingFace username automatically\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"âœ“ HuggingFace token loaded\")\n",
    "print(f\"âœ“ Username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "**What this does:** Choose the base model you want to fine-tune and configure basic settings.\n",
    "\n",
    "Think of this like choosing which \"brain\" you want to teach tool-calling skills to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title âš™ï¸ Model & Dataset Configuration\n",
    "# @markdown Use the dropdowns to select your model and configure your dataset.\n",
    "\n",
    "# @markdown ### ğŸ§  Base Model Selection\n",
    "# @markdown Choose a model based on your VRAM availability. Models are ordered by size (1B - 24B).\n",
    "# @markdown * **1B-3B:** Fast, runs on any GPU\n",
    "# @markdown * **7B-9B:** Standard balance of speed/intelligence\n",
    "# @markdown * **12B-24B:** High intelligence, requires ~12GB-24GB VRAM (A100 recommended for 20B+)\n",
    "MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # @param [\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\", \"unsloth/gemma-2-2b-it-bnb-4bit\", \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", \"unsloth/Phi-3.5-mini-instruct\", \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\", \"unsloth/mistral-7b-v0.3-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\", \"unsloth/gemma-2-9b-it-bnb-4bit\", \"unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\", \"unsloth/Mistral-Nemo-Instruct-v1-bnb-4bit\", \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\", \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\", \"unsloth/Phi-4\", \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", \"unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit\", \"unsloth/Magistral-Small-2509-unsloth-bnb-4bit\"]\n",
    "\n",
    "# @markdown ### ğŸ“ Max Output Length\n",
    "# @markdown  2048 is standard. Higher values require more VRAM.\n",
    "MAX_SEQ_LENGTH = 2048 # @param [1024, 2048, 4096, 8192] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### ğŸ“š Dataset Configuration\n",
    "DATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\" # @param {type:\"string\"}\n",
    "DATASET_FILE = \"syngen_tools_sft_11.24.25_with_tools.jsonl\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### ğŸ·ï¸ Output Model Name\n",
    "# @markdown Name your fine-tuned model (e.g., `my-tool-model-v1`).\n",
    "OUTPUT_MODEL_NAME = \"nexus-tools-sft-7b\" # @param {type:\"string\"}\n",
    "\n",
    "print(f\"âœ“ Configuration set:\")\n",
    "print(f\"  â€¢ Model: {MODEL_NAME}\")\n",
    "print(f\"  â€¢ Context: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  â€¢ Dataset: {DATASET_NAME}/{DATASET_FILE}\")\n",
    "print(f\"  â€¢ Output: {OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "**What this does:** Downloads the base model and prepares it for training.\n",
    "\n",
    "The model is the \"brain\" that will learn tool-calling. The tokenizer converts text into numbers the model can process. We use 4-bit quantization to fit large models into limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU and store info for lineage\n",
    "GPU_NAME = torch.cuda.get_device_name(0)\n",
    "CUDA_VERSION = torch.version.cuda\n",
    "GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "print(f\"Using GPU: {GPU_NAME}\")\n",
    "print(f\"CUDA version: {CUDA_VERSION}\")\n",
    "print(f\"Available VRAM: {GPU_MEMORY_GB:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model and tokenizer from HuggingFace\n",
    "# This downloads the model weights (~7GB for 7B models)\n",
    "#\n",
    "# Parameters explained:\n",
    "#   model_name: Which model to download\n",
    "#   max_seq_length: Max tokens model can process at once\n",
    "#   dtype=None: Auto-detect best precision for your GPU\n",
    "#   load_in_4bit=True: Use 4-bit quantization to save memory\n",
    "#   token: Your HF token for accessing the model\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect (usually bfloat16 or float16)\n",
    "    load_in_4bit=True,  # Reduces memory usage by ~75%\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Store for lineage\n",
    "TOTAL_PARAMS = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "print(f\"  Model has {TOTAL_PARAMS:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA Adapters\n",
    "\n",
    "**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n",
    "\n",
    "Think of LoRA like teaching a new skill through muscle memory - we add small specialized layers that learn the new behavior, while keeping the main \"brain\" frozen. This is way faster and uses less memory than retraining everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ğŸ”§ LoRA Adapter Configuration\n",
    "# @markdown Configure the size and strength of the fine-tuning adapters.\n",
    "\n",
    "# @markdown ### ğŸ›ï¸ LoRA Parameters\n",
    "# @markdown **Rank (r):** Higher = smarter but slower/more memory (Standard: 16-64).\n",
    "# @markdown Alpha will be automatically set to 2 * r.\n",
    "LORA_R = 32 # @param [8, 16, 32, 64, 128] {type:\"raw\"}\n",
    "\n",
    "LORA_ALPHA = LORA_R * 2\n",
    "\n",
    "# @markdown **Dropout:** Helps prevent overfitting (Standard: 0.05).\n",
    "LORA_DROPOUT = 0.05 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown **Random Seed:** Change this for different initialization.\n",
    "RANDOM_STATE = 3407 # @param {type:\"integer\"}\n",
    "\n",
    "# Target modules for LoRA\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Store for lineage\n",
    "TRAINABLE_PARAMS = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ“ LoRA adapters applied:\")\n",
    "print(f\"  â€¢ Rank: {LORA_R}\")\n",
    "print(f\"  â€¢ Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  â€¢ Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  â€¢ Trainable params: {TRAINABLE_PARAMS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Dataset\n",
    "\n",
    "**What this does:** Downloads training examples and formats them for the model.\n",
    "\n",
    "The dataset contains examples of correct tool-calling behavior. Think of it like a textbook full of solved problems that the model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "LOAD DATASET FROM HUGGINGFACE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "This downloads pre-made training examples of correct tool-calling behavior.\n",
    "Each example shows: user request â†’ tool call â†’ result â†’ assistant response\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_NAME}/{DATASET_FILE}\")\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,  # HuggingFace repository containing the dataset\n",
    "    data_files=DATASET_FILE,  # Specific JSONL file to use\n",
    "    split=\"train\"  # Use the training split\n",
    ")\n",
    "\n",
    "# Store dataset info for lineage\n",
    "DATASET_SIZE = len(dataset)\n",
    "\n",
    "print(f\"âœ“ Loaded {DATASET_SIZE} examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(dataset[0])\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SET CHAT TEMPLATE (Model-Specific)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "The chat template defines how conversations are formatted for the model.\n",
    "Different models use different formats - we auto-detect and use the right one!\n",
    "\"\"\"\n",
    "\n",
    "# Mistral-specific template (uses [INST] format)\n",
    "MISTRAL_CHAT_TEMPLATE = \"\"\"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'system' %}{% if loop.index == 1 %}{{ message['content'] + ' ' }}{% endif %}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "# Generic fallback template for other models\n",
    "DEFAULT_CHAT_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|user|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ '<|system|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>\\\\n'  + message['content'] + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "# Store chat template type for lineage\n",
    "CHAT_TEMPLATE_TYPE = \"default\"\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    # Detect model type and use appropriate template\n",
    "    is_mistral = 'mistral' in MODEL_NAME.lower()\n",
    "\n",
    "    if is_mistral:\n",
    "        print(\"\\nâœ“ Detected Mistral model - using [INST] format\")\n",
    "        tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE\n",
    "        CHAT_TEMPLATE_TYPE = \"mistral\"\n",
    "        print(\"   Format: <s>[INST] user [/INST] assistant</s>\")\n",
    "        print(\"   This is the official Mistral chat format!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No chat template found, using default format\")\n",
    "        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "        CHAT_TEMPLATE_TYPE = \"default\"\n",
    "        print(\"   Format: <|user|>\\\\ncontent</s>\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Tokenizer already has chat template\")\n",
    "    CHAT_TEMPLATE_TYPE = \"native\"\n",
    "    if 'mistral' in MODEL_NAME.lower():\n",
    "        print(\"   Using Mistral model - template should use [INST] format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "FORMAT DATASET FOR TRAINING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Convert the conversation format into the exact text format the model expects.\n",
    "This applies the chat template to each example.\n",
    "\"\"\"\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Convert conversations to tokenizer's chat template.\n",
    "\n",
    "    Input: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, ...]}\n",
    "    Output: {\"text\": \"<|im_start|>user\\n...<|im_end|>\\n...\"}\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"conversations\"],  # List of message dicts\n",
    "        tokenize=False,  # Return string, not token IDs\n",
    "        add_generation_prompt=False  # Don't add prompt for model to continue\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting to entire dataset\n",
    "# This creates a new \"text\" field with formatted conversations\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,  # Function to apply\n",
    "    remove_columns=dataset.column_names,  # Remove original columns\n",
    "    desc=\"Formatting dataset\"  # Progress bar description\n",
    ")\n",
    "\n",
    "print(\"âœ“ Dataset formatted for training\")\n",
    "print(f\"\\nFormatted example (first 500 characters):\")\n",
    "print(dataset[0][\"text\"][:500])\n",
    "print(\"\\nğŸ’¡ The full formatted text will be used during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "**What this does:** Set the hyperparameters that control how the model learns.\n",
    "\n",
    "This is the **most important section** - these settings determine how fast the model learns, how much memory it uses, and how good the final result will be. Think of it like configuring a study plan: how many hours per day, how many review sessions, how to handle difficult material, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped output directory\n",
    "TRAINING_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{DRIVE_OUTPUT_DIR}/{TRAINING_TIMESTAMP}\"\n",
    "\n",
    "# @title ğŸƒ Training Hyperparameters\n",
    "# @markdown Control the speed and quality of training.\n",
    "\n",
    "# @markdown ### âš¡ Performance Settings\n",
    "# @markdown **Batch Size:** Examples per step. Lower if you run out of memory.\n",
    "BATCH_SIZE = 2 # @param [1, 2, 4, 6, 8, 10, 12] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Gradient Accumulation:** Simulates larger batches.\n",
    "GRADIENT_ACCUMULATION = 4 # @param [1, 2, 4, 6, 8, 10, 12] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### ğŸ§  Learning Rate Configuration\n",
    "# @markdown **Step 1: Choose the Magnitude (Exponent)**\n",
    "# @markdown This is the most important setting. It determines the \"speed\" of learning.\n",
    "# @markdown * **4** = Standard (1e-4). Recommended for 7B models and SFT.\n",
    "# @markdown * **5** = Slow (1e-5). Use if training is unstable or for larger models.\n",
    "# @markdown * **6** = Very Slow (1e-6). Precise but takes much longer.\n",
    "LEARNING_RATE_EXPONENT = 4 # @param [4, 5, 6, 7] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Step 2: Choose the Multiplier**\n",
    "# @markdown Fine-tunes the rate within that magnitude (e.g., Multiplier 2 + Exponent 4 = 2e-4).\n",
    "LEARNING_RATE_MULTIPLIER = 2 # @param [1, 2, 3, 4, 5, 6, 7, 8, 9] {type:\"raw\"}\n",
    "\n",
    "LEARNING_RATE = LEARNING_RATE_MULTIPLIER * (10 ** -LEARNING_RATE_EXPONENT)\n",
    "\n",
    "# @markdown ### ğŸ”„ Epochs\n",
    "# @markdown Number of passes through the dataset.\n",
    "NUM_EPOCHS = 3 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### ğŸ’¾ Saving & Logging\n",
    "SAVE_STEPS = 100 # @param {type:\"integer\"}\n",
    "LOGGING_STEPS = 5 # @param {type:\"integer\"}\n",
    "\n",
    "# Other training settings\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LR_SCHEDULER = \"cosine\"\n",
    "OPTIMIZER = \"adamw_8bit\"\n",
    "USE_BF16 = is_bfloat16_supported()\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    fp16=not USE_BF16,\n",
    "    bf16=USE_BF16,\n",
    "    optim=OPTIMIZER,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    seed=RANDOM_STATE,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Calculate effective batch size\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "\n",
    "print(\"âœ“ Training configuration ready\")\n",
    "print(f\"  â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  â€¢ Gradient Accum.: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  â€¢ Effective Batch: {EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"  â€¢ Learning Rate: {LEARNING_RATE} ({LEARNING_RATE_MULTIPLIER}e-{LEARNING_RATE_EXPONENT})\")\n",
    "print(f\"  â€¢ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  â€¢ Output Dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer\n",
    "\n",
    "**What this does:** Creates the training engine that coordinates everything.\n",
    "\n",
    "The SFTTrainer is the orchestrator - it takes the model, dataset, and configuration, then handles all the training mechanics (gradient updates, checkpointing, logging, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SFTTrainer\n",
    "# This combines the model, dataset, and configuration into one training pipeline\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The model with LoRA adapters\n",
    "    tokenizer=tokenizer,  # For converting text to tokens\n",
    "    train_dataset=dataset,  # Our formatted training examples\n",
    "    args=training_args,  # All the hyperparameters we configured\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(\"  Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 10. Train!\n",
    "\n",
    "**What this does:** The actual learning happens here!\n",
    "\n",
    "The model will:\n",
    "1. **Read examples** from the dataset\n",
    "2. **Predict** what the response should be\n",
    "3. **Compare** its prediction to the correct answer\n",
    "4. **Update weights** to get closer to the correct answer\n",
    "5. **Repeat** this process for 3 epochs (3 full passes through the data)\n",
    "\n",
    "**What to expect:**\n",
    "- Training takes ~45 minutes for 7B models on T4 GPU\n",
    "- You'll see progress updates every 5 steps\n",
    "- Loss should generally decrease over time (learning is working!)\n",
    "- Checkpoints are saved every 100 steps to Google Drive\n",
    "\n",
    "**What the metrics mean:**\n",
    "- **Loss:** How \"wrong\" the model is (lower = better, aim for <1.0)\n",
    "- **Learning Rate:** Gradually decreases as training progresses\n",
    "- **Samples/sec:** Training speed (depends on GPU)\n",
    "\n",
    "**ğŸ’¾ Checkpoint Resumption:**\n",
    "If your Colab session disconnects, don't worry! Your checkpoints are saved to Google Drive. You can resume training by:\n",
    "1. Re-running cells 1-9 (setup, model loading, dataset prep, config)\n",
    "2. In the training cell below, the code will automatically detect the latest checkpoint and resume from there\n",
    "3. Your progress is preserved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” Check for existing checkpoints (automatic resumption)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Found checkpoints - get the latest one\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = latest_checkpoint\n",
    "    print(f\"âœ“ Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"  Resuming training from this checkpoint\")\n",
    "    print(f\"  Total checkpoints found: {len(checkpoint_dirs)}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No existing checkpoints found - starting fresh training\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Start training (or resume)\n",
    "print(\"=\" * 60)\n",
    "if resume_from_checkpoint:\n",
    "    print(\"RESUMING TRAINING FROM CHECKPOINT\")\n",
    "else:\n",
    "    print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Track training time\n",
    "training_start_time = time.time()\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "# Calculate training duration\n",
    "TRAINING_DURATION_SECONDS = time.time() - training_start_time\n",
    "TRAINING_DURATION_MINUTES = TRAINING_DURATION_SECONDS / 60\n",
    "\n",
    "# Store final metrics for lineage\n",
    "FINAL_LOSS = trainer_stats.training_loss\n",
    "TOTAL_STEPS = trainer_stats.global_step\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final loss: {FINAL_LOSS:.4f}\")\n",
    "print(f\"Total steps: {TOTAL_STEPS}\")\n",
    "print(f\"Training time: {TRAINING_DURATION_MINUTES:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 11. Build Training Lineage\n",
    "\n",
    "**What this does:** Captures all training metadata for reproducibility and analysis.\n",
    "\n",
    "This creates a complete record of:\n",
    "- Base model and configuration\n",
    "- Dataset details\n",
    "- All hyperparameters used\n",
    "- Training results and metrics\n",
    "- Hardware and environment info\n",
    "\n",
    "This information will be automatically added to your HuggingFace model card!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "BUILD COMPLETE TRAINING LINEAGE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "This dictionary captures EVERYTHING about the training run for:\n",
    "- Reproducibility\n",
    "- Model card generation\n",
    "- Experiment tracking\n",
    "- Analysis and comparison\n",
    "\"\"\"\n",
    "\n",
    "TRAINING_LINEAGE = {\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # IDENTIFICATION\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"model_name\": OUTPUT_MODEL_NAME,\n",
    "    \"training_method\": \"SFT\",\n",
    "    \"training_timestamp\": TRAINING_TIMESTAMP,\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # BASE MODEL\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"base_model\": {\n",
    "        \"name\": MODEL_NAME,\n",
    "        \"total_parameters\": TOTAL_PARAMS,\n",
    "        \"quantization\": \"4-bit\",\n",
    "        \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    },\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # LORA CONFIGURATION\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"lora_config\": {\n",
    "        \"r\": LORA_R,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": TARGET_MODULES,\n",
    "        \"trainable_parameters\": TRAINABLE_PARAMS,\n",
    "        \"trainable_percentage\": round(TRAINABLE_PARAMS / TOTAL_PARAMS * 100, 4),\n",
    "    },\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # DATASET\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"dataset\": {\n",
    "        \"name\": DATASET_NAME,\n",
    "        \"file\": DATASET_FILE,\n",
    "        \"huggingface_url\": f\"https://huggingface.co/datasets/{DATASET_NAME}\",\n",
    "        \"total_examples\": DATASET_SIZE,\n",
    "        \"chat_template\": CHAT_TEMPLATE_TYPE,\n",
    "    },\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # TRAINING HYPERPARAMETERS\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"training_config\": {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n",
    "        \"effective_batch_size\": EFFECTIVE_BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_scheduler\": LR_SCHEDULER,\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"optimizer\": OPTIMIZER,\n",
    "        \"precision\": \"bf16\" if USE_BF16 else \"fp16\",\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"packing\": False,\n",
    "        \"random_seed\": RANDOM_STATE,\n",
    "    },\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # TRAINING RESULTS\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"training_results\": {\n",
    "        \"final_loss\": round(FINAL_LOSS, 4),\n",
    "        \"total_steps\": TOTAL_STEPS,\n",
    "        \"training_duration_minutes\": round(TRAINING_DURATION_MINUTES, 1),\n",
    "    },\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # HARDWARE & ENVIRONMENT\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"hardware\": {\n",
    "        \"gpu\": GPU_NAME,\n",
    "        \"gpu_memory_gb\": round(GPU_MEMORY_GB, 1),\n",
    "        \"cuda_version\": CUDA_VERSION,\n",
    "        \"platform\": \"Google Colab\",\n",
    "    },\n",
    "    \n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    # FRAMEWORK VERSIONS\n",
    "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"framework_versions\": {\n",
    "        \"torch\": torch.__version__,\n",
    "        \"transformers\": __import__(\"transformers\").__version__,\n",
    "        \"trl\": __import__(\"trl\").__version__,\n",
    "        \"peft\": __import__(\"peft\").__version__,\n",
    "        \"unsloth\": \"latest\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save lineage to file\n",
    "lineage_path = f\"{output_dir}/training_lineage.json\"\n",
    "with open(lineage_path, \"w\") as f:\n",
    "    json.dump(TRAINING_LINEAGE, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Training lineage captured!\")\n",
    "print(f\"  Saved to: {lineage_path}\")\n",
    "print()\n",
    "print(\"ğŸ“‹ Summary:\")\n",
    "print(f\"  â€¢ Base Model: {MODEL_NAME}\")\n",
    "print(f\"  â€¢ Dataset: {DATASET_NAME}/{DATASET_FILE} ({DATASET_SIZE} examples)\")\n",
    "print(f\"  â€¢ Method: SFT (Supervised Fine-Tuning)\")\n",
    "print(f\"  â€¢ LoRA: r={LORA_R}, Î±={LORA_ALPHA}\")\n",
    "print(f\"  â€¢ LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  â€¢ Final Loss: {FINAL_LOSS:.4f}\")\n",
    "print(f\"  â€¢ Duration: {TRAINING_DURATION_MINUTES:.1f} min on {GPU_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 12. Upload to HuggingFace\n",
    "\n",
    "**What this does:** Share your trained model with the world!\n",
    "\n",
    "The model card will be **automatically generated** with all your training details:\n",
    "- Base model and configuration\n",
    "- Dataset information\n",
    "- All hyperparameters\n",
    "- Training results\n",
    "- Hardware used\n",
    "\n",
    "We'll create **three versions** of your model:\n",
    "\n",
    "1. **LoRA adapters** - Small files that contain just the changes\n",
    "2. **Merged 16-bit model** - Full model with adapters merged in\n",
    "3. **GGUF quantizations** - Optimized versions for CPU/GPU inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(lineage: dict, hf_username: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive HuggingFace model card from training lineage.\n",
    "    \n",
    "    This creates a professional README.md with all training details\n",
    "    for reproducibility and transparency.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_model = lineage[\"base_model\"][\"name\"]\n",
    "    dataset = lineage[\"dataset\"]\n",
    "    lora = lineage[\"lora_config\"]\n",
    "    training = lineage[\"training_config\"]\n",
    "    results = lineage[\"training_results\"]\n",
    "    hardware = lineage[\"hardware\"]\n",
    "    frameworks = lineage[\"framework_versions\"]\n",
    "    \n",
    "    model_card = f'''---\n",
    "language:\n",
    "- en\n",
    "license: apache-2.0\n",
    "library_name: transformers\n",
    "tags:\n",
    "- tool-calling\n",
    "- sft\n",
    "- supervised-fine-tuning\n",
    "- claudesidian\n",
    "- obsidian\n",
    "- fine-tuned\n",
    "- unsloth\n",
    "base_model: {base_model}\n",
    "datasets:\n",
    "- {dataset[\"name\"]}\n",
    "pipeline_tag: text-generation\n",
    "model-index:\n",
    "- name: {lineage[\"model_name\"]}\n",
    "  results:\n",
    "  - task:\n",
    "      type: text-generation\n",
    "    metrics:\n",
    "    - name: Final Loss\n",
    "      type: loss\n",
    "      value: {results[\"final_loss\"]}\n",
    "---\n",
    "\n",
    "# {lineage[\"model_name\"]}\n",
    "\n",
    "This model was fine-tuned using **SFT (Supervised Fine-Tuning)** to learn tool-calling behavior for the Claudesidian vault application.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Base Model:** [{base_model}](https://huggingface.co/{base_model})\n",
    "- **Training Method:** SFT (Supervised Fine-Tuning)\n",
    "- **Task:** Tool-calling for Obsidian vault operations\n",
    "- **Training Date:** {lineage[\"training_date\"]}\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Dataset\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Dataset | [{dataset[\"name\"]}]({dataset[\"huggingface_url\"]}) |\n",
    "| File | {dataset[\"file\"]} |\n",
    "| Total Examples | {dataset[\"total_examples\"]:,} |\n",
    "| Chat Template | {dataset[\"chat_template\"]} |\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Rank (r) | {lora[\"r\"]} |\n",
    "| Alpha (Î±) | {lora[\"alpha\"]} |\n",
    "| Dropout | {lora[\"dropout\"]} |\n",
    "| Target Modules | {', '.join(lora[\"target_modules\"])} |\n",
    "| Trainable Parameters | {lora[\"trainable_parameters\"]:,} ({lora[\"trainable_percentage\"]}%) |\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Batch Size | {training[\"batch_size\"]} |\n",
    "| Gradient Accumulation | {training[\"gradient_accumulation_steps\"]} |\n",
    "| Effective Batch Size | {training[\"effective_batch_size\"]} |\n",
    "| Learning Rate | {training[\"learning_rate\"]} |\n",
    "| LR Scheduler | {training[\"learning_rate_scheduler\"]} |\n",
    "| Warmup Ratio | {training[\"warmup_ratio\"]} |\n",
    "| Max Grad Norm | {training[\"max_grad_norm\"]} |\n",
    "| Epochs | {training[\"num_epochs\"]} |\n",
    "| Optimizer | {training[\"optimizer\"]} |\n",
    "| Precision | {training[\"precision\"]} |\n",
    "| Packing | {training[\"packing\"]} |\n",
    "| Random Seed | {training[\"random_seed\"]} |\n",
    "\n",
    "### Training Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Final Loss | {results[\"final_loss\"]} |\n",
    "| Total Steps | {results[\"total_steps\"]:,} |\n",
    "| Training Duration | {results[\"training_duration_minutes\"]} minutes |\n",
    "\n",
    "### Hardware\n",
    "\n",
    "| Component | Value |\n",
    "|-----------|-------|\n",
    "| GPU | {hardware[\"gpu\"]} |\n",
    "| GPU Memory | {hardware[\"gpu_memory_gb\"]} GB |\n",
    "| CUDA Version | {hardware[\"cuda_version\"]} |\n",
    "| Platform | {hardware[\"platform\"]} |\n",
    "\n",
    "### Framework Versions\n",
    "\n",
    "| Library | Version |\n",
    "|---------|--------|\n",
    "| PyTorch | {frameworks[\"torch\"]} |\n",
    "| Transformers | {frameworks[\"transformers\"]} |\n",
    "| TRL | {frameworks[\"trl\"]} |\n",
    "| PEFT | {frameworks[\"peft\"]} |\n",
    "| Unsloth | {frameworks[\"unsloth\"]} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "### With Transformers\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{hf_username}/{lineage[\"model_name\"]}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{hf_username}/{lineage[\"model_name\"]}\")\n",
    "\n",
    "# Example tool-calling prompt\n",
    "messages = [{{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Show me the contents of my project roadmap file.\"\n",
    "}}]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "### With Ollama (GGUF)\n",
    "\n",
    "```bash\n",
    "# Download the GGUF version\n",
    "ollama pull {hf_username}/{lineage[\"model_name\"]}\n",
    "\n",
    "# Run inference\n",
    "ollama run {hf_username}/{lineage[\"model_name\"]}\n",
    "```\n",
    "\n",
    "### With LM Studio\n",
    "\n",
    "1. Open LM Studio â†’ \"Discover\" tab\n",
    "2. Search for `{hf_username}/{lineage[\"model_name\"]}`\n",
    "3. Download the Q4_K_M or Q5_K_M GGUF version\n",
    "4. Load and test with tool-calling prompts\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is designed for:\n",
    "- Tool-calling in Obsidian vault management applications\n",
    "- Claudesidian MCP integration\n",
    "- Local AI assistants that interact with note-taking systems\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Trained specifically for Claudesidian tool schemas\n",
    "- May not generalize to other tool-calling formats\n",
    "- Best performance with the specific tool set it was trained on\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{{lineage[\"model_name\"].replace(\"-\", \"_\")},\n",
    "  author = {{{hf_username}}},\n",
    "  title = {{{lineage[\"model_name\"]}: SFT Fine-tuned Tool-Calling Model}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{HuggingFace}},\n",
    "  url = {{https://huggingface.co/{hf_username}/{lineage[\"model_name\"]}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Training Lineage\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand full training configuration (JSON)</summary>\n",
    "\n",
    "```json\n",
    "{json.dumps(lineage, indent=2)}\n",
    "```\n",
    "\n",
    "</details>\n",
    "'''\n",
    "    \n",
    "    return model_card\n",
    "\n",
    "# Generate model card\n",
    "MODEL_CARD = generate_model_card(TRAINING_LINEAGE, hf_user)\n",
    "\n",
    "# Save model card locally\n",
    "model_card_path = f\"{output_dir}/README.md\"\n",
    "with open(model_card_path, \"w\") as f:\n",
    "    f.write(MODEL_CARD)\n",
    "\n",
    "print(\"âœ“ Model card generated!\")\n",
    "print(f\"  Saved to: {model_card_path}\")\n",
    "print()\n",
    "print(\"Preview (first 50 lines):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\".join(MODEL_CARD.split(\"\\n\")[:50]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "\n",
    "# Upload LoRA adapters with model card\n",
    "print(\"Uploading LoRA adapters...\")\n",
    "\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload model card (README.md)\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card_path,\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Upload training lineage JSON\n",
    "api.upload_file(\n",
    "    path_or_fileobj=lineage_path,\n",
    "    path_in_repo=\"training_lineage.json\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"âœ“ Model card with full lineage uploaded\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model with model card\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload model card to merged repo too\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card_path,\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=lineage_path,\n",
    "    path_in_repo=\"training_lineage.json\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Merged model uploaded to HuggingFace\")\n",
    "print(f\"âœ“ Model card with full lineage uploaded\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GGUF quantizations\n",
    "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    "\n",
    "print(\"Creating GGUF quantizations...\")\n",
    "print(f\"This will create {len(quantization_methods)} versions\")\n",
    "print()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_methods,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"âœ“ GGUF quantizations created and uploaded!\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vjfp44pq4q",
   "source": "## 13. Evaluate Model (Optional)\n\n**What this does:** Run automated tests to measure your model's tool-calling accuracy.\n\nThis will:\n- Load your trained model with vLLM for fast inference\n- Run test prompts covering all 47 tools\n- Calculate pass rates by category\n- Generate evaluation lineage that can be added to your model card\n\n**Skip this section** if you want to evaluate later using the standalone evaluation notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ugsy4u6gaf",
   "source": "# @title ğŸ§ª Run Evaluation (Optional)\n# @markdown Test your trained model's tool-calling accuracy.\n\n# @markdown ### Enable Evaluation\nrun_evaluation = True # @param {type:\"boolean\"}\n\n# @markdown ### Test Suite Selection\neval_test_suite = \"Full Coverage (47 tools)\" # @param [\"Full Coverage (47 tools)\", \"Baseline (6 tests)\", \"All Suites\"]\n\nif run_evaluation:\n    print(\"Installing vLLM for evaluation...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"-q\", \"vllm>=0.6.0\"], check=True)\n    \n    # Download evaluator framework\n    import requests\n    from pathlib import Path\n    \n    os.makedirs(\"Evaluator/prompts\", exist_ok=True)\n    os.makedirs(\"Evaluator/results\", exist_ok=True)\n    os.makedirs(\"tools\", exist_ok=True)\n    \n    REPO_BASE = \"https://raw.githubusercontent.com/ProfSynapse/Toolset-Training/main\"\n    \n    eval_files = {\n        \"Evaluator/__init__.py\": \"Evaluator/__init__.py\",\n        \"Evaluator/runner.py\": \"Evaluator/runner.py\",\n        \"Evaluator/schema_validator.py\": \"Evaluator/schema_validator.py\",\n        \"Evaluator/prompt_sets.py\": \"Evaluator/prompt_sets.py\",\n        \"Evaluator/reporting.py\": \"Evaluator/reporting.py\",\n        \"Evaluator/config.py\": \"Evaluator/config.py\",\n        \"Evaluator/prompts/full_coverage.json\": \"Evaluator/prompts/full_coverage.json\",\n        \"Evaluator/prompts/baseline.json\": \"Evaluator/prompts/baseline.json\",\n        \"tools/tool_schemas.json\": \"tools/tool_schemas.json\",\n    }\n    \n    print(\"Downloading evaluation framework...\")\n    for remote_path, local_path in eval_files.items():\n        url = f\"{REPO_BASE}/{remote_path}\"\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n            with open(local_path, 'w', encoding='utf-8') as f:\n                f.write(response.text)\n        except Exception as e:\n            print(f\"  âš ï¸  Failed: {remote_path}\")\n    \n    print(\"âœ“ Evaluation framework ready\")\nelse:\n    print(\"â„¹ï¸  Evaluation skipped. Set run_evaluation = True to enable.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a8c3kehv1j",
   "source": "if run_evaluation:\n    from vllm import LLM, SamplingParams\n    from dataclasses import dataclass\n    from typing import Any, Dict, Mapping, Sequence\n    import time\n    import sys\n    \n    sys.path.insert(0, '/content')\n    \n    # Use the merged model for evaluation\n    EVAL_MODEL = f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\"\n    \n    print(f\"Loading model for evaluation: {EVAL_MODEL}\")\n    print(\"This may take 1-2 minutes...\")\n    \n    # Initialize vLLM\n    eval_llm = LLM(\n        model=EVAL_MODEL,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n        max_model_len=2048,\n        trust_remote_code=True,\n        dtype=\"auto\",\n        token=HF_TOKEN,\n    )\n    \n    # Create vLLM client for evaluator\n    @dataclass\n    class VLLMResponse:\n        message: str\n        raw: Dict[str, Any]\n        latency_s: float\n    \n    class VLLMClient:\n        def __init__(self, llm, temperature=0.2, max_tokens=1024, seed=42):\n            self.llm = llm\n            self.temperature = temperature\n            self.max_tokens = max_tokens\n            self.seed = seed\n        \n        def chat(self, messages):\n            # Format as Mistral\n            prompt_parts = []\n            for msg in messages:\n                if msg[\"role\"] == \"user\":\n                    prompt_parts.append(f\"[INST] {msg['content']} [/INST]\")\n                elif msg[\"role\"] == \"assistant\":\n                    prompt_parts.append(f\" {msg['content']}</s>\")\n            prompt = \"<s>\" + \"\".join(prompt_parts)\n            \n            sampling_params = SamplingParams(\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                seed=self.seed,\n            )\n            \n            start = time.perf_counter()\n            outputs = self.llm.generate([prompt], sampling_params)\n            latency_s = time.perf_counter() - start\n            \n            output = outputs[0]\n            message = output.outputs[0].text.strip()\n            \n            return VLLMResponse(\n                message=message,\n                raw={\"output\": message},\n                latency_s=latency_s\n            )\n    \n    eval_client = VLLMClient(eval_llm)\n    print(\"âœ“ Model loaded for evaluation\")\n    \n    # Run evaluation\n    from Evaluator.prompt_sets import load_prompt_cases, filter_prompts\n    from Evaluator.runner import evaluate_cases\n    from Evaluator.reporting import build_evaluation_lineage, generate_evaluation_model_card_section\n    from Evaluator.config import PromptFilter\n    \n    # Map test suite to files\n    eval_suite_map = {\n        \"Full Coverage (47 tools)\": [\"Evaluator/prompts/full_coverage.json\"],\n        \"Baseline (6 tests)\": [\"Evaluator/prompts/baseline.json\"],\n        \"All Suites\": [\"Evaluator/prompts/full_coverage.json\", \"Evaluator/prompts/baseline.json\"],\n    }\n    \n    eval_prompt_files = eval_suite_map[eval_test_suite]\n    all_eval_records = []\n    \n    print()\n    print(\"=\" * 60)\n    print(\"RUNNING EVALUATION\")\n    print(\"=\" * 60)\n    \n    for prompt_file in eval_prompt_files:\n        cases = load_prompt_cases(prompt_file)\n        print(f\"\\nğŸ“ Running {len(cases)} tests from {prompt_file}\")\n        \n        records = evaluate_cases(\n            cases=cases,\n            client=eval_client,\n            dry_run=False,\n        )\n        all_eval_records.extend(records)\n        \n        passed = sum(1 for r in records if r.passed)\n        print(f\"   Results: {passed}/{len(records)} passed ({passed/len(records)*100:.1f}%)\")\n    \n    # Calculate overall results\n    eval_passed = sum(1 for r in all_eval_records if r.passed)\n    eval_total = len(all_eval_records)\n    EVAL_PASS_RATE = round(eval_passed / eval_total * 100, 1)\n    \n    print()\n    print(\"=\" * 60)\n    print(\"EVALUATION COMPLETE\")\n    print(\"=\" * 60)\n    print(f\"Overall: {eval_passed}/{eval_total} passed ({EVAL_PASS_RATE}%)\")\n    \n    # Build evaluation lineage\n    eval_config = {\"temperature\": 0.2, \"max_tokens\": 1024, \"seed\": 42}\n    eval_hardware = {\n        \"gpu\": GPU_NAME,\n        \"gpu_memory_gb\": round(GPU_MEMORY_GB, 1),\n        \"platform\": \"Google Colab\",\n    }\n    \n    EVALUATION_LINEAGE = build_evaluation_lineage(\n        records=all_eval_records,\n        model_name=EVAL_MODEL,\n        test_suites=eval_prompt_files,\n        eval_config=eval_config,\n        hardware_info=eval_hardware,\n    )\n    \n    MODEL_CARD_EVAL_SECTION = generate_evaluation_model_card_section(EVALUATION_LINEAGE)\n    \n    # Save evaluation lineage\n    eval_lineage_path = f\"{output_dir}/evaluation_lineage.json\"\n    with open(eval_lineage_path, \"w\") as f:\n        json.dump(EVALUATION_LINEAGE, f, indent=2)\n    \n    print()\n    print(f\"âœ“ Evaluation lineage saved: {eval_lineage_path}\")\n    print(f\"  Pass Rate: {EVAL_PASS_RATE}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "plzavcyr2e",
   "source": "if run_evaluation:\n    import re\n    import tempfile\n    from huggingface_hub import hf_hub_download\n    \n    print(\"Uploading evaluation results to HuggingFace...\")\n    \n    # Upload to both LoRA and merged repos\n    repos_to_update = [\n        f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n        f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n    ]\n    \n    for repo_id in repos_to_update:\n        try:\n            # Upload evaluation lineage JSON\n            api.upload_file(\n                path_or_fileobj=eval_lineage_path,\n                path_in_repo=\"evaluation_lineage.json\",\n                repo_id=repo_id,\n                token=HF_TOKEN,\n            )\n            \n            # Download and update README with evaluation section\n            try:\n                readme_path = hf_hub_download(repo_id=repo_id, filename=\"README.md\", token=HF_TOKEN)\n                with open(readme_path, 'r') as f:\n                    existing_readme = f.read()\n                \n                if \"## Evaluation Results\" in existing_readme:\n                    pattern = r'## Evaluation Results.*?(?=\\n## |\\Z)'\n                    updated_readme = re.sub(pattern, MODEL_CARD_EVAL_SECTION, existing_readme, flags=re.DOTALL)\n                else:\n                    updated_readme = existing_readme.rstrip() + \"\\n\\n\" + MODEL_CARD_EVAL_SECTION\n                \n                with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:\n                    f.write(updated_readme)\n                    temp_readme = f.name\n                \n                api.upload_file(\n                    path_or_fileobj=temp_readme,\n                    path_in_repo=\"README.md\",\n                    repo_id=repo_id,\n                    token=HF_TOKEN,\n                )\n                print(f\"  âœ“ Updated: {repo_id}\")\n            except Exception as e:\n                print(f\"  âš ï¸  Could not update README for {repo_id}: {e}\")\n        \n        except Exception as e:\n            print(f\"  âš ï¸  Failed to update {repo_id}: {e}\")\n    \n    print()\n    print(\"=\" * 60)\n    print(\"âœ“ EVALUATION RESULTS UPLOADED\")\n    print(\"=\" * 60)\n    print(f\"Pass Rate: {EVAL_PASS_RATE}%\")\n    print(f\"View model card: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": "## Done!\n\nYour model has been trained, evaluated, and uploaded to HuggingFace!\n\n### What You Accomplished\n\n| Step | Description |\n|------|-------------|\n| âœ… Fine-tuned | Trained a language model to use the Claudesidian toolset |\n| âœ… Lineage | Captured complete training metadata for reproducibility |\n| âœ… Model Card | Auto-generated with all hyperparameters |\n| âœ… Formats | Created LoRA, merged 16-bit, and GGUF versions |\n| âœ… Evaluation | Tested tool-calling accuracy (if enabled) |\n| âœ… Published | Model card includes training AND evaluation results |\n\n### Your Lineage Files\n\n| File | Location | Description |\n|------|----------|-------------|\n| `training_lineage.json` | HuggingFace + Google Drive | All training parameters |\n| `evaluation_lineage.json` | HuggingFace + Google Drive | Test results by category |\n| `README.md` | HuggingFace | Auto-generated model card |\n\n### Next Steps\n\n**Test locally with LM Studio:**\n1. Open LM Studio â†’ \"Discover\" tab\n2. Search for your model name\n3. Download the GGUF version\n4. Test with tool-calling prompts\n\n**Test with Ollama:**\n```bash\nollama pull {your-username}/{model-name}\nollama run {your-username}/{model-name}\n```\n\n**Refine with KTO:**\nAfter SFT, you can further improve your model with KTO (preference learning) to teach it to prefer better tool calls over worse ones.\n\n---\n\n**Questions?** Check the [Evaluator README](https://github.com/ProfSynapse/Toolset-Training/blob/main/Evaluator/README.md) or open an issue on GitHub."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}