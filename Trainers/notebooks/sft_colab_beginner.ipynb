{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tool-Calling Fine-Tuning with SFT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_beginner.ipynb)\n",
    "\n",
    "## üéì What You'll Learn\n",
    "\n",
    "This notebook teaches you how to fine-tune a language model to use the **Claudesidian-MCP toolset** for Obsidian vault operations. By the end, you'll have:\n",
    "\n",
    "- **A custom AI model** that can call tools to manage Obsidian vaults\n",
    "- **Hands-on experience** with supervised fine-tuning (SFT)\n",
    "- **Understanding** of hyperparameters and how they affect training\n",
    "\n",
    "## üî¨ What is SFT?\n",
    "\n",
    "**SFT (Supervised Fine-Tuning)** is like teaching through examples:\n",
    "- You show the model examples of correct tool-calling behavior\n",
    "- The model learns to replicate those patterns\n",
    "- Use SFT when teaching a model a **new skill** (like using tools)\n",
    "\n",
    "**When to use SFT:**\n",
    "- ‚úÖ Teaching tool-calling from scratch\n",
    "- ‚úÖ Learning new task formats\n",
    "- ‚úÖ Initial training with positive examples\n",
    "\n",
    "**Not for:**\n",
    "- ‚ùå Refining existing behavior (use KTO instead)\n",
    "- ‚ùå Teaching preferences between good/bad outputs (use preference learning)\n",
    "\n",
    "## üíª Hardware Requirements\n",
    "\n",
    "**Recommended GPU:**\n",
    "- 7B models: T4 (15GB VRAM) - ‚úÖ **Free Colab tier works!**\n",
    "- 13B models: A100 (40GB VRAM) - Colab Pro\n",
    "- 70B models: A100 (80GB VRAM) - Colab Pro+\n",
    "\n",
    "**Training time:** ~45 minutes for a 7B model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets>=2.14.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to Google Drive so they persist if runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Google Drive mounted\")\n",
    "print(f\"‚úì Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets:\n",
    "1. Click the üîë key icon in the left sidebar\n",
    "2. Add new secret: `HF_TOKEN`\n",
    "3. Get token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Get your HuggingFace username automatically\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"‚úì HuggingFace token loaded\")\n",
    "print(f\"‚úì Username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "**What this does:** Choose the base model you want to fine-tune and configure basic settings.\n",
    "\n",
    "Think of this like choosing which \"brain\" you want to teach tool-calling skills to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# @title ‚öôÔ∏è Model & Dataset Configuration\n# @markdown Use the dropdowns to select your model and configure your dataset.\n\n# @markdown ### üß† Base Model Selection\n# @markdown Choose a model based on your VRAM availability. Models are ordered by size (1B - 24B).\n# @markdown * **1B-3B:** Fast, runs on any GPU\n# @markdown * **7B-9B:** Standard balance of speed/intelligence\n# @markdown * **12B-24B:** High intelligence, requires ~12GB-24GB VRAM (A100 recommended for 20B+)\nMODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # @param [\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\", \"unsloth/gemma-2-2b-it-bnb-4bit\", \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", \"unsloth/Phi-3.5-mini-instruct\", \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\", \"unsloth/mistral-7b-v0.3-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\", \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\", \"unsloth/gemma-2-9b-it-bnb-4bit\", \"unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\", \"unsloth/Mistral-Nemo-Instruct-v1-bnb-4bit\", \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\", \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\", \"unsloth/Phi-4\", \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", \"unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit\", \"unsloth/Magistral-Small-2509-unsloth-bnb-4bit\"]\n\n# @markdown ### üìè Max Output Length\n# @markdown  2048 is standard. Higher values require more VRAM.\nMAX_SEQ_LENGTH = 2048 # @param [1024, 2048, 4096, 8192] {type:\"raw\"}\n\n# @markdown ### üìö Dataset Configuration\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\" # @param {type:\"string\"}\nDATASET_FILE = \"tools_sft_v1.5_11.29.25.jsonl\" # @param {type:\"string\"}\n\n# @markdown ### üè∑Ô∏è Output Model Name\n# @markdown Name your fine-tuned model (e.g., `my-tool-model-v1`).\nOUTPUT_MODEL_NAME = \"nexus-tools-sft-7b\" # @param {type:\"string\"}\n\nprint(f\"‚úì Configuration set:\")\nprint(f\"  ‚Ä¢ Model: {MODEL_NAME}\")\nprint(f\"  ‚Ä¢ Context: {MAX_SEQ_LENGTH}\")\nprint(f\"  ‚Ä¢ Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"  ‚Ä¢ Output: {OUTPUT_MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "**What this does:** Downloads the base model and prepares it for training.\n",
    "\n",
    "The model is the \"brain\" that will learn tool-calling. The tokenizer converts text into numbers the model can process. We use 4-bit quantization to fit large models into limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU and store info for lineage\n",
    "GPU_NAME = torch.cuda.get_device_name(0)\n",
    "CUDA_VERSION = torch.version.cuda\n",
    "GPU_MEMORY_GB = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "print(f\"Using GPU: {GPU_NAME}\")\n",
    "print(f\"CUDA version: {CUDA_VERSION}\")\n",
    "print(f\"Available VRAM: {GPU_MEMORY_GB:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Load the base model and tokenizer from HuggingFace\n# This downloads the model weights (~7GB for 7B models)\n#\n# Parameters explained:\n#   model_name: Which model to download\n#   max_seq_length: Max tokens model can process at once\n#   dtype=None: Auto-detect best precision for your GPU\n#   load_in_4bit=True: Use 4-bit quantization to save memory\n#   token: Your HF token for accessing the model\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=MODEL_NAME,\n    max_seq_length=MAX_SEQ_LENGTH,\n    dtype=None,  # Auto-detect (usually bfloat16 or float16)\n    load_in_4bit=True,  # Reduces memory usage by ~75%\n    token=HF_TOKEN,\n)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# CRITICAL: Apply chat template BEFORE training using Unsloth\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# This ensures the special tokens (<|im_start|>, <|im_end|>, [INST], etc.)\n# are properly handled as single tokens, not as literal text strings.\n# Without this, the model will output special tokens as literal text!\n\nfrom unsloth.chat_templates import get_chat_template\n\n# Detect the correct chat template based on model name\nif \"qwen\" in MODEL_NAME.lower():\n    CHAT_TEMPLATE_NAME = \"chatml\"  # Qwen uses ChatML format\nelif \"llama\" in MODEL_NAME.lower():\n    CHAT_TEMPLATE_NAME = \"llama-3\"\nelif \"mistral\" in MODEL_NAME.lower():\n    CHAT_TEMPLATE_NAME = \"mistral\"\nelif \"gemma\" in MODEL_NAME.lower():\n    CHAT_TEMPLATE_NAME = \"gemma\"\nelif \"phi\" in MODEL_NAME.lower():\n    CHAT_TEMPLATE_NAME = \"phi-3\"\nelif \"deepseek\" in MODEL_NAME.lower():\n    CHAT_TEMPLATE_NAME = \"chatml\"  # DeepSeek uses ChatML\nelse:\n    CHAT_TEMPLATE_NAME = \"chatml\"  # Default fallback\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template=CHAT_TEMPLATE_NAME,\n)\n\nprint(f\"Applied {CHAT_TEMPLATE_NAME} chat template\")\n\n# Store for lineage\nTOTAL_PARAMS = sum(p.numel() for p in model.parameters())\n\nprint(\"Model loaded successfully\")\nprint(f\"  Model has {TOTAL_PARAMS:,} parameters\")\nprint(f\"  Chat template: {CHAT_TEMPLATE_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA Adapters\n",
    "\n",
    "**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n",
    "\n",
    "Think of LoRA like teaching a new skill through muscle memory - we add small specialized layers that learn the new behavior, while keeping the main \"brain\" frozen. This is way faster and uses less memory than retraining everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üîß LoRA Adapter Configuration\n",
    "# @markdown Configure the size and strength of the fine-tuning adapters.\n",
    "\n",
    "# @markdown ### üéõÔ∏è LoRA Parameters\n",
    "# @markdown **Rank (r):** Higher = smarter but slower/more memory (Standard: 16-64).\n",
    "# @markdown Alpha will be automatically set to 2 * r.\n",
    "LORA_R = 32 # @param [8, 16, 32, 64, 128] {type:\"raw\"}\n",
    "\n",
    "LORA_ALPHA = LORA_R * 2\n",
    "\n",
    "# @markdown **Dropout:** Helps prevent overfitting (Standard: 0.05).\n",
    "LORA_DROPOUT = 0.05 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown **Random Seed:** Change this for different initialization.\n",
    "RANDOM_STATE = 3407 # @param {type:\"integer\"}\n",
    "\n",
    "# Target modules for LoRA\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                  \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Store for lineage\n",
    "TRAINABLE_PARAMS = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úì LoRA adapters applied:\")\n",
    "print(f\"  ‚Ä¢ Rank: {LORA_R}\")\n",
    "print(f\"  ‚Ä¢ Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  ‚Ä¢ Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  ‚Ä¢ Trainable params: {TRAINABLE_PARAMS:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Load and Prepare Dataset\n",
    "\n",
    "**What this does:** Downloads training examples and formats them for the model.\n",
    "\n",
    "The dataset contains examples of correct tool-calling behavior. Think of it like a textbook full of solved problems that the model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\n\"\"\"\nLOAD DATASET FROM HUGGINGFACE\n\nThis downloads pre-made training examples of correct tool-calling behavior.\nEach example shows: user request ‚Üí tool call ‚Üí result ‚Üí assistant response\n\"\"\"\n\nprint(f\"Loading dataset: {DATASET_NAME}/{DATASET_FILE}\")\ndataset = load_dataset(\n    DATASET_NAME,  # HuggingFace repository containing the dataset\n    data_files=DATASET_FILE,  # Specific JSONL file to use\n    split=\"train\"  # Use the training split\n)\n\n# Store dataset info for lineage\nDATASET_SIZE = len(dataset)\n\nprint(f\"Loaded {DATASET_SIZE} examples\")\nprint(f\"\\nSample example:\")\nprint(dataset[0])\n\n\"\"\"\nCONVERT TOOL CALLS FORMAT FOR QWEN MODELS\n\nQwen models expect a flattened tool_calls format:\n  - Qwen format:  tool_call.name, tool_call.arguments\n  - OpenAI format: tool_call.function.name, tool_call.function.arguments\n\nThis conversion ensures the dataset works with Qwen's native chat template.\n\"\"\"\n\ndef convert_to_qwen_tool_format(example):\n    \"\"\"\n    Convert OpenAI-style tool_calls to Qwen-compatible format.\n\n    OpenAI format:\n        {\"tool_calls\": [{\"function\": {\"name\": \"...\", \"arguments\": \"...\"}}]}\n\n    Qwen format:\n        {\"tool_calls\": [{\"name\": \"...\", \"arguments\": \"...\"}]}\n    \"\"\"\n    conversations = example.get(\"conversations\", [])\n    converted_conversations = []\n\n    for msg in conversations:\n        new_msg = dict(msg)  # Copy the message\n\n        if \"tool_calls\" in msg and msg[\"tool_calls\"]:\n            # Convert from OpenAI nested format to Qwen flat format\n            new_msg[\"tool_calls\"] = [\n                {\n                    \"name\": tc[\"function\"][\"name\"],\n                    \"arguments\": tc[\"function\"][\"arguments\"]\n                }\n                for tc in msg[\"tool_calls\"]\n                if \"function\" in tc  # Only convert if in OpenAI format\n            ]\n\n        converted_conversations.append(new_msg)\n\n    return {\"conversations\": converted_conversations}\n\n# Detect Qwen models and apply conversion\nis_qwen = 'qwen' in MODEL_NAME.lower()\n\nif is_qwen:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"QWEN MODEL DETECTED - Converting tool call format\")\n    print(\"=\" * 60)\n    print(\"Converting: OpenAI format -> Qwen format\")\n    print(\"  Before: tool_call.function.name\")\n    print(\"  After:  tool_call.name\")\n\n    dataset = dataset.map(\n        convert_to_qwen_tool_format,\n        desc=\"Converting to Qwen format\"\n    )\n\n    print(\"Tool calls converted to Qwen-compatible format\")\n    print()\nelse:\n    print(f\"\\nNon-Qwen model detected - keeping original tool call format\")\n\n# Chat template was already applied in cell-12 using get_chat_template\n# No need for manual template definitions here\nprint(f\"\\nUsing chat template: {CHAT_TEMPLATE_NAME} (applied in cell-12)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nFORMAT DATASET FOR TRAINING\n\nConvert the conversation format into the exact text format the model expects.\nThis applies the chat template (already configured in cell-12) to each example.\n\"\"\"\nimport json\n\ndef render_tool_calls_to_content(tool_calls):\n    \"\"\"\n    Convert tool_calls array to text content for Qwen ChatML format.\n    \n    Renders as:\n    <tool_call>\n    {\"name\": \"toolName\", \"arguments\": {...}}\n    </tool_call>\n    \"\"\"\n    if not tool_calls:\n        return \"\"\n    \n    rendered_parts = []\n    for tc in tool_calls:\n        # Handle OpenAI nested format: {\"function\": {\"name\": ..., \"arguments\": ...}}\n        if \"function\" in tc and tc[\"function\"]:\n            func = tc[\"function\"]\n            name = func.get(\"name\", \"\")\n            args = func.get(\"arguments\", \"{}\")\n        else:\n            # Handle flat format: {\"name\": ..., \"arguments\": ...}\n            name = tc.get(\"name\", \"\")\n            args = tc.get(\"arguments\", \"{}\")\n        \n        # Parse arguments if it's a string, keep as-is if dict\n        if isinstance(args, str):\n            try:\n                args_obj = json.loads(args)\n            except:\n                args_obj = args\n        else:\n            args_obj = args\n        \n        # Format the tool call\n        tool_call_obj = {\"name\": name, \"arguments\": args_obj}\n        rendered_parts.append(\n            f\"<tool_call>\\n{json.dumps(tool_call_obj, indent=2)}\\n</tool_call>\"\n        )\n    \n    return \"\\n\".join(rendered_parts)\n\n\ndef sanitize_conversations(conversations):\n    \"\"\"\n    Ensure all message fields are properly set and tool_calls are rendered to content.\n    \"\"\"\n    sanitized = []\n    for msg in conversations:\n        new_msg = dict(msg)\n        \n        # Get existing content (or empty string if None)\n        content = new_msg.get(\"content\") or \"\"\n        \n        # If there are tool_calls, render them to content\n        if \"tool_calls\" in new_msg and new_msg[\"tool_calls\"]:\n            tool_content = render_tool_calls_to_content(new_msg[\"tool_calls\"])\n            if tool_content:\n                # Combine existing content with tool calls\n                if content:\n                    content = f\"{content}\\n\\n{tool_content}\"\n                else:\n                    content = tool_content\n        \n        new_msg[\"content\"] = content\n        \n        # Remove tool_calls since we've rendered them to content\n        # (the chat template doesn't handle them)\n        if \"tool_calls\" in new_msg:\n            del new_msg[\"tool_calls\"]\n        \n        sanitized.append(new_msg)\n    return sanitized\n\n\ndef format_chat_template(example):\n    \"\"\"\n    Convert conversations to tokenizer's chat template.\n\n    Input: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, ...]}\n    Output: {\"text\": \"<|im_start|>user\\n...<|im_end|>\\n...\"}\n    \"\"\"\n    # Sanitize conversations and render tool_calls to content\n    conversations = sanitize_conversations(example[\"conversations\"])\n    \n    text = tokenizer.apply_chat_template(\n        conversations,\n        tokenize=False,\n        add_generation_prompt=False\n    )\n    return {\"text\": text}\n\n# Apply formatting to entire dataset\ndataset = dataset.map(\n    format_chat_template,\n    remove_columns=dataset.column_names,\n    desc=\"Formatting dataset\"\n)\n\nprint(\"Dataset formatted for training\")\nprint(f\"Chat template: {CHAT_TEMPLATE_NAME}\")\nprint(f\"\\nFormatted example (first 1000 characters):\")\nprint(dataset[0][\"text\"][:1000])\nprint(\"\\n... (truncated)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "**What this does:** Set the hyperparameters that control how the model learns.\n",
    "\n",
    "This is the **most important section** - these settings determine how fast the model learns, how much memory it uses, and how good the final result will be. Think of it like configuring a study plan: how many hours per day, how many review sessions, how to handle difficult material, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped output directory\n",
    "TRAINING_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{DRIVE_OUTPUT_DIR}/{TRAINING_TIMESTAMP}\"\n",
    "\n",
    "# @title üèÉ Training Hyperparameters\n",
    "# @markdown Control the speed and quality of training.\n",
    "\n",
    "# @markdown ### ‚ö° Performance Settings\n",
    "# @markdown **Batch Size:** Examples per step. Lower if you run out of memory.\n",
    "BATCH_SIZE = 2 # @param [1, 2, 4, 6, 8, 10, 12, 16] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Gradient Accumulation:** Simulates larger batches.\n",
    "GRADIENT_ACCUMULATION = 4 # @param [1, 2, 4, 6, 8, 10, 12, 16] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### üß† Learning Rate Configuration\n",
    "# @markdown **Step 1: Choose the Magnitude (Exponent)**\n",
    "# @markdown This is the most important setting. It determines the \"speed\" of learning.\n",
    "# @markdown * **4** = Standard (1e-4). Recommended for 7B models and SFT.\n",
    "# @markdown * **5** = Slow (1e-5). Use if training is unstable or for larger models.\n",
    "# @markdown * **6** = Very Slow (1e-6). Precise but takes much longer.\n",
    "LEARNING_RATE_EXPONENT = 4 # @param [4, 5, 6, 7] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Step 2: Choose the Multiplier**\n",
    "# @markdown Fine-tunes the rate within that magnitude (e.g., Multiplier 2 + Exponent 4 = 2e-4).\n",
    "LEARNING_RATE_MULTIPLIER = 2 # @param [1, 2, 3, 4, 5, 6, 7, 8, 9] {type:\"raw\"}\n",
    "\n",
    "LEARNING_RATE = LEARNING_RATE_MULTIPLIER * (10 ** -LEARNING_RATE_EXPONENT)\n",
    "\n",
    "# @markdown ### üîÑ Epochs\n",
    "# @markdown Number of passes through the dataset.\n",
    "NUM_EPOCHS = 3 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### üíæ Saving & Logging\n",
    "SAVE_STEPS = 50 # @param {type:\"integer\"}\n",
    "LOGGING_STEPS = 5 # @param {type:\"integer\"}\n",
    "\n",
    "# Other training settings\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LR_SCHEDULER = \"cosine\"\n",
    "OPTIMIZER = \"adamw_8bit\"\n",
    "USE_BF16 = is_bfloat16_supported()\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    lr_scheduler_type=LR_SCHEDULER,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    fp16=not USE_BF16,\n",
    "    bf16=USE_BF16,\n",
    "    optim=OPTIMIZER,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    seed=RANDOM_STATE,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Calculate effective batch size\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION\n",
    "\n",
    "print(\"‚úì Training configuration ready\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Gradient Accum.: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  ‚Ä¢ Effective Batch: {EFFECTIVE_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {LEARNING_RATE} ({LEARNING_RATE_MULTIPLIER}e-{LEARNING_RATE_EXPONENT})\")\n",
    "print(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Output Dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Initialize Trainer\n",
    "\n",
    "**What this does:** Creates the training engine that coordinates everything.\n",
    "\n",
    "The SFTTrainer is the orchestrator - it takes the model, dataset, and configuration, then handles all the training mechanics (gradient updates, checkpointing, logging, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SFTTrainer\n",
    "# This combines the model, dataset, and configuration into one training pipeline\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The model with LoRA adapters\n",
    "    tokenizer=tokenizer,  # For converting text to tokens\n",
    "    train_dataset=dataset,  # Our formatted training examples\n",
    "    args=training_args,  # All the hyperparameters we configured\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")\n",
    "print(\"  Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 10. Train!\n",
    "\n",
    "**What this does:** The actual learning happens here!\n",
    "\n",
    "The model will:\n",
    "1. **Read examples** from the dataset\n",
    "2. **Predict** what the response should be\n",
    "3. **Compare** its prediction to the correct answer\n",
    "4. **Update weights** to get closer to the correct answer\n",
    "5. **Repeat** this process for 3 epochs (3 full passes through the data)\n",
    "\n",
    "**What to expect:**\n",
    "- Training takes ~45 minutes for 7B models on T4 GPU\n",
    "- You'll see progress updates every 5 steps\n",
    "- Loss should generally decrease over time (learning is working!)\n",
    "- Checkpoints are saved every 100 steps to Google Drive\n",
    "\n",
    "**What the metrics mean:**\n",
    "- **Loss:** How \"wrong\" the model is (lower = better, aim for <1.0)\n",
    "- **Learning Rate:** Gradually decreases as training progresses\n",
    "- **Samples/sec:** Training speed (depends on GPU)\n",
    "\n",
    "**üíæ Checkpoint Resumption:**\n",
    "If your Colab session disconnects, don't worry! Your checkpoints are saved to Google Drive. You can resume training by:\n",
    "1. Re-running cells 1-9 (setup, model loading, dataset prep, config)\n",
    "2. In the training cell below, the code will automatically detect the latest checkpoint and resume from there\n",
    "3. Your progress is preserved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üîç Check for existing checkpoints (automatic resumption)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Found checkpoints - get the latest one\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = latest_checkpoint\n",
    "    print(f\"‚úì Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"  Resuming training from this checkpoint\")\n",
    "    print(f\"  Total checkpoints found: {len(checkpoint_dirs)}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No existing checkpoints found - starting fresh training\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Start training (or resume)\n",
    "print(\"=\" * 60)\n",
    "if resume_from_checkpoint:\n",
    "    print(\"RESUMING TRAINING FROM CHECKPOINT\")\n",
    "else:\n",
    "    print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Track training time\n",
    "training_start_time = time.time()\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "# Calculate training duration\n",
    "TRAINING_DURATION_SECONDS = time.time() - training_start_time\n",
    "TRAINING_DURATION_MINUTES = TRAINING_DURATION_SECONDS / 60\n",
    "\n",
    "# Store final metrics for lineage\n",
    "FINAL_LOSS = trainer_stats.training_loss\n",
    "TOTAL_STEPS = trainer_stats.global_step\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final loss: {FINAL_LOSS:.4f}\")\n",
    "print(f\"Total steps: {TOTAL_STEPS}\")\n",
    "print(f\"Training time: {TRAINING_DURATION_MINUTES:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 11. Build Training Lineage\n",
    "\n",
    "**What this does:** Captures all training metadata for reproducibility and analysis.\n",
    "\n",
    "This creates a complete record of:\n",
    "- Base model and configuration\n",
    "- Dataset details\n",
    "- All hyperparameters used\n",
    "- Training results and metrics\n",
    "- Hardware and environment info\n",
    "\n",
    "This information will be automatically added to your HuggingFace model card!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datetime import datetime\n\n\"\"\"\nBUILD COMPLETE TRAINING LINEAGE\n\nThis dictionary captures EVERYTHING about the training run for:\n- Reproducibility\n- Model card generation\n- Experiment tracking\n- Analysis and comparison\n\"\"\"\n\nTRAINING_LINEAGE = {\n    # IDENTIFICATION\n    \"model_name\": OUTPUT_MODEL_NAME,\n    \"training_method\": \"SFT\",\n    \"training_timestamp\": TRAINING_TIMESTAMP,\n    \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n    \n    # BASE MODEL\n    \"base_model\": {\n        \"name\": MODEL_NAME,\n        \"total_parameters\": TOTAL_PARAMS,\n        \"quantization\": \"4-bit\",\n        \"max_seq_length\": MAX_SEQ_LENGTH,\n        \"chat_template\": CHAT_TEMPLATE_NAME,  # Track chat template used\n        \"is_qwen\": is_qwen,\n    },\n    \n    # LORA CONFIGURATION\n    \"lora_config\": {\n        \"r\": LORA_R,\n        \"alpha\": LORA_ALPHA,\n        \"dropout\": LORA_DROPOUT,\n        \"target_modules\": TARGET_MODULES,\n        \"trainable_parameters\": TRAINABLE_PARAMS,\n        \"trainable_percentage\": round(TRAINABLE_PARAMS / TOTAL_PARAMS * 100, 4),\n    },\n    \n    # DATASET\n    \"dataset\": {\n        \"name\": DATASET_NAME,\n        \"file\": DATASET_FILE,\n        \"huggingface_url\": f\"https://huggingface.co/datasets/{DATASET_NAME}\",\n        \"total_examples\": DATASET_SIZE,\n        \"chat_template\": CHAT_TEMPLATE_NAME,\n        \"qwen_tool_format_conversion\": is_qwen,  # Track if conversion was applied\n    },\n    \n    # TRAINING HYPERPARAMETERS\n    \"training_config\": {\n        \"batch_size\": BATCH_SIZE,\n        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION,\n        \"effective_batch_size\": EFFECTIVE_BATCH_SIZE,\n        \"learning_rate\": LEARNING_RATE,\n        \"learning_rate_scheduler\": LR_SCHEDULER,\n        \"warmup_ratio\": WARMUP_RATIO,\n        \"max_grad_norm\": MAX_GRAD_NORM,\n        \"num_epochs\": NUM_EPOCHS,\n        \"optimizer\": OPTIMIZER,\n        \"precision\": \"bf16\" if USE_BF16 else \"fp16\",\n        \"gradient_checkpointing\": True,\n        \"packing\": False,\n        \"random_seed\": RANDOM_STATE,\n    },\n    \n    # TRAINING RESULTS\n    \"training_results\": {\n        \"final_loss\": round(FINAL_LOSS, 4),\n        \"total_steps\": TOTAL_STEPS,\n        \"training_duration_minutes\": round(TRAINING_DURATION_MINUTES, 1),\n    },\n    \n    # HARDWARE & ENVIRONMENT\n    \"hardware\": {\n        \"gpu\": GPU_NAME,\n        \"gpu_memory_gb\": round(GPU_MEMORY_GB, 1),\n        \"cuda_version\": CUDA_VERSION,\n        \"platform\": \"Google Colab\",\n    },\n    \n    # FRAMEWORK VERSIONS\n    \"framework_versions\": {\n        \"torch\": torch.__version__,\n        \"transformers\": __import__(\"transformers\").__version__,\n        \"trl\": __import__(\"trl\").__version__,\n        \"peft\": __import__(\"peft\").__version__,\n        \"unsloth\": \"latest\",\n    },\n}\n\n# Save lineage to file\nlineage_path = f\"{output_dir}/training_lineage.json\"\nwith open(lineage_path, \"w\") as f:\n    json.dump(TRAINING_LINEAGE, f, indent=2)\n\nprint(\"Training lineage captured!\")\nprint(f\"  Saved to: {lineage_path}\")\nprint()\nprint(\"Summary:\")\nprint(f\"  Base Model: {MODEL_NAME}\")\nprint(f\"  Chat Template: {CHAT_TEMPLATE_NAME}\")\nprint(f\"  Dataset: {DATASET_NAME}/{DATASET_FILE} ({DATASET_SIZE} examples)\")\nprint(f\"  Method: SFT (Supervised Fine-Tuning)\")\nprint(f\"  LoRA: r={LORA_R}, alpha={LORA_ALPHA}\")\nprint(f\"  LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}\")\nprint(f\"  Final Loss: {FINAL_LOSS:.4f}\")\nprint(f\"  Duration: {TRAINING_DURATION_MINUTES:.1f} min on {GPU_NAME}\")\nif is_qwen:\n    print(f\"  Qwen tool format conversion: Applied\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 12. Upload to HuggingFace\n",
    "\n",
    "**What this does:** Share your trained model with the world!\n",
    "\n",
    "The model card will be **automatically generated** with all your training details:\n",
    "- Base model and configuration\n",
    "- Dataset information\n",
    "- All hyperparameters\n",
    "- Training results\n",
    "- Hardware used\n",
    "\n",
    "We'll create **three versions** of your model:\n",
    "\n",
    "1. **LoRA adapters** - Small files that contain just the changes\n",
    "2. **Merged 16-bit model** - Full model with adapters merged in\n",
    "3. **GGUF quantizations** - Optimized versions for CPU/GPU inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_card(lineage: dict, hf_username: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive HuggingFace model card from training lineage.\n",
    "    \n",
    "    This creates a professional README.md with all training details\n",
    "    for reproducibility and transparency.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_model = lineage[\"base_model\"][\"name\"]\n",
    "    dataset = lineage[\"dataset\"]\n",
    "    lora = lineage[\"lora_config\"]\n",
    "    training = lineage[\"training_config\"]\n",
    "    results = lineage[\"training_results\"]\n",
    "    hardware = lineage[\"hardware\"]\n",
    "    frameworks = lineage[\"framework_versions\"]\n",
    "    \n",
    "    model_card = f'''---\n",
    "language:\n",
    "- en\n",
    "license: apache-2.0\n",
    "library_name: transformers\n",
    "tags:\n",
    "- tool-calling\n",
    "- sft\n",
    "- supervised-fine-tuning\n",
    "- claudesidian\n",
    "- obsidian\n",
    "- fine-tuned\n",
    "- unsloth\n",
    "base_model: {base_model}\n",
    "datasets:\n",
    "- {dataset[\"name\"]}\n",
    "pipeline_tag: text-generation\n",
    "model-index:\n",
    "- name: {lineage[\"model_name\"]}\n",
    "  results:\n",
    "  - task:\n",
    "      type: text-generation\n",
    "    metrics:\n",
    "    - name: Final Loss\n",
    "      type: loss\n",
    "      value: {results[\"final_loss\"]}\n",
    "---\n",
    "\n",
    "# {lineage[\"model_name\"]}\n",
    "\n",
    "This model was fine-tuned using **SFT (Supervised Fine-Tuning)** to learn tool-calling behavior for the Claudesidian vault application.\n",
    "\n",
    "## Model Description\n",
    "\n",
    "- **Base Model:** [{base_model}](https://huggingface.co/{base_model})\n",
    "- **Training Method:** SFT (Supervised Fine-Tuning)\n",
    "- **Task:** Tool-calling for Obsidian vault operations\n",
    "- **Training Date:** {lineage[\"training_date\"]}\n",
    "\n",
    "## Training Details\n",
    "\n",
    "### Dataset\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Dataset | [{dataset[\"name\"]}]({dataset[\"huggingface_url\"]}) |\n",
    "| File | {dataset[\"file\"]} |\n",
    "| Total Examples | {dataset[\"total_examples\"]:,} |\n",
    "| Chat Template | {dataset[\"chat_template\"]} |\n",
    "\n",
    "### LoRA Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Rank (r) | {lora[\"r\"]} |\n",
    "| Alpha (Œ±) | {lora[\"alpha\"]} |\n",
    "| Dropout | {lora[\"dropout\"]} |\n",
    "| Target Modules | {', '.join(lora[\"target_modules\"])} |\n",
    "| Trainable Parameters | {lora[\"trainable_parameters\"]:,} ({lora[\"trainable_percentage\"]}%) |\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Batch Size | {training[\"batch_size\"]} |\n",
    "| Gradient Accumulation | {training[\"gradient_accumulation_steps\"]} |\n",
    "| Effective Batch Size | {training[\"effective_batch_size\"]} |\n",
    "| Learning Rate | {training[\"learning_rate\"]} |\n",
    "| LR Scheduler | {training[\"learning_rate_scheduler\"]} |\n",
    "| Warmup Ratio | {training[\"warmup_ratio\"]} |\n",
    "| Max Grad Norm | {training[\"max_grad_norm\"]} |\n",
    "| Epochs | {training[\"num_epochs\"]} |\n",
    "| Optimizer | {training[\"optimizer\"]} |\n",
    "| Precision | {training[\"precision\"]} |\n",
    "| Packing | {training[\"packing\"]} |\n",
    "| Random Seed | {training[\"random_seed\"]} |\n",
    "\n",
    "### Training Results\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Final Loss | {results[\"final_loss\"]} |\n",
    "| Total Steps | {results[\"total_steps\"]:,} |\n",
    "| Training Duration | {results[\"training_duration_minutes\"]} minutes |\n",
    "\n",
    "### Hardware\n",
    "\n",
    "| Component | Value |\n",
    "|-----------|-------|\n",
    "| GPU | {hardware[\"gpu\"]} |\n",
    "| GPU Memory | {hardware[\"gpu_memory_gb\"]} GB |\n",
    "| CUDA Version | {hardware[\"cuda_version\"]} |\n",
    "| Platform | {hardware[\"platform\"]} |\n",
    "\n",
    "### Framework Versions\n",
    "\n",
    "| Library | Version |\n",
    "|---------|--------|\n",
    "| PyTorch | {frameworks[\"torch\"]} |\n",
    "| Transformers | {frameworks[\"transformers\"]} |\n",
    "| TRL | {frameworks[\"trl\"]} |\n",
    "| PEFT | {frameworks[\"peft\"]} |\n",
    "| Unsloth | {frameworks[\"unsloth\"]} |\n",
    "\n",
    "## Usage\n",
    "\n",
    "### With Transformers\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{hf_username}/{lineage[\"model_name\"]}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{hf_username}/{lineage[\"model_name\"]}\")\n",
    "\n",
    "# Example tool-calling prompt\n",
    "messages = [{{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Show me the contents of my project roadmap file.\"\n",
    "}}]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_new_tokens=512)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "### With Ollama (GGUF)\n",
    "\n",
    "```bash\n",
    "# Download the GGUF version\n",
    "ollama pull {hf_username}/{lineage[\"model_name\"]}\n",
    "\n",
    "# Run inference\n",
    "ollama run {hf_username}/{lineage[\"model_name\"]}\n",
    "```\n",
    "\n",
    "### With LM Studio\n",
    "\n",
    "1. Open LM Studio ‚Üí \"Discover\" tab\n",
    "2. Search for `{hf_username}/{lineage[\"model_name\"]}`\n",
    "3. Download the Q4_K_M or Q5_K_M GGUF version\n",
    "4. Load and test with tool-calling prompts\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is designed for:\n",
    "- Tool-calling in Obsidian vault management applications\n",
    "- Claudesidian MCP integration\n",
    "- Local AI assistants that interact with note-taking systems\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Trained specifically for Claudesidian tool schemas\n",
    "- May not generalize to other tool-calling formats\n",
    "- Best performance with the specific tool set it was trained on\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{{{lineage[\"model_name\"].replace(\"-\", \"_\")},\n",
    "  author = {{{hf_username}}},\n",
    "  title = {{{lineage[\"model_name\"]}: SFT Fine-tuned Tool-Calling Model}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{HuggingFace}},\n",
    "  url = {{https://huggingface.co/{hf_username}/{lineage[\"model_name\"]}}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## Training Lineage\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand full training configuration (JSON)</summary>\n",
    "\n",
    "```json\n",
    "{json.dumps(lineage, indent=2)}\n",
    "```\n",
    "\n",
    "</details>\n",
    "'''\n",
    "    \n",
    "    return model_card\n",
    "\n",
    "# Generate model card\n",
    "MODEL_CARD = generate_model_card(TRAINING_LINEAGE, hf_user)\n",
    "\n",
    "# Save model card locally\n",
    "model_card_path = f\"{output_dir}/README.md\"\n",
    "with open(model_card_path, \"w\") as f:\n",
    "    f.write(MODEL_CARD)\n",
    "\n",
    "print(\"‚úì Model card generated!\")\n",
    "print(f\"  Saved to: {model_card_path}\")\n",
    "print()\n",
    "print(\"Preview (first 50 lines):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\".join(MODEL_CARD.split(\"\\n\")[:50]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_file\n",
    "\n",
    "# Upload LoRA adapters with model card\n",
    "print(\"Uploading LoRA adapters...\")\n",
    "\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload model card (README.md)\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card_path,\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Upload training lineage JSON\n",
    "api.upload_file(\n",
    "    path_or_fileobj=lineage_path,\n",
    "    path_in_repo=\"training_lineage.json\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"‚úì LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"‚úì Model card with full lineage uploaded\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model with model card\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "# Upload model card to merged repo too\n",
    "api.upload_file(\n",
    "    path_or_fileobj=model_card_path,\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=lineage_path,\n",
    "    path_in_repo=\"training_lineage.json\",\n",
    "    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Merged model uploaded to HuggingFace\")\n",
    "print(f\"‚úì Model card with full lineage uploaded\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "# Create GGUF quantizations\nquantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n\nprint(\"Creating GGUF quantizations...\")\nprint(f\"This will create {len(quantization_methods)} versions\")\nprint()\n\nmodel.push_to_hub_gguf(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    tokenizer,\n    quantization_method=quantization_methods,\n    token=HF_TOKEN,\n)\n\n# Re-upload model card (GGUF upload overwrites it with Unsloth's generic template)\napi.upload_file(\n    path_or_fileobj=model_card_path,\n    path_in_repo=\"README.md\",\n    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    token=HF_TOKEN,\n)\n\napi.upload_file(\n    path_or_fileobj=lineage_path,\n    path_in_repo=\"training_lineage.json\",\n    repo_id=f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    token=HF_TOKEN,\n)\n\nprint()\nprint(\"‚úì GGUF quantizations created and uploaded!\")\nprint(\"‚úì Model card restored (GGUF upload overwrites it)\")\nprint(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "id": "vjfp44pq4q",
   "metadata": {},
   "source": [
    "## 13. Evaluate Model (Optional)\n",
    "\n",
    "**What this does:** Run automated tests to measure your model's tool-calling accuracy.\n",
    "\n",
    "This will:\n",
    "- Load your trained model with vLLM for fast inference\n",
    "- Run test prompts covering all 47 tools\n",
    "- Calculate pass rates by category\n",
    "- Generate evaluation lineage that can be added to your model card\n",
    "\n",
    "**Skip this section** if you want to evaluate later using the standalone evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ugsy4u6gaf",
   "metadata": {},
   "outputs": [],
   "source": "# @title Run Evaluation (Optional)\n# @markdown Test your trained model's tool-calling accuracy and behavioral patterns.\n\n# @markdown ### Enable Evaluation\nrun_evaluation = True # @param {type:\"boolean\"}\n\n# @markdown ### Test Suite Selection\n# @markdown * **Tool Coverage (47 tools):** Tests each tool individually\n# @markdown * **Behavioral Patterns (24 tests):** Tests context efficiency, executePrompt delegation, etc.\n# @markdown * **All Suites:** Runs both tool coverage and behavioral patterns\neval_test_suite = \"All Suites\" # @param [\"Tool Coverage (47 tools)\", \"Behavioral Patterns (24 tests)\", \"All Suites\"]\n\nif run_evaluation:\n    print(\"Installing vLLM for evaluation...\")\n    import subprocess\n    subprocess.run([\"pip\", \"install\", \"-q\", \"vllm>=0.6.0\"], check=True)\n    \n    # Download evaluator framework\n    import requests\n    from pathlib import Path\n    \n    os.makedirs(\"Evaluator/prompts\", exist_ok=True)\n    os.makedirs(\"Evaluator/results\", exist_ok=True)\n    os.makedirs(\"tools\", exist_ok=True)\n    \n    REPO_BASE = \"https://raw.githubusercontent.com/ProfSynapse/Toolset-Training/main\"\n    \n    eval_files = {\n        \"Evaluator/__init__.py\": \"Evaluator/__init__.py\",\n        \"Evaluator/runner.py\": \"Evaluator/runner.py\",\n        \"Evaluator/schema_validator.py\": \"Evaluator/schema_validator.py\",\n        \"Evaluator/prompt_sets.py\": \"Evaluator/prompt_sets.py\",\n        \"Evaluator/reporting.py\": \"Evaluator/reporting.py\",\n        \"Evaluator/config.py\": \"Evaluator/config.py\",\n        \"Evaluator/prompts/tool_prompts.json\": \"Evaluator/prompts/tool_prompts.json\",\n        \"Evaluator/prompts/baseline.json\": \"Evaluator/prompts/baseline.json\",\n        \"Evaluator/prompts/behavioral_patterns.json\": \"Evaluator/prompts/behavioral_patterns.json\",\n        \"tools/tool_schemas.json\": \"tools/tool_schemas.json\",\n    }\n    \n    print(\"Downloading evaluation framework...\")\n    for remote_path, local_path in eval_files.items():\n        url = f\"{REPO_BASE}/{remote_path}\"\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n            with open(local_path, 'w', encoding='utf-8') as f:\n                f.write(response.text)\n        except Exception as e:\n            print(f\"  Warning: Failed to download {remote_path}\")\n    \n    print(\"Evaluation framework ready\")\nelse:\n    print(\"Evaluation skipped. Set run_evaluation = True to enable.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3kehv1j",
   "metadata": {},
   "outputs": [],
   "source": "if run_evaluation:\n    from vllm import LLM, SamplingParams\n    from dataclasses import dataclass\n    from typing import Any, Dict, Mapping, Sequence\n    import time\n    import sys\n    \n    sys.path.insert(0, '/content')\n    \n    # Use the merged model for evaluation\n    EVAL_MODEL = f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\"\n    \n    print(f\"Loading model for evaluation: {EVAL_MODEL}\")\n    print(\"This may take 1-2 minutes...\")\n    \n    # Initialize vLLM\n    eval_llm = LLM(\n        model=EVAL_MODEL,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n        max_model_len=2048,\n        trust_remote_code=True,\n        dtype=\"auto\",\n        token=HF_TOKEN,\n    )\n    \n    # Get the vLLM tokenizer for proper chat template handling\n    eval_tokenizer = eval_llm.get_tokenizer()\n    \n    # Create vLLM client for evaluator\n    @dataclass\n    class VLLMResponse:\n        message: str\n        raw: Dict[str, Any]\n        latency_s: float\n    \n    class VLLMClient:\n        def __init__(self, llm, llm_tokenizer, temperature=0.2, max_tokens=1024, seed=42):\n            self.llm = llm\n            self.tokenizer = llm_tokenizer\n            self.temperature = temperature\n            self.max_tokens = max_tokens\n            self.seed = seed\n        \n        def chat(self, messages):\n            # Use the tokenizer's chat template instead of manual construction\n            # This ensures special tokens are handled correctly\n            prompt = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            \n            sampling_params = SamplingParams(\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                seed=self.seed,\n            )\n            \n            start = time.perf_counter()\n            outputs = self.llm.generate([prompt], sampling_params)\n            latency_s = time.perf_counter() - start\n            \n            output = outputs[0]\n            message = output.outputs[0].text.strip()\n            \n            return VLLMResponse(\n                message=message,\n                raw={\"output\": message},\n                latency_s=latency_s\n            )\n    \n    eval_client = VLLMClient(eval_llm, eval_tokenizer)\n    print(\"Model loaded for evaluation\")\n    \n    # Run evaluation\n    from Evaluator.prompt_sets import load_prompt_cases, filter_prompts\n    from Evaluator.runner import evaluate_cases\n    from Evaluator.reporting import build_evaluation_lineage, generate_evaluation_model_card_section\n    from Evaluator.config import PromptFilter\n    \n    # Map test suite to files - updated with behavioral patterns\n    eval_suite_map = {\n        \"Tool Coverage (47 tools)\": [\"Evaluator/prompts/tool_prompts.json\"],\n        \"Behavioral Patterns (24 tests)\": [\"Evaluator/prompts/behavioral_patterns.json\"],\n        \"All Suites\": [\n            \"Evaluator/prompts/tool_prompts.json\",\n            \"Evaluator/prompts/behavioral_patterns.json\"\n        ],\n    }\n    \n    eval_prompt_files = eval_suite_map[eval_test_suite]\n    all_eval_records = []\n    suite_results = {}  # Track results per suite\n    \n    print()\n    print(\"=\" * 60)\n    print(\"RUNNING EVALUATION\")\n    print(f\"Test Suite: {eval_test_suite}\")\n    print(\"=\" * 60)\n    \n    for prompt_file in eval_prompt_files:\n        suite_name = prompt_file.split(\"/\")[-1].replace(\".json\", \"\")\n        cases = load_prompt_cases(prompt_file)\n        print(f\"\\nRunning {len(cases)} tests from {suite_name}\")\n        \n        records = evaluate_cases(\n            cases=cases,\n            client=eval_client,\n            dry_run=False,\n        )\n        all_eval_records.extend(records)\n        \n        passed = sum(1 for r in records if r.passed)\n        suite_results[suite_name] = {\n            \"passed\": passed,\n            \"total\": len(records),\n            \"rate\": passed/len(records)*100\n        }\n        print(f\"   Results: {passed}/{len(records)} passed ({passed/len(records)*100:.1f}%)\")\n    \n    # Calculate overall results\n    eval_passed = sum(1 for r in all_eval_records if r.passed)\n    eval_total = len(all_eval_records)\n    EVAL_PASS_RATE = round(eval_passed / eval_total * 100, 1)\n    \n    print()\n    print(\"=\" * 60)\n    print(\"EVALUATION COMPLETE\")\n    print(\"=\" * 60)\n    print(f\"\\nResults by Suite:\")\n    for suite_name, results in suite_results.items():\n        print(f\"  {suite_name}: {results['passed']}/{results['total']} ({results['rate']:.1f}%)\")\n    print(f\"\\nOverall: {eval_passed}/{eval_total} passed ({EVAL_PASS_RATE}%)\")\n    \n    # Build evaluation lineage\n    eval_config = {\"temperature\": 0.2, \"max_tokens\": 1024, \"seed\": 42}\n    eval_hardware = {\n        \"gpu\": GPU_NAME,\n        \"gpu_memory_gb\": round(GPU_MEMORY_GB, 1),\n        \"platform\": \"Google Colab\",\n    }\n    \n    EVALUATION_LINEAGE = build_evaluation_lineage(\n        records=all_eval_records,\n        model_name=EVAL_MODEL,\n        test_suites=eval_prompt_files,\n        eval_config=eval_config,\n        hardware_info=eval_hardware,\n    )\n    \n    # Add suite-level breakdown to lineage\n    EVALUATION_LINEAGE[\"suite_results\"] = suite_results\n    \n    MODEL_CARD_EVAL_SECTION = generate_evaluation_model_card_section(EVALUATION_LINEAGE)\n    \n    # Save evaluation lineage\n    eval_lineage_path = f\"{output_dir}/evaluation_lineage.json\"\n    with open(eval_lineage_path, \"w\") as f:\n        json.dump(EVALUATION_LINEAGE, f, indent=2)\n    \n    print()\n    print(f\"Evaluation lineage saved: {eval_lineage_path}\")\n    print(f\"  Overall Pass Rate: {EVAL_PASS_RATE}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plzavcyr2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_evaluation:\n",
    "    import re\n",
    "    import tempfile\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    print(\"Uploading evaluation results to HuggingFace...\")\n",
    "    \n",
    "    # Upload to both LoRA and merged repos\n",
    "    repos_to_update = [\n",
    "        f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "        f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    ]\n",
    "    \n",
    "    for repo_id in repos_to_update:\n",
    "        try:\n",
    "            # Upload evaluation lineage JSON\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=eval_lineage_path,\n",
    "                path_in_repo=\"evaluation_lineage.json\",\n",
    "                repo_id=repo_id,\n",
    "                token=HF_TOKEN,\n",
    "            )\n",
    "            \n",
    "            # Download and update README with evaluation section\n",
    "            try:\n",
    "                readme_path = hf_hub_download(repo_id=repo_id, filename=\"README.md\", token=HF_TOKEN)\n",
    "                with open(readme_path, 'r') as f:\n",
    "                    existing_readme = f.read()\n",
    "                \n",
    "                if \"## Evaluation Results\" in existing_readme:\n",
    "                    pattern = r'## Evaluation Results.*?(?=\\n## |\\Z)'\n",
    "                    updated_readme = re.sub(pattern, MODEL_CARD_EVAL_SECTION, existing_readme, flags=re.DOTALL)\n",
    "                else:\n",
    "                    updated_readme = existing_readme.rstrip() + \"\\n\\n\" + MODEL_CARD_EVAL_SECTION\n",
    "                \n",
    "                with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:\n",
    "                    f.write(updated_readme)\n",
    "                    temp_readme = f.name\n",
    "                \n",
    "                api.upload_file(\n",
    "                    path_or_fileobj=temp_readme,\n",
    "                    path_in_repo=\"README.md\",\n",
    "                    repo_id=repo_id,\n",
    "                    token=HF_TOKEN,\n",
    "                )\n",
    "                print(f\"  ‚úì Updated: {repo_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Could not update README for {repo_id}: {e}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Failed to update {repo_id}: {e}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úì EVALUATION RESULTS UPLOADED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Pass Rate: {EVAL_PASS_RATE}%\")\n",
    "    print(f\"View model card: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Your model has been trained, evaluated, and uploaded to HuggingFace!\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| ‚úÖ Fine-tuned | Trained a language model to use the Claudesidian toolset |\n",
    "| ‚úÖ Lineage | Captured complete training metadata for reproducibility |\n",
    "| ‚úÖ Model Card | Auto-generated with all hyperparameters |\n",
    "| ‚úÖ Formats | Created LoRA, merged 16-bit, and GGUF versions |\n",
    "| ‚úÖ Evaluation | Tested tool-calling accuracy (if enabled) |\n",
    "| ‚úÖ Published | Model card includes training AND evaluation results |\n",
    "\n",
    "### Your Lineage Files\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `training_lineage.json` | HuggingFace + Google Drive | All training parameters |\n",
    "| `evaluation_lineage.json` | HuggingFace + Google Drive | Test results by category |\n",
    "| `README.md` | HuggingFace | Auto-generated model card |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Test locally with LM Studio:**\n",
    "1. Open LM Studio ‚Üí \"Discover\" tab\n",
    "2. Search for your model name\n",
    "3. Download the GGUF version\n",
    "4. Test with tool-calling prompts\n",
    "\n",
    "**Test with Ollama:**\n",
    "```bash\n",
    "ollama pull {your-username}/{model-name}\n",
    "ollama run {your-username}/{model-name}\n",
    "```\n",
    "\n",
    "**Refine with KTO:**\n",
    "After SFT, you can further improve your model with KTO (preference learning) to teach it to prefer better tool calls over worse ones.\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Check the [Evaluator README](https://github.com/ProfSynapse/Toolset-Training/blob/main/Evaluator/README.md) or open an issue on GitHub."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}