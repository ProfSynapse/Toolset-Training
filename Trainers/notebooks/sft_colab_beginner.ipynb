{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocGQlIoGRtIL"
   },
   "source": [
    "# Tool-Calling Fine-Tuning with SFT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_tool_calling.ipynb)\n",
    "\n",
    "## ğŸ“ What You'll Learn\n",
    "\n",
    "This notebook teaches you how to fine-tune a language model to use the **Claudesidian-MCP toolset** for Obsidian vault operations. By the end, you'll have:\n",
    "\n",
    "- **A custom AI model** that can call tools to manage Obsidian vaults\n",
    "- **Hands-on experience** with supervised fine-tuning (SFT)\n",
    "- **Understanding** of hyperparameters and how they affect training\n",
    "\n",
    "## ğŸ”¬ What is SFT?\n",
    "\n",
    "**SFT (Supervised Fine-Tuning)** is like teaching through examples:\n",
    "- You show the model examples of correct tool-calling behavior\n",
    "- The model learns to replicate those patterns\n",
    "- Use SFT when teaching a model a **new skill** (like using tools)\n",
    "\n",
    "**When to use SFT:**\n",
    "- âœ… Teaching tool-calling from scratch\n",
    "- âœ… Learning new task formats\n",
    "- âœ… Initial training with positive examples\n",
    "\n",
    "**Not for:**\n",
    "- âŒ Refining existing behavior (use KTO instead)\n",
    "- âŒ Teaching preferences between good/bad outputs (use preference learning)\n",
    "\n",
    "## ğŸ’» Hardware Requirements\n",
    "\n",
    "**Recommended GPU:**\n",
    "- 7B models: T4 (15GB VRAM) - âœ… **Free Colab tier works!**\n",
    "- 13B models: A100 (40GB VRAM) - Colab Pro\n",
    "- 70B models: A100 (80GB VRAM) - Colab Pro+\n",
    "\n",
    "**Training time:** ~45 minutes for a 7B model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG-NCBytRtIM"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-_FO9oORtIM"
   },
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0di49m8RtIM"
   },
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets==4.3.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZd9FBPeRtIM"
   },
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to Google Drive so they persist if runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMgubaVhRtIM"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30bXXvR_RtIM"
   },
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets:\n",
    "1. Click the ğŸ”‘ key icon in the left sidebar\n",
    "2. Add new secret: `HF_TOKEN`\n",
    "3. Get token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TtcHRkzRtIN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Get your HuggingFace username automatically\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"âœ“ HuggingFace token loaded\")\n",
    "print(f\"âœ“ Username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8Y2ETsnRtIN"
   },
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "**What this does:** Choose the base model you want to fine-tune and configure basic settings.\n",
    "\n",
    "Think of this like choosing which \"brain\" you want to teach tool-calling skills to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEhWlq5wRtIN"
   },
   "outputs": [],
   "source": "# @title âš™ï¸ Model & Dataset Configuration\n# @markdown Use the dropdowns to select your model and configure your dataset.\n\n# @markdown ### ğŸ§  Base Model Selection\n# @markdown Choose a model based on your VRAM availability. Models are ordered by size (1B - 24B).\n# @markdown * **1B-3B:** Fast, runs on any GPU\n# @markdown * **7B-9B:** Standard balance of speed/intelligence\n# @markdown * **12B-24B:** High intelligence, requires ~12GB-24GB VRAM (A100 recommended for 20B+)\nMODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # @param [\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\", \"unsloth/gemma-2-2b-it-bnb-4bit\", \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", \"unsloth/Phi-3.5-mini-instruct\", \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\", \"unsloth/mistral-7b-v0.3-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\", \"unsloth/gemma-2-9b-it-bnb-4bit\", \"unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\", \"unsloth/Mistral-Nemo-Instruct-v1-bnb-4bit\", \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\", \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\", \"unsloth/Phi-4\", \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", \"unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit\", \"unsloth/Magistral-Small-2509-unsloth-bnb-4bit\"]\n\n# @markdown ### ğŸ“ Max Output Length\n# @markdown  2048 is standard. Higher values require more VRAM.\nMAX_SEQ_LENGTH = 2048 # @param [1024, 2048, 4096, 8192] {type:\"raw\"}\n\n# @markdown ### ğŸ“š Dataset Configuration\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\" # @param {type:\"string\"}\nDATASET_FILE = \"syngen_tools_sft_11.23.25_toolcall.jsonl\" # @param {type:\"string\"}\n\n# @markdown ### ğŸ·ï¸ Output Model Name\n# @markdown Name your fine-tuned model (e.g., `my-tool-model-v1`).\nOUTPUT_MODEL_NAME = \"nexus-tools-sft-7b\" # @param {type:\"string\"}\n\nprint(f\"âœ“ Configuration set:\")\nprint(f\"  â€¢ Model: {MODEL_NAME}\")\nprint(f\"  â€¢ Context: {MAX_SEQ_LENGTH}\")\nprint(f\"  â€¢ Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"  â€¢ Output: {OUTPUT_MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoPq_st8RtIN"
   },
   "source": [
    "## 5. Load Model and Tokenizer\n",
    "\n",
    "**What this does:** Downloads the base model and prepares it for training.\n",
    "\n",
    "The model is the \"brain\" that will learn tool-calling. The tokenizer converts text into numbers the model can process. We use 4-bit quantization to fit large models into limited GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bbAGd7TRtIN"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQXkG71jRtIN"
   },
   "outputs": [],
   "source": [
    "# Load the base model and tokenizer from HuggingFace\n",
    "# This downloads the model weights (~7GB for 7B models)\n",
    "#\n",
    "# Parameters explained:\n",
    "#   model_name: Which model to download\n",
    "#   max_seq_length: Max tokens model can process at once\n",
    "#   dtype=None: Auto-detect best precision for your GPU\n",
    "#   load_in_4bit=True: Use 4-bit quantization to save memory\n",
    "#   token: Your HF token for accessing the model\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect (usually bfloat16 or float16)\n",
    "    load_in_4bit=True,  # Reduces memory usage by ~75%\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")\n",
    "print(f\"  Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLeRYX9ORtIN"
   },
   "source": [
    "## 6. Apply LoRA Adapters\n",
    "\n",
    "**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n",
    "\n",
    "Think of LoRA like teaching a new skill through muscle memory - we add small specialized layers that learn the new behavior, while keeping the main \"brain\" frozen. This is way faster and uses less memory than retraining everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-Y8IQtdRtIN"
   },
   "outputs": [],
   "source": [
    "# @title ğŸ”§ LoRA Adapter Configuration\n",
    "# @markdown Configure the size and strength of the fine-tuning adapters.\n",
    "\n",
    "# @markdown ### ğŸ›ï¸ LoRA Parameters\n",
    "# @markdown **Rank (r):** Higher = smarter but slower/more memory (Standard: 16-64).\n",
    "# @markdown Alpha will be automatically set to 2 * r.\n",
    "r = 32 # @param [8, 16, 32, 64, 128] {type:\"raw\"}\n",
    "\n",
    "lora_alpha = r * 2\n",
    "\n",
    "# @markdown **Dropout:** Helps prevent overfitting (Standard: 0.05).\n",
    "lora_dropout = 0.05 # @param {type:\"number\"}\n",
    "\n",
    "# @markdown **Random Seed:** Change this for different initialization.\n",
    "random_state = 3407 # @param {type:\"integer\"}\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters applied:\")\n",
    "print(f\"  â€¢ Rank: {r}\")\n",
    "print(f\"  â€¢ Alpha: {lora_alpha}\")\n",
    "print(f\"  â€¢ Dropout: {lora_dropout}\")\n",
    "print(f\"  â€¢ Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRo35-UkRtIN"
   },
   "source": [
    "## 7. Load and Prepare Dataset\n",
    "\n",
    "**What this does:** Downloads training examples and formats them for the model.\n",
    "\n",
    "The dataset contains ~5,500 examples of correct tool-calling behavior. Think of it like a textbook full of solved problems that the model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiAxKyv1RtIN"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "LOAD DATASET FROM HUGGINGFACE\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "This downloads pre-made training examples of correct tool-calling behavior.\n",
    "Each example shows: user request â†’ tool call â†’ result â†’ assistant response\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Loading dataset: {DATASET_NAME}\")\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,  # HuggingFace repository containing the dataset\n",
    "    data_files=DATASET_FILE,  # Specific JSONL file to use\n",
    "    split=\"train\"  # Use the training split\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {len(dataset)} examples\")\n",
    "print(f\"\\nSample example:\")\n",
    "print(dataset[0])\n",
    "\n",
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SET CHAT TEMPLATE (Model-Specific)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "The chat template defines how conversations are formatted for the model.\n",
    "Different models use different formats - we auto-detect and use the right one!\n",
    "\"\"\"\n",
    "\n",
    "# Mistral-specific template (uses [INST] format)\n",
    "MISTRAL_CHAT_TEMPLATE = \"\"\"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'system' %}{% if loop.index == 1 %}{{ message['content'] + ' ' }}{% endif %}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "# Generic fallback template for other models\n",
    "DEFAULT_CHAT_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|user|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ '<|system|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>\\\\n'  + message['content'] + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    # Detect model type and use appropriate template\n",
    "    is_mistral = 'mistral' in MODEL_NAME.lower()\n",
    "\n",
    "    if is_mistral:\n",
    "        print(\"\\nâœ“ Detected Mistral model - using [INST] format\")\n",
    "        tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE\n",
    "        print(\"   Format: <s>[INST] user [/INST] assistant</s>\")\n",
    "        print(\"   This is the official Mistral chat format!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No chat template found, using default format\")\n",
    "        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n",
    "        print(\"   Format: <|user|>\\\\ncontent</s>\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Tokenizer already has chat template\")\n",
    "    if 'mistral' in MODEL_NAME.lower():\n",
    "        print(\"   Using Mistral model - template should use [INST] format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9G_kCjSRtIN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "FORMAT DATASET FOR TRAINING\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "Convert the conversation format into the exact text format the model expects.\n",
    "This applies the chat template to each example.\n",
    "\"\"\"\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"\n",
    "    Convert conversations to tokenizer's chat template.\n",
    "\n",
    "    Input: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, ...]}\n",
    "    Output: {\"text\": \"<|im_start|>user\\n...<|im_end|>\\n...\"}\n",
    "    \"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example[\"conversations\"],  # List of message dicts\n",
    "        tokenize=False,  # Return string, not token IDs\n",
    "        add_generation_prompt=False  # Don't add prompt for model to continue\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting to entire dataset\n",
    "# This creates a new \"text\" field with formatted conversations\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,  # Function to apply\n",
    "    remove_columns=dataset.column_names,  # Remove original columns\n",
    "    desc=\"Formatting dataset\"  # Progress bar description\n",
    ")\n",
    "\n",
    "print(\"âœ“ Dataset formatted for training\")\n",
    "print(f\"\\nFormatted example (first 500 characters):\")\n",
    "print(dataset[0][\"text\"][:500])\n",
    "print(\"\\nğŸ’¡ The full formatted text will be used during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43Kqdd9wRtIO"
   },
   "source": [
    "## 8. Training Configuration\n",
    "\n",
    "**What this does:** Set the hyperparameters that control how the model learns.\n",
    "\n",
    "This is the **most important section** - these settings determine how fast the model learns, how much memory it uses, and how good the final result will be. Think of it like configuring a study plan: how many hours per day, how many review sessions, how to handle difficult material, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlOVjlIGRtIO"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped output directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{DRIVE_OUTPUT_DIR}/{timestamp}\"\n",
    "\n",
    "# @title ğŸƒ Training Hyperparameters\n",
    "# @markdown Control the speed and quality of training.\n",
    "\n",
    "# @markdown ### âš¡ Performance Settings\n",
    "# @markdown **Batch Size:** Examples per step. Lower if you run out of memory.\n",
    "per_device_train_batch_size = 2 # @param [1, 2, 4, 6, 8, 10, 12] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Gradient Accumulation:** Simulates larger batches.\n",
    "gradient_accumulation_steps = 4 # @param [1, 2, 4, 6, 8, 10, 12] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### ğŸ§  Learning Rate Configuration\n",
    "# @markdown **Step 1: Choose the Magnitude (Exponent)**\n",
    "# @markdown This is the most important setting. It determines the \"speed\" of learning.\n",
    "# @markdown * **4** = Standard (1e-4). Recommended for 7B models and SFT.\n",
    "# @markdown * **5** = Slow (1e-5). Use if training is unstable or for larger models.\n",
    "# @markdown * **6** = Very Slow (1e-6). Precise but takes much longer.\n",
    "learning_rate_exponent = 4 # @param [4, 5, 6, 7] {type:\"raw\"}\n",
    "\n",
    "# @markdown **Step 2: Choose the Multiplier**\n",
    "# @markdown Fine-tunes the rate within that magnitude (e.g., Multiplier 2 + Exponent 4 = 2e-4).\n",
    "learning_rate_multiplier = 2 # @param [1, 2, 3, 4, 5, 6, 7, 8, 9] {type:\"raw\"}\n",
    "\n",
    "learning_rate = learning_rate_multiplier * (10 ** -learning_rate_exponent)\n",
    "\n",
    "# @markdown ### ğŸ”„ Epochs\n",
    "# @markdown Number of passes through the dataset.\n",
    "num_train_epochs = 3 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### ğŸ’¾ Saving & Logging\n",
    "save_steps = 100 # @param {type:\"integer\"}\n",
    "logging_steps = 5 # @param {type:\"integer\"}\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=3,\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training configuration ready\")\n",
    "print(f\"  â€¢ Batch Size: {per_device_train_batch_size}\")\n",
    "print(f\"  â€¢ Gradient Accum.: {gradient_accumulation_steps}\")\n",
    "print(f\"  â€¢ Learning Rate: {learning_rate} ({learning_rate_multiplier}e-{learning_rate_exponent})\")\n",
    "print(f\"  â€¢ Epochs: {num_train_epochs}\")\n",
    "print(f\"  â€¢ Output Dir: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b6bueM4RtIO"
   },
   "source": [
    "## 9. Initialize Trainer\n",
    "\n",
    "**What this does:** Creates the training engine that coordinates everything.\n",
    "\n",
    "The SFTTrainer is the orchestrator - it takes the model, dataset, and configuration, then handles all the training mechanics (gradient updates, checkpointing, logging, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xRGa_dXRtIO"
   },
   "outputs": [],
   "source": [
    "# Create the SFTTrainer\n",
    "# This combines the model, dataset, and configuration into one training pipeline\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # The model with LoRA adapters\n",
    "    tokenizer=tokenizer,  # For converting text to tokens\n",
    "    train_dataset=dataset,  # Our formatted training examples\n",
    "    args=training_args,  # All the hyperparameters we configured\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized\")\n",
    "print(\"  Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5yi6rJ1RtIO"
   },
   "source": [
    "## 10. Train!\n",
    "\n",
    "**What this does:** The actual learning happens here!\n",
    "\n",
    "The model will:\n",
    "1. **Read examples** from the dataset\n",
    "2. **Predict** what the response should be\n",
    "3. **Compare** its prediction to the correct answer\n",
    "4. **Update weights** to get closer to the correct answer\n",
    "5. **Repeat** this process for 3 epochs (3 full passes through the data)\n",
    "\n",
    "**What to expect:**\n",
    "- Training takes ~45 minutes for 7B models on T4 GPU\n",
    "- You'll see progress updates every 5 steps\n",
    "- Loss should generally decrease over time (learning is working!)\n",
    "- Checkpoints are saved every 100 steps to Google Drive\n",
    "\n",
    "**What the metrics mean:**\n",
    "- **Loss:** How \"wrong\" the model is (lower = better, aim for <1.0)\n",
    "- **Learning Rate:** Gradually decreases as training progresses\n",
    "- **Samples/sec:** Training speed (depends on GPU)\n",
    "\n",
    "**ğŸ’¾ Checkpoint Resumption:**\n",
    "If your Colab session disconnects, don't worry! Your checkpoints are saved to Google Drive. You can resume training by:\n",
    "1. Re-running cells 1-9 (setup, model loading, dataset prep, config)\n",
    "2. In the training cell below, the code will automatically detect the latest checkpoint and resume from there\n",
    "3. Your progress is preserved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6490TDHRtIO"
   },
   "outputs": [],
   "source": [
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” Check for existing checkpoints (automatic resumption)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import glob\n",
    "import os\n",
    "\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Found checkpoints - get the latest one\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = latest_checkpoint\n",
    "    print(f\"âœ“ Found existing checkpoint: {os.path.basename(latest_checkpoint)}\")\n",
    "    print(f\"  Resuming training from this checkpoint\")\n",
    "    print(f\"  Total checkpoints found: {len(checkpoint_dirs)}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No existing checkpoints found - starting fresh training\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Start training (or resume)\n",
    "print(\"=\" * 60)\n",
    "if resume_from_checkpoint:\n",
    "    print(\"RESUMING TRAINING FROM CHECKPOINT\")\n",
    "else:\n",
    "    print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZwBSPBvRtIO"
   },
   "source": [
    "## 11. Upload to HuggingFace\n",
    "\n",
    "**What this does:** Share your trained model with the world!\n",
    "\n",
    "We'll create **three versions** of your model:\n",
    "\n",
    "1. **LoRA adapters** - Small files that contain just the changes\n",
    "   - Fast to download\n",
    "   - Need to be combined with base model to use\n",
    "   \n",
    "2. **Merged 16-bit model** - Full model with adapters merged in\n",
    "   - High quality, no precision loss\n",
    "   - Large file size\n",
    "   - Best for local deployment (Ollama, LM Studio)\n",
    "   \n",
    "3. **GGUF quantizations** - Optimized versions for CPU/GPU inference\n",
    "   - Q4_K_M - Recommended for most users\n",
    "   - Q5_K_M - Better quality, larger size\n",
    "   - Q8_0 - Nearly full quality\n",
    "   - These work directly with Ollama and LM Studio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yU-kZcxaRtIO"
   },
   "outputs": [],
   "source": [
    "# Upload LoRA adapters\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN89CamTRtIO"
   },
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Merged model uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K90WE6frRtIO"
   },
   "outputs": [],
   "source": [
    "# Create GGUF quantizations\n",
    "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    "\n",
    "print(\"Creating GGUF quantizations...\")\n",
    "print(f\"This will create {len(quantization_methods)} versions\")\n",
    "print()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_methods,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"âœ“ GGUF quantizations created and uploaded!\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7fuKcZuRtIO"
   },
   "source": [
    "## ğŸ‰ Done!\n",
    "\n",
    "Your model has been trained and uploaded to HuggingFace!\n",
    "\n",
    "## ğŸ“Š What You Accomplished\n",
    "\n",
    "âœ… **Fine-tuned a language model** to use the Claudesidian toolset  \n",
    "âœ… **Learned about SFT hyperparameters** and how they affect training  \n",
    "âœ… **Created multiple model formats** (LoRA, merged, GGUF) for different use cases  \n",
    "âœ… **Published your model** to HuggingFace for others to use  \n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "### 1. Test Your Model\n",
    "\n",
    "**Using LM Studio:**\n",
    "1. Open LM Studio â†’ \"Discover\" tab\n",
    "2. Search for your username or. model name\n",
    "3. Download the GGUF version of your choice\n",
    "4. Load and test with tool-calling prompts!\n",
    "\n",
    "**Using Ollama:**\n",
    "```bash\n",
    "# Download your model\n",
    "ollama pull {hf_user}/{OUTPUT_MODEL_NAME}\n",
    "\n",
    "# Test it\n",
    "ollama run {hf_user}/{OUTPUT_MODEL_NAME}\n",
    "```\n",
    "\n",
    "### 2. Evaluate Quality\n",
    "\n",
    "Run the Evaluator to test tool-calling accuracy:\n",
    "```bash\n",
    "python Evaluator\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the documentation or open an issue on GitHub!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}