{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Inference & Evaluation for Tool-Calling Models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/vllm_inference_evaluation.ipynb)\n",
    "\n",
    "## üéØ Purpose\n",
    "\n",
    "This notebook evaluates fine-tuned models for **Claudesidian-MCP tool-calling accuracy**. Use it to:\n",
    "\n",
    "- **Load any model** (HuggingFace, local path, LoRA adapters)\n",
    "- **Run comprehensive evaluations** (47 tools + behavioral tests)\n",
    "- **Compare model performance** across different test suites\n",
    "- **Generate detailed reports** with pass rates and failure analysis\n",
    "\n",
    "## üìä Test Suites Available\n",
    "\n",
    "1. **Full Coverage** (47 tests) - One test per tool\n",
    "2. **Behavioral Patterns** (21 tests) - Context efficiency, executePrompt usage\n",
    "3. **Baseline** (6 tests) - General workflows and clarification handling\n",
    "4. **Tool Combos** - Multi-step tool sequences\n",
    "\n",
    "## üíª Hardware Requirements\n",
    "\n",
    "- **7B models:** T4 GPU (15GB VRAM) - ‚úÖ Free Colab works\n",
    "- **13B models:** A100 (40GB VRAM) - Colab Pro\n",
    "- **Inference time:** ~1-2 minutes for full coverage (47 tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install vLLM for fast inference and evaluation dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM and dependencies\n",
    "%%capture\n",
    "!pip install vllm>=0.6.0\n",
    "!pip install requests pandas\n",
    "!pip install huggingface_hub\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Evaluation Framework\n",
    "\n",
    "Download the Evaluator code and all test suites from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport requests\nfrom pathlib import Path\n\n# Create directory structure\nos.makedirs(\"Evaluator/prompts\", exist_ok=True)\nos.makedirs(\"Evaluator/results\", exist_ok=True)\nos.makedirs(\"tools\", exist_ok=True)\n\n# Base URL for raw files from GitHub\nREPO_BASE = \"https://raw.githubusercontent.com/ProfSynapse/Toolset-Training/main\"\n\n# Files to download\nfiles_to_download = {\n    # Core evaluator modules\n    \"Evaluator/__init__.py\": \"Evaluator/__init__.py\",\n    \"Evaluator/runner.py\": \"Evaluator/runner.py\",\n    \"Evaluator/schema_validator.py\": \"Evaluator/schema_validator.py\",\n    \"Evaluator/prompt_sets.py\": \"Evaluator/prompt_sets.py\",\n    \"Evaluator/reporting.py\": \"Evaluator/reporting.py\",\n    \"Evaluator/config.py\": \"Evaluator/config.py\",\n    \n    # Prompt sets\n    \"Evaluator/prompts/tool_prompts.json\": \"Evaluator/prompts/tool_prompts.json\",\n    \"Evaluator/prompts/behavioral_patterns.json\": \"Evaluator/prompts/behavioral_patterns.json\",\n    \"Evaluator/prompts/baseline.json\": \"Evaluator/prompts/baseline.json\",\n    \"Evaluator/prompts/tool_combos.json\": \"Evaluator/prompts/tool_combos.json\",\n    \n    # Tool schemas (needed for validation)\n    \"tools/tool_schemas.json\": \"tools/tool_schemas.json\",\n}\n\ndef download_file(url, dest):\n    \"\"\"Download a file from URL to destination.\"\"\"\n    response = requests.get(url, timeout=30)\n    response.raise_for_status()\n    Path(dest).parent.mkdir(parents=True, exist_ok=True)\n    with open(dest, 'w', encoding='utf-8') as f:\n        f.write(response.text)\n\nprint(\"Downloading evaluation framework...\")\nfailed_downloads = []\n\nfor remote_path, local_path in files_to_download.items():\n    url = f\"{REPO_BASE}/{remote_path}\"\n    try:\n        download_file(url, local_path)\n        print(f\"  ‚úì {remote_path}\")\n    except Exception as e:\n        print(f\"  ‚úó Failed: {remote_path} - {e}\")\n        failed_downloads.append(remote_path)\n\nif failed_downloads:\n    print(f\"\\n‚ö†Ô∏è  Failed to download {len(failed_downloads)} files:\")\n    for path in failed_downloads:\n        print(f\"    - {path}\")\nelse:\n    print(\"\\n‚úì Evaluation framework ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Model to Load\n",
    "\n",
    "Choose which model you want to evaluate. You can load from:\n",
    "- **HuggingFace Hub** - Any public or private model\n",
    "- **Local path** - Model saved in this session\n",
    "- **LoRA adapters** - Base model + your adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è Model Configuration\n",
    "# @markdown Select how you want to load the model.\n",
    "\n",
    "# @markdown ### üìç Model Source\n",
    "model_source = \"HuggingFace\" # @param [\"HuggingFace\", \"Local Path\", \"LoRA Adapters\"]\n",
    "\n",
    "# @markdown ### ü§ó HuggingFace Configuration\n",
    "# @markdown If using HuggingFace, enter the model name (e.g., `username/model-name`).\n",
    "hf_model_name = \"professorsynapse/nexus-tools-sft-7b-merged\" # @param {type:\"string\"}\n",
    "hf_token_required = False # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown ### üìÅ Local Path Configuration\n",
    "# @markdown If using a local path, enter the full path to the model directory.\n",
    "local_model_path = \"/content/drive/MyDrive/model\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### üîß LoRA Configuration\n",
    "# @markdown If using LoRA adapters, specify base model and adapter path.\n",
    "base_model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # @param {type:\"string\"}\n",
    "lora_adapter_path = \"/content/drive/MyDrive/lora_adapters\" # @param {type:\"string\"}\n",
    "\n",
    "# Determine final model configuration\n",
    "if model_source == \"HuggingFace\":\n",
    "    MODEL_NAME = hf_model_name\n",
    "    USE_LORA = False\n",
    "    print(f\"‚úì Configuration set: HuggingFace model\")\n",
    "    print(f\"  Model: {MODEL_NAME}\")\n",
    "elif model_source == \"Local Path\":\n",
    "    MODEL_NAME = local_model_path\n",
    "    USE_LORA = False\n",
    "    print(f\"‚úì Configuration set: Local model\")\n",
    "    print(f\"  Path: {MODEL_NAME}\")\n",
    "else:  # LoRA Adapters\n",
    "    MODEL_NAME = base_model_name\n",
    "    USE_LORA = True\n",
    "    LORA_PATH = lora_adapter_path\n",
    "    print(f\"‚úì Configuration set: LoRA adapters\")\n",
    "    print(f\"  Base model: {MODEL_NAME}\")\n",
    "    print(f\"  Adapters: {LORA_PATH}\")\n",
    "\n",
    "# Handle HF token if needed\n",
    "HF_TOKEN = None\n",
    "if hf_token_required:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "        print(\"  ‚úì HuggingFace token loaded from secrets\")\n",
    "    except:\n",
    "        print(\"  ‚ö†Ô∏è  Could not load HF_TOKEN from secrets. Add it in the üîë Secrets panel if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with vLLM\n",
    "\n",
    "Initialize the vLLM engine for fast inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from vllm import LLM, SamplingParams\nimport torch\n\n# @title üöÄ vLLM Configuration\n# @markdown Configure vLLM inference settings.\n\n# @markdown ### üîß Performance Settings\ntensor_parallel_size = 1 # @param {type:\"integer\"}\ngpu_memory_utilization = 0.85 # @param {type:\"slider\", min:0.5, max:0.95, step:0.05}\nmax_model_len = 2048 # @param [1024, 2048, 4096, 8192] {type:\"raw\"}\n\n# @markdown ### üêõ Troubleshooting Options\n# @markdown Enable if you're having issues loading the model.\nenforce_eager = False # @param {type:\"boolean\"}\ndisable_custom_all_reduce = False # @param {type:\"boolean\"}\n\n# Check GPU\nif torch.cuda.is_available():\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"Available VRAM: {total_vram:.1f} GB\")\n    \n    # Memory check\n    estimated_usage = total_vram * gpu_memory_utilization\n    print(f\"Target VRAM usage: {estimated_usage:.1f} GB\")\n    \n    if total_vram < 15:\n        print(f\"\\n‚ö†Ô∏è  Warning: Limited VRAM detected ({total_vram:.1f} GB)\")\n        print(f\"   Consider using smaller models or reducing max_model_len\")\nelse:\n    print(\"‚ö†Ô∏è  No GPU detected. vLLM requires a GPU.\")\n    raise RuntimeError(\"GPU required for vLLM\")\n\nprint(f\"\\nInitializing vLLM engine...\")\nprint(f\"  ‚Ä¢ Model: {MODEL_NAME}\")\nprint(f\"  ‚Ä¢ Tensor Parallel: {tensor_parallel_size}\")\nprint(f\"  ‚Ä¢ GPU Memory: {gpu_memory_utilization:.0%}\")\nprint(f\"  ‚Ä¢ Max Length: {max_model_len}\")\n\n# Build vLLM kwargs\nvllm_kwargs = {\n    \"model\": MODEL_NAME,\n    \"tensor_parallel_size\": tensor_parallel_size,\n    \"gpu_memory_utilization\": gpu_memory_utilization,\n    \"max_model_len\": max_model_len,\n    \"trust_remote_code\": True,\n    \"dtype\": \"auto\",\n}\n\n# Add LoRA if needed\nif USE_LORA:\n    vllm_kwargs[\"enable_lora\"] = True\n    print(f\"  ‚Ä¢ LoRA enabled: {LORA_PATH}\")\n\n# Add HF token if needed\nif HF_TOKEN:\n    os.environ['HF_TOKEN'] = HF_TOKEN\n\n# Add troubleshooting options if enabled\nif enforce_eager:\n    vllm_kwargs[\"enforce_eager\"] = True\n    print(f\"  ‚Ä¢ Enforce eager mode: True (slower but more compatible)\")\n\nif disable_custom_all_reduce:\n    vllm_kwargs[\"disable_custom_all_reduce\"] = True\n    print(f\"  ‚Ä¢ Custom all-reduce disabled: True\")\n\n# Initialize vLLM with better error handling\nprint(\"\\n‚è≥ Loading model... (this may take 1-2 minutes)\")\ntry:\n    llm = LLM(**vllm_kwargs)\n    print(\"\\n‚úì vLLM engine ready!\")\n    \n    # Check actual memory usage\n    torch.cuda.synchronize()\n    current_vram = torch.cuda.memory_allocated() / 1024**3\n    max_vram = torch.cuda.max_memory_allocated() / 1024**3\n    print(f\"  VRAM allocated: {current_vram:.1f} GB\")\n    print(f\"  Peak VRAM: {max_vram:.1f} GB\")\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå Failed to initialize vLLM\")\n    print(f\"\\nError: {str(e)}\")\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TROUBLESHOOTING STEPS\")\n    print(\"=\" * 60)\n    print(\"\\n1. **Model Not Found**\")\n    print(\"   ‚Ä¢ Verify model name is correct\")\n    print(\"   ‚Ä¢ If private model, enable 'hf_token_required' above\")\n    print(\"   ‚Ä¢ Try downloading manually first:\")\n    print(f\"     !huggingface-cli download {MODEL_NAME}\")\n    print(\"\\n2. **Out of Memory (OOM)**\")\n    print(\"   ‚Ä¢ Reduce gpu_memory_utilization to 0.7 or lower\")\n    print(\"   ‚Ä¢ Reduce max_model_len to 1024\")\n    print(\"   ‚Ä¢ Use smaller model (3B instead of 7B)\")\n    print(f\"   ‚Ä¢ Your GPU has {total_vram:.1f} GB VRAM\")\n    print(\"\\n3. **Compatibility Issues**\")\n    print(\"   ‚Ä¢ Enable 'enforce_eager' option above\")\n    print(\"   ‚Ä¢ Enable 'disable_custom_all_reduce' option above\")\n    print(\"   ‚Ä¢ Try upgrading vLLM:\")\n    print(\"     !pip install --upgrade vllm\")\n    print(\"\\n4. **Model Format Issues**\")\n    print(\"   ‚Ä¢ Some models need specific vLLM versions\")\n    print(\"   ‚Ä¢ Try a different model format (merged vs GGUF)\")\n    print(\"   ‚Ä¢ Check if model is compatible with vLLM\")\n    print(\"\\n5. **Colab-Specific Issues**\")\n    print(\"   ‚Ä¢ Free tier T4 may be overloaded - try later\")\n    print(\"   ‚Ä¢ Restart runtime and try again\")\n    print(\"   ‚Ä¢ Consider Colab Pro for A100 GPUs\")\n    print(\"\\n\" + \"=\" * 60)\n    \n    # Re-raise to stop execution\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create vLLM Client for Evaluator\n",
    "\n",
    "Wrap vLLM in a client that works with the Evaluator framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Mapping, Sequence\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class VLLMResponse:\n",
    "    \"\"\"Response from vLLM inference.\"\"\"\n",
    "    message: str\n",
    "    raw: Dict[str, Any]\n",
    "    latency_s: float\n",
    "\n",
    "class VLLMClient:\n",
    "    \"\"\"\n",
    "    vLLM client that implements the same interface as OllamaClient/LMStudioClient.\n",
    "    This allows it to work seamlessly with the Evaluator framework.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLM,\n",
    "        temperature: float = 0.2,\n",
    "        top_p: float = 0.9,\n",
    "        max_tokens: int = 1024,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.max_tokens = max_tokens\n",
    "        self.seed = seed\n",
    "\n",
    "    def chat(self, messages: Sequence[Mapping[str, str]]) -> VLLMResponse:\n",
    "        \"\"\"\n",
    "        Send a chat conversation to vLLM and return the response.\n",
    "\n",
    "        Args:\n",
    "            messages: List of message dicts with 'role' and 'content' keys\n",
    "\n",
    "        Returns:\n",
    "            VLLMResponse with the assistant's message, raw output, and latency\n",
    "        \"\"\"\n",
    "        # Format messages into a prompt\n",
    "        # Detect model type and use appropriate format\n",
    "        model_name_lower = MODEL_NAME.lower()\n",
    "        \n",
    "        if 'mistral' in model_name_lower:\n",
    "            # Mistral format: <s>[INST] user [/INST] assistant</s>\n",
    "            prompt = self._format_mistral(messages)\n",
    "        elif 'llama-3' in model_name_lower or 'llama3' in model_name_lower:\n",
    "            # Llama 3 format\n",
    "            prompt = self._format_llama3(messages)\n",
    "        elif 'qwen' in model_name_lower:\n",
    "            # Qwen format\n",
    "            prompt = self._format_qwen(messages)\n",
    "        else:\n",
    "            # Generic ChatML format\n",
    "            prompt = self._format_chatml(messages)\n",
    "\n",
    "        # Create sampling params\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            max_tokens=self.max_tokens,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "\n",
    "        # Generate\n",
    "        start = time.perf_counter()\n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        latency_s = time.perf_counter() - start\n",
    "\n",
    "        # Extract response\n",
    "        output = outputs[0]\n",
    "        message = output.outputs[0].text.strip()\n",
    "\n",
    "        # Build raw response dict\n",
    "        raw = {\n",
    "            \"prompt\": prompt,\n",
    "            \"output\": message,\n",
    "            \"finish_reason\": output.outputs[0].finish_reason,\n",
    "            \"prompt_tokens\": len(output.prompt_token_ids),\n",
    "            \"completion_tokens\": len(output.outputs[0].token_ids),\n",
    "        }\n",
    "\n",
    "        return VLLMResponse(\n",
    "            message=message,\n",
    "            raw=raw,\n",
    "            latency_s=latency_s\n",
    "        )\n",
    "\n",
    "    def _format_mistral(self, messages: Sequence[Mapping[str, str]]) -> str:\n",
    "        \"\"\"Format for Mistral models.\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            if role == \"user\":\n",
    "                prompt_parts.append(f\"[INST] {content} [/INST]\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\" {content}</s>\")\n",
    "            elif role == \"system\":\n",
    "                prompt_parts.append(f\"{content} \")\n",
    "        return \"<s>\" + \"\".join(prompt_parts)\n",
    "\n",
    "    def _format_llama3(self, messages: Sequence[Mapping[str, str]]) -> str:\n",
    "        \"\"\"Format for Llama 3 models.\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            prompt_parts.append(f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\")\n",
    "        prompt_parts.append(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "        return \"<|begin_of_text|>\" + \"\".join(prompt_parts)\n",
    "\n",
    "    def _format_qwen(self, messages: Sequence[Mapping[str, str]]) -> str:\n",
    "        \"\"\"Format for Qwen models.\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            prompt_parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\")\n",
    "        prompt_parts.append(\"<|im_start|>assistant\\n\")\n",
    "        return \"\".join(prompt_parts)\n",
    "\n",
    "    def _format_chatml(self, messages: Sequence[Mapping[str, str]]) -> str:\n",
    "        \"\"\"Generic ChatML format.\"\"\"\n",
    "        prompt_parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            prompt_parts.append(f\"<|{role}|>\\n{content}<|end|>\\n\")\n",
    "        prompt_parts.append(\"<|assistant|>\\n\")\n",
    "        return \"\".join(prompt_parts)\n",
    "\n",
    "# Create client with default settings\n",
    "vllm_client = VLLMClient(\n",
    "    llm=llm,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1024,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"‚úì vLLM client created and ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Evaluation\n",
    "\n",
    "Choose which test suites to run and configure generation settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üéØ Evaluation Configuration\n",
    "# @markdown Select test suites and configure generation parameters.\n",
    "\n",
    "# @markdown ### üìã Test Suite Selection\n",
    "test_suite = \"Full Coverage (47 tools)\" # @param [\"Full Coverage (47 tools)\", \"Behavioral Patterns (21 tests)\", \"Baseline (6 tests)\", \"Tool Combos (Multi-step)\", \"All Suites\"]\n",
    "\n",
    "# @markdown ### üî¢ Limits\n",
    "# @markdown Limit prompts for quick testing (0 = no limit).\n",
    "max_prompts = 0 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### üé≤ Generation Settings\n",
    "eval_temperature = 0.2 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "eval_top_p = 0.9 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "eval_max_tokens = 1024 # @param {type:\"integer\"}\n",
    "eval_seed = 42 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### üíæ Output Settings\n",
    "save_to_drive = True # @param {type:\"boolean\"}\n",
    "drive_output_dir = \"/content/drive/MyDrive/Evaluation_Results\" # @param {type:\"string\"}\n",
    "\n",
    "# Map test suite to prompt files\n",
    "suite_map = {\n",
    "    \"Full Coverage (47 tools)\": [\"Evaluator/prompts/tool_prompts.json\"],\n",
    "    \"Behavioral Patterns (21 tests)\": [\"Evaluator/prompts/behavioral_patterns.json\"],\n",
    "    \"Baseline (6 tests)\": [\"Evaluator/prompts/baseline.json\"],\n",
    "    \"Tool Combos (Multi-step)\": [\"Evaluator/prompts/tool_combos.json\"],\n",
    "    \"All Suites\": [\n",
    "        \"Evaluator/prompts/tool_prompts.json\",\n",
    "        \"Evaluator/prompts/behavioral_patterns.json\",\n",
    "        \"Evaluator/prompts/baseline.json\",\n",
    "        \"Evaluator/prompts/tool_combos.json\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "prompt_files = suite_map[test_suite]\n",
    "\n",
    "# Update client settings\n",
    "vllm_client.temperature = eval_temperature\n",
    "vllm_client.top_p = eval_top_p\n",
    "vllm_client.max_tokens = eval_max_tokens\n",
    "vllm_client.seed = eval_seed\n",
    "\n",
    "# Setup output directory\n",
    "if save_to_drive:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        os.makedirs(drive_output_dir, exist_ok=True)\n",
    "        print(f\"‚úì Google Drive mounted: {drive_output_dir}\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  Could not mount Google Drive. Results will only be saved locally.\")\n",
    "        save_to_drive = False\n",
    "\n",
    "print(f\"\\n‚úì Evaluation configured:\")\n",
    "print(f\"  ‚Ä¢ Test Suite: {test_suite}\")\n",
    "print(f\"  ‚Ä¢ Prompt Files: {len(prompt_files)}\")\n",
    "if max_prompts > 0:\n",
    "    print(f\"  ‚Ä¢ Max Prompts: {max_prompts}\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Max Prompts: No limit (all prompts)\")\n",
    "print(f\"  ‚Ä¢ Temperature: {eval_temperature}\")\n",
    "print(f\"  ‚Ä¢ Top-p: {eval_top_p}\")\n",
    "print(f\"  ‚Ä¢ Max Tokens: {eval_max_tokens}\")\n",
    "print(f\"  ‚Ä¢ Seed: {eval_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Evaluation\n",
    "\n",
    "Execute the test suite and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '/content')  # Add current dir to path\n\nfrom Evaluator.prompt_sets import load_prompt_cases, filter_prompts\nfrom Evaluator.runner import evaluate_cases\nfrom Evaluator.reporting import build_run_payload, build_evaluation_lineage\nfrom Evaluator.config import PromptFilter\nfrom datetime import datetime\nimport json\n\n# Results storage\nall_records = []\n\nprint(\"=\" * 60)\nprint(\"STARTING EVALUATION\")\nprint(\"=\" * 60)\nprint()\n\nfor prompt_file in prompt_files:\n    print(f\"\\nüìù Loading prompts from: {prompt_file}\")\n\n    # Load and filter prompts\n    cases = load_prompt_cases(prompt_file)\n    prompt_filter = PromptFilter(tags=None, limit=max_prompts if max_prompts > 0 else None)\n    selected_cases = filter_prompts(cases, prompt_filter)\n\n    print(f\"   ‚Ä¢ Loaded {len(cases)} prompts\")\n    print(f\"   ‚Ä¢ Selected {len(selected_cases)} prompts for evaluation\")\n\n    if not selected_cases:\n        print(\"   ‚ö†Ô∏è  No prompts matched filters, skipping...\")\n        continue\n\n    # Progress callback\n    completed = 0\n    def on_record(record):\n        nonlocal completed\n        completed += 1\n        status = \"‚úì\" if record.passed else \"‚úó\"\n        time_str = f\"{record.latency_s:.2f}s\" if record.latency_s else \"N/A\"\n        print(f\"   [{completed}/{len(selected_cases)}] {status} {record.case.id} ({time_str})\")\n\n    # Run evaluation\n    print(f\"\\nüîÑ Running evaluation...\")\n    records = evaluate_cases(\n        cases=selected_cases,\n        client=vllm_client,\n        dry_run=False,\n        on_record=on_record,\n    )\n\n    all_records.extend(records)\n\n    # Calculate stats for this file\n    passed = sum(1 for r in records if r.passed)\n    failed = sum(1 for r in records if not r.passed)\n    avg_latency = sum(r.latency_s for r in records if r.latency_s) / len(records) if records else 0\n\n    print(f\"\\n   Results: {passed}/{len(records)} passed ({passed/len(records)*100:.1f}%)\")\n    print(f\"   Average latency: {avg_latency:.2f}s\")\n\n# Overall summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"EVALUATION COMPLETE\")\nprint(\"=\" * 60)\n\ntotal_passed = sum(1 for r in all_records if r.passed)\ntotal_failed = sum(1 for r in all_records if not r.passed)\ntotal_tests = len(all_records)\noverall_avg_latency = sum(r.latency_s for r in all_records if r.latency_s) / total_tests if total_tests else 0\n\nprint(f\"\\nüìä Overall Results:\")\nprint(f\"   ‚Ä¢ Total Tests: {total_tests}\")\nprint(f\"   ‚Ä¢ Passed: {total_passed} ({total_passed/total_tests*100:.1f}%)\")\nprint(f\"   ‚Ä¢ Failed: {total_failed} ({total_failed/total_tests*100:.1f}%)\")\nprint(f\"   ‚Ä¢ Average Latency: {overall_avg_latency:.2f}s\")\nprint(f\"   ‚Ä¢ Total Time: {sum(r.latency_s for r in all_records if r.latency_s):.2f}s\")\n\n# Save results\nEVAL_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nmodel_name_safe = MODEL_NAME.replace(\"/\", \"_\").replace(\":\", \"_\")\nresults_file = f\"Evaluator/results/eval_{model_name_safe}_{EVAL_TIMESTAMP}.json\"\n\n# Build metadata for payload\neval_metadata = {\n    \"model\": MODEL_NAME,\n    \"prompts_path\": \", \".join(prompt_files),\n    \"test_suite\": test_suite,\n    \"temperature\": eval_temperature,\n    \"top_p\": eval_top_p,\n    \"max_tokens\": eval_max_tokens,\n    \"seed\": eval_seed,\n    \"max_prompts\": max_prompts if max_prompts > 0 else \"all\",\n}\n\n# Build payload\npayload = build_run_payload(\n    records=all_records,\n    metadata=eval_metadata,\n)\n\n# Save locally\nwith open(results_file, 'w') as f:\n    json.dump(payload, f, indent=2)\nprint(f\"\\nüíæ Results saved locally: {results_file}\")\n\n# Save to Google Drive if enabled\nif save_to_drive and os.path.exists(drive_output_dir):\n    drive_results_file = f\"{drive_output_dir}/eval_{model_name_safe}_{EVAL_TIMESTAMP}.json\"\n    with open(drive_results_file, 'w') as f:\n        json.dump(payload, f, indent=2)\n    print(f\"üíæ Results saved to Drive: {drive_results_file}\")\n\n# Store for analysis\neval_results = {\n    \"records\": all_records,\n    \"payload\": payload,\n    \"timestamp\": EVAL_TIMESTAMP,\n    \"results_file\": results_file,\n}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results by Category\n",
    "\n",
    "Break down pass rates by tool category and show detailed failure information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Group by tags\n",
    "results_by_tag = defaultdict(lambda: {\"passed\": 0, \"failed\": 0, \"total\": 0})\n",
    "\n",
    "for record in all_records:\n",
    "    tags = record.case.tags if hasattr(record.case, 'tags') and record.case.tags else [\"untagged\"]\n",
    "\n",
    "    for tag in tags:\n",
    "        results_by_tag[tag][\"total\"] += 1\n",
    "        if record.passed:\n",
    "            results_by_tag[tag][\"passed\"] += 1\n",
    "        else:\n",
    "            results_by_tag[tag][\"failed\"] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_data = []\n",
    "for tag, stats in sorted(results_by_tag.items()):\n",
    "    pass_rate = (stats[\"passed\"] / stats[\"total\"] * 100) if stats[\"total\"] > 0 else 0\n",
    "    df_data.append({\n",
    "        \"Category\": tag,\n",
    "        \"Passed\": stats[\"passed\"],\n",
    "        \"Failed\": stats[\"failed\"],\n",
    "        \"Total\": stats[\"total\"],\n",
    "        \"Pass Rate\": f\"{pass_rate:.1f}%\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(df_data)\n",
    "print(\"\\nüìä Results by Category:\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Show failures if any\n",
    "failures = [r for r in all_records if not r.passed]\n",
    "if failures:\n",
    "    print(f\"\\n\\n‚ùå Failed Tests ({len(failures)}):\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, record in enumerate(failures[:15], 1):  # Show first 15 failures\n",
    "        print(f\"\\n{i}. {record.case.id}\")\n",
    "        print(f\"   Question: {record.case.question[:100]}...\" if len(record.case.question) > 100 else f\"   Question: {record.case.question}\")\n",
    "        \n",
    "        if record.error:\n",
    "            print(f\"   Error: {record.error}\")\n",
    "        elif record.validator and record.validator.issues:\n",
    "            print(f\"   Issues:\")\n",
    "            for issue in record.validator.issues[:3]:  # Show first 3 issues\n",
    "                print(f\"      ‚Ä¢ [{issue.level}] {issue.message}\")\n",
    "\n",
    "    if len(failures) > 15:\n",
    "        print(f\"\\n   ... and {len(failures) - 15} more failures\")\n",
    "else:\n",
    "    print(\"\\n\\n‚úÖ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Markdown Report\n",
    "\n",
    "Create a human-readable markdown report of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from Evaluator.reporting import render_markdown\n\n# Generate markdown\nmarkdown_file = f\"Evaluator/results/eval_{model_name_safe}_{EVAL_TIMESTAMP}.md\"\nmarkdown_content = render_markdown(all_records, MODEL_NAME, test_suite)\n\nwith open(markdown_file, 'w') as f:\n    f.write(markdown_content)\n\nprint(f\"‚úì Markdown report saved locally: {markdown_file}\")\n\n# Save to Google Drive if enabled\nif save_to_drive and os.path.exists(drive_output_dir):\n    drive_markdown_file = f\"{drive_output_dir}/eval_{model_name_safe}_{EVAL_TIMESTAMP}.md\"\n    with open(drive_markdown_file, 'w') as f:\n        f.write(markdown_content)\n    print(f\"‚úì Markdown report saved to Drive: {drive_markdown_file}\")\n\n# Display preview\nprint(\"\\n\" + \"=\" * 60)\nprint(\"REPORT PREVIEW\")\nprint(\"=\" * 60)\nprint(markdown_content[:2000])  # Show first 2000 chars\nif len(markdown_content) > 2000:\n    print(\"\\n... (see full report in markdown file)\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Build Evaluation Lineage\n\n**What this does:** Creates a comprehensive record of the evaluation for model cards and tracking.\n\nThis captures:\n- All test configurations and settings\n- Pass rates by category\n- Failure analysis with specific issues\n- Performance metrics (latency, throughput)\n- Hardware information\n\nThe lineage can be:\n1. **Embedded in model cards** - Shows evaluation results on HuggingFace\n2. **Saved as JSON** - For programmatic analysis and comparison\n3. **Uploaded to HuggingFace** - Alongside your model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from Evaluator.reporting import build_evaluation_lineage, generate_evaluation_model_card_section\n\n# Build evaluation configuration dict\neval_config = {\n    \"temperature\": eval_temperature,\n    \"top_p\": eval_top_p,\n    \"max_tokens\": eval_max_tokens,\n    \"seed\": eval_seed,\n}\n\n# Capture hardware info\nhardware_info = {\n    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n    \"gpu_memory_gb\": round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1) if torch.cuda.is_available() else 0,\n    \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n    \"platform\": \"Google Colab\",\n    \"vllm_config\": {\n        \"tensor_parallel_size\": tensor_parallel_size,\n        \"gpu_memory_utilization\": gpu_memory_utilization,\n        \"max_model_len\": max_model_len,\n    }\n}\n\n# Build evaluation lineage\nEVALUATION_LINEAGE = build_evaluation_lineage(\n    records=all_records,\n    model_name=MODEL_NAME,\n    test_suites=prompt_files,\n    eval_config=eval_config,\n    hardware_info=hardware_info,\n)\n\n# Generate model card section\nMODEL_CARD_EVAL_SECTION = generate_evaluation_model_card_section(EVALUATION_LINEAGE)\n\n# Save lineage to file\nlineage_file = f\"Evaluator/results/eval_lineage_{model_name_safe}_{EVAL_TIMESTAMP}.json\"\nwith open(lineage_file, 'w') as f:\n    json.dump(EVALUATION_LINEAGE, f, indent=2)\n\nprint(\"‚úì Evaluation lineage built!\")\nprint(f\"  Saved to: {lineage_file}\")\n\n# Save to Google Drive if enabled\nif save_to_drive and os.path.exists(drive_output_dir):\n    drive_lineage_file = f\"{drive_output_dir}/eval_lineage_{model_name_safe}_{EVAL_TIMESTAMP}.json\"\n    with open(drive_lineage_file, 'w') as f:\n        json.dump(EVALUATION_LINEAGE, f, indent=2)\n    print(f\"  Saved to Drive: {drive_lineage_file}\")\n\nprint()\nprint(\"üìã Lineage Summary:\")\nprint(f\"  ‚Ä¢ Model: {MODEL_NAME}\")\nprint(f\"  ‚Ä¢ Test Suites: {', '.join(prompt_files)}\")\nprint(f\"  ‚Ä¢ Pass Rate: {EVALUATION_LINEAGE['results_summary']['overall_pass_rate']}%\")\nprint(f\"  ‚Ä¢ Tests: {EVALUATION_LINEAGE['results_summary']['passed']}/{EVALUATION_LINEAGE['test_config']['total_prompts']} passed\")\nprint(f\"  ‚Ä¢ Avg Latency: {EVALUATION_LINEAGE['performance']['avg_latency_s']}s\")\n\n# Show preview of model card section\nprint(\"\\n\" + \"=\" * 60)\nprint(\"MODEL CARD SECTION PREVIEW\")\nprint(\"=\" * 60)\nprint(MODEL_CARD_EVAL_SECTION[:1500])\nif len(MODEL_CARD_EVAL_SECTION) > 1500:\n    print(\"\\n... (truncated)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Upload Results to HuggingFace (Optional)\n\n**What this does:** Upload evaluation results to your model's HuggingFace repository.\n\nThis will upload:\n- `evaluation_lineage.json` - Complete evaluation data for programmatic access\n- Update the model card with evaluation results section\n\n**Prerequisites:**\n- HF_TOKEN with write access in Colab secrets\n- Model must already exist on HuggingFace\n\n**Skip this section** if you just want to save results locally or to Google Drive.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# @title üì§ Upload to HuggingFace\n# @markdown Configure and upload evaluation results to your model's HuggingFace repository.\n\n# @markdown ### üîê HuggingFace Token\n# @markdown Make sure HF_TOKEN is set in Colab secrets (üîë icon in sidebar).\nupload_to_hf = True # @param {type:\"boolean\"}\n\n# @markdown ### üì¶ Repository Settings\n# @markdown The repo where results will be uploaded. Should match your model.\nhf_repo_id = \"professorsynapse/nexus-tools-sft-7b-merged\" # @param {type:\"string\"}\n\nif upload_to_hf:\n    from huggingface_hub import HfApi, hf_hub_download\n    import tempfile\n    \n    # Get HF token\n    try:\n        from google.colab import userdata\n        upload_token = userdata.get('HF_TOKEN')\n        if not upload_token:\n            raise ValueError(\"HF_TOKEN not found\")\n    except Exception as e:\n        print(f\"‚ùå Could not get HF_TOKEN: {e}\")\n        print(\"   Add HF_TOKEN to Colab secrets (üîë icon in sidebar)\")\n        upload_to_hf = False\n\nif upload_to_hf:\n    api = HfApi()\n    \n    print(f\"üì§ Uploading evaluation results to: {hf_repo_id}\")\n    print()\n    \n    try:\n        # 1. Upload evaluation lineage JSON\n        print(\"1. Uploading evaluation_lineage.json...\")\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n            json.dump(EVALUATION_LINEAGE, f, indent=2)\n            temp_lineage_path = f.name\n        \n        api.upload_file(\n            path_or_fileobj=temp_lineage_path,\n            path_in_repo=\"evaluation_lineage.json\",\n            repo_id=hf_repo_id,\n            token=upload_token,\n        )\n        print(\"   ‚úì evaluation_lineage.json uploaded\")\n        \n        # 2. Try to update the model card with evaluation section\n        print(\"2. Updating model card with evaluation results...\")\n        try:\n            # Download existing README\n            readme_path = hf_hub_download(\n                repo_id=hf_repo_id,\n                filename=\"README.md\",\n                token=upload_token,\n            )\n            with open(readme_path, 'r') as f:\n                existing_readme = f.read()\n            \n            # Check if evaluation section already exists\n            if \"## Evaluation Results\" in existing_readme:\n                # Replace existing evaluation section\n                import re\n                pattern = r'## Evaluation Results.*?(?=\\n## |\\Z)'\n                updated_readme = re.sub(pattern, MODEL_CARD_EVAL_SECTION, existing_readme, flags=re.DOTALL)\n                print(\"   Replacing existing evaluation section...\")\n            else:\n                # Append evaluation section\n                updated_readme = existing_readme.rstrip() + \"\\n\\n\" + MODEL_CARD_EVAL_SECTION\n                print(\"   Adding new evaluation section...\")\n            \n            # Upload updated README\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:\n                f.write(updated_readme)\n                temp_readme_path = f.name\n            \n            api.upload_file(\n                path_or_fileobj=temp_readme_path,\n                path_in_repo=\"README.md\",\n                repo_id=hf_repo_id,\n                token=upload_token,\n            )\n            print(\"   ‚úì README.md updated with evaluation results\")\n            \n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è  Could not update README: {e}\")\n            print(\"   The evaluation_lineage.json was still uploaded successfully.\")\n        \n        print()\n        print(\"=\" * 60)\n        print(\"‚úì UPLOAD COMPLETE\")\n        print(\"=\" * 60)\n        print(f\"\\nView your model: https://huggingface.co/{hf_repo_id}\")\n        print(f\"\\nUploaded files:\")\n        print(f\"  ‚Ä¢ evaluation_lineage.json - Full evaluation data\")\n        print(f\"  ‚Ä¢ README.md - Model card with evaluation section\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Upload failed: {e}\")\n        print(\"\\nTroubleshooting:\")\n        print(\"  ‚Ä¢ Verify HF_TOKEN has write access\")\n        print(\"  ‚Ä¢ Check that the repository exists\")\n        print(\"  ‚Ä¢ Ensure you have permission to write to the repo\")\nelse:\n    print(\"‚ÑπÔ∏è  Upload to HuggingFace skipped\")\n    print(\"   Set upload_to_hf = True to enable\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Quick Test Interface\n\nTest your model with custom prompts interactively.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üß™ Quick Test Interface\n",
    "# @markdown Test your model with a custom prompt.\n",
    "\n",
    "# @markdown ### Enter your test prompt:\n",
    "test_prompt = \"Can you search for all notes that mention 'Claude Code' and show me the results?\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### Generation settings:\n",
    "quick_temperature = 0.2 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
    "quick_max_tokens = 512 # @param {type:\"integer\"}\n",
    "\n",
    "print(\"ü§ñ Generating response...\\n\")\n",
    "\n",
    "# Create message\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "\n",
    "# Update client\n",
    "vllm_client.temperature = quick_temperature\n",
    "vllm_client.max_tokens = quick_max_tokens\n",
    "\n",
    "# Generate\n",
    "response = vllm_client.chat(messages)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(response.message)\n",
    "print()\n",
    "print(f\"‚è±Ô∏è  Latency: {response.latency_s:.2f}s\")\n",
    "print(f\"üìä Tokens: {response.raw.get('completion_tokens', 'N/A')}\")\n",
    "\n",
    "# Validate response\n",
    "from Evaluator.schema_validator import validate_assistant_response\n",
    "\n",
    "try:\n",
    "    validation = validate_assistant_response(response.message)\n",
    "    print(f\"\\n‚úì Validation: {'PASSED' if validation.passed else 'FAILED'}\")\n",
    "\n",
    "    if validation.tool_calls:\n",
    "        print(f\"\\nüîß Tool Calls Detected ({len(validation.tool_calls)}):\")\n",
    "        for tc in validation.tool_calls:\n",
    "            print(f\"   ‚Ä¢ {tc.name}\")\n",
    "            print(f\"     Arguments: {list(tc.arguments.keys())}\")\n",
    "\n",
    "    if validation.issues:\n",
    "        print(f\"\\n‚ö†Ô∏è  Issues ({len(validation.issues)}):\")\n",
    "        for issue in validation.issues:\n",
    "            print(f\"   ‚Ä¢ [{issue.level}] {issue.message}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Done!\n\n### What You Have\n\n| Output | Description |\n|--------|-------------|\n| **JSON Results** | Full test details with pass/fail for each prompt |\n| **Markdown Report** | Human-readable summary with tables |\n| **Evaluation Lineage** | Complete metadata for reproducibility |\n| **Model Card Section** | Auto-generated evaluation results for HuggingFace |\n| **Category Breakdown** | Pass rates per tool type |\n| **Failure Analysis** | Specific issues identified |\n\n### Lineage Tracking\n\nThe evaluation lineage captures:\n- Test suites and configurations used\n- Pass rates by category\n- Failure analysis with specific issues\n- Performance metrics (latency, throughput)\n- Hardware information (GPU, VRAM, CUDA)\n- Full JSON embedded in model card\n\n### Pass Rate Targets\n\n| Suite | Target | Description |\n|-------|--------|-------------|\n| Full Coverage | 85%+ | Model knows all 47 tools |\n| Behavioral Patterns | 75%+ | Context efficiency, executePrompt usage |\n| Baseline | 100% | General workflows |\n| Tool Combos | 80%+ | Multi-step sequences |\n\n### Common Issues\n\n- **Missing context object** - Model didn't include required context fields\n- **Wrong tool called** - Model used incorrect tool for task\n- **Invalid arguments** - Parameters don't match schema\n- **No tool call** - Model responded with text instead of tool call\n\n### Next Steps\n\n1. **Review failures** - Check the detailed failure breakdown\n2. **Identify patterns** - Are failures concentrated in specific categories?\n3. **Retrain if needed** - Use failures to improve training data\n4. **Upload results** - Evaluation results auto-populate model cards\n5. **Deploy** - Models passing 85%+ are production-ready\n\n---\n\n**Questions?** Check the [Evaluator README](https://github.com/ProfSynapse/Toolset-Training/blob/main/Evaluator/README.md) or open an issue on GitHub."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}