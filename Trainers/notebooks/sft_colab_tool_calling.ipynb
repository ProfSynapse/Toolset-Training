{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tool-Calling Fine-Tuning with SFT\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_tool_calling.ipynb)\n\n## ğŸ“ What You'll Learn\n\nThis notebook teaches you how to fine-tune a language model to use the **Claudesidian-MCP toolset** for Obsidian vault operations. By the end, you'll have:\n\n- **A custom AI model** that can call tools to manage Obsidian vaults\n- **Hands-on experience** with supervised fine-tuning (SFT)\n- **Understanding** of hyperparameters and how they affect training\n\n## ğŸ”¬ What is SFT?\n\n**SFT (Supervised Fine-Tuning)** is like teaching through examples:\n- You show the model examples of correct tool-calling behavior\n- The model learns to replicate those patterns\n- Use SFT when teaching a model a **new skill** (like using tools)\n\n**When to use SFT:**\n- âœ… Teaching tool-calling from scratch\n- âœ… Learning new task formats\n- âœ… Initial training with positive examples\n\n**Not for:**\n- âŒ Refining existing behavior (use KTO instead)\n- âŒ Teaching preferences between good/bad outputs (use preference learning)\n\n## ğŸ’» Hardware Requirements\n\n**Recommended GPU:**\n- 7B models: T4 (15GB VRAM) - âœ… **Free Colab tier works!**\n- 13B models: A100 (40GB VRAM) - Colab Pro\n- 70B models: A100 (80GB VRAM) - Colab Pro+\n\n**Training time:** ~45 minutes for a 7B model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets==4.3.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to Google Drive so they persist if runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets:\n",
    "1. Click the ğŸ”‘ key icon in the left sidebar\n",
    "2. Add new secret: `HF_TOKEN`\n",
    "3. Get token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Get your HuggingFace username automatically\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"âœ“ HuggingFace token loaded\")\n",
    "print(f\"âœ“ Username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Configuration\n\n**What this does:** Choose the base model you want to fine-tune and configure basic settings.\n\nThink of this like choosing which \"brain\" you want to teach tool-calling skills to."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nBASE MODEL SELECTION\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nChoose the foundation model that you'll train. Larger = smarter but slower.\n\"\"\"\n\n# ğŸ¯ MODEL_NAME: The base model to fine-tune\n#    What it is: The starting \"brain\" before we teach it tool-calling\n#    Trade-offs:\n#      - 7B models: Faster training (~45 min), good quality, works on free Colab T4\n#      - 8B models: Similar to 7B, slightly better at reasoning\n#      - 13B models: Better quality, slower (~90 min), needs Colab Pro (A100)\n#\nMODEL_NAME = \"unsloth/mistral-7b-v0.3-bnb-4bit\"  # â† Change this to experiment!\n\n# Other model options (uncomment to try):\n# MODEL_NAME = \"unsloth/llama-3.1-8b-instruct-bnb-4bit\"  # Good for reasoning\n# MODEL_NAME = \"unsloth/llama-2-13b-bnb-4bit\"            # Highest quality, slower\n\n\n# ğŸ¯ MAX_SEQ_LENGTH: Maximum conversation length in tokens\n#    What it is: How much text the model can \"see\" at once\n#    Trade-offs:\n#      - Higher (4096): Can handle longer conversations, uses more VRAM\n#      - Lower (1024): Faster training, less memory, but truncates long examples\n#      - Sweet spot (2048): Balances context and memory usage\n#\nMAX_SEQ_LENGTH = 2048  # â† Most examples fit in 2048 tokens\n\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nDATASET CONFIGURATION\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\"\n\n# ğŸ¯ DATASET: Where to get training examples\n#    What it is: Pre-made examples of correct tool-calling behavior\n#    This dataset has ~5,500 high-quality examples of tool usage\n#\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\"\nDATASET_FILE = \"syngen_tools_sft_pingpong_11.18.25.jsonl\"\n\n# Note: You can upload your own dataset to HuggingFace and change these values!\n\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nOUTPUT CONFIGURATION\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\"\n\n# ğŸ¯ OUTPUT_MODEL_NAME: What to call your trained model\n#    What it is: The name that appears on HuggingFace\n#    Pick something descriptive like \"nexus-tools-7b\" or \"my-assistant-v1\"\n#\nOUTPUT_MODEL_NAME = \"nexus-tools-sft\"  # â† Customize this!\n\n\n# ============================================================================\n# Print configuration summary\n# ============================================================================\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"\\nOutput will be uploaded to:\")\nprint(f\"  - LoRA adapters: {hf_user}/{OUTPUT_MODEL_NAME}\")\nprint(f\"  - Merged model: {hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Load Model and Tokenizer\n\n**What this does:** Downloads the base model and prepares it for training.\n\nThe model is the \"brain\" that will learn tool-calling. The tokenizer converts text into numbers the model can process. We use 4-bit quantization to fit large models into limited GPU memory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the base model and tokenizer from HuggingFace\n# This downloads the model weights (~7GB for 7B models)\n# \n# Parameters explained:\n#   model_name: Which model to download\n#   max_seq_length: Max tokens model can process at once\n#   dtype=None: Auto-detect best precision for your GPU\n#   load_in_4bit=True: Use 4-bit quantization to save memory\n#   token: Your HF token for accessing the model\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=MODEL_NAME,\n    max_seq_length=MAX_SEQ_LENGTH,\n    dtype=None,  # Auto-detect (usually bfloat16 or float16)\n    load_in_4bit=True,  # Reduces memory usage by ~75%\n    token=HF_TOKEN,\n)\n\nprint(\"âœ“ Model loaded successfully\")\nprint(f\"  Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Apply LoRA Adapters\n\n**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n\nThink of LoRA like teaching a new skill through muscle memory - we add small specialized layers that learn the new behavior, while keeping the main \"brain\" frozen. This is way faster and uses less memory than retraining everything."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nLORA CONFIGURATION - How the model learns\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThese parameters control how we add trainable \"adapter\" layers to the model.\n\"\"\"\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    \n    # ğŸ¯ r (rank): Size of the adapter matrices\n    #    What it controls: Learning capacity vs memory usage\n    #    Turn UP (64, 128): More capacity to learn complex patterns, uses more VRAM\n    #    Turn DOWN (8, 16): Faster, less memory, but might not capture all nuances\n    #    Sweet spot (32): Good balance for most tasks\n    r=32,\n    \n    # ğŸ¯ lora_alpha: Scaling factor for LoRA updates\n    #    What it controls: How much the adapters influence the model\n    #    Turn UP (128): Stronger influence from fine-tuning (be careful of overfitting!)\n    #    Turn DOWN (16): More conservative, preserves base model behavior\n    #    Rule of thumb: Usually set to 2x the rank (if r=32, use alpha=64)\n    lora_alpha=64,\n    \n    # ğŸ¯ lora_dropout: Regularization to prevent overfitting\n    #    What it controls: Randomly drops connections during training\n    #    Turn UP (0.1): More regularization, helps if model memorizes too much\n    #    Turn DOWN (0.0): No regularization, use if you have lots of diverse data\n    #    Sweet spot (0.05): Light regularization for most cases\n    lora_dropout=0.05,\n    \n    # ğŸ¯ bias: Whether to train bias parameters\n    #    Options: \"none\" (don't train), \"all\" (train all), \"lora_only\" (train LoRA biases)\n    #    \"none\" is standard and works well for most cases\n    bias=\"none\",\n    \n    # ğŸ¯ target_modules: Which parts of the model to add adapters to\n    #    What it controls: Where learning happens in the neural network\n    #    These are the \"attention\" and \"feed-forward\" layers\n    #    More modules = more capacity but slower training\n    #    This list covers all the key layers for language understanding\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    \n    # ğŸ¯ use_gradient_checkpointing: Memory optimization technique\n    #    What it controls: Trade computation for memory\n    #    \"unsloth\": Fast checkpointing (recommended)\n    #    False: Faster but uses more VRAM\n    use_gradient_checkpointing=\"unsloth\",\n    \n    # ğŸ¯ random_state: Random seed for reproducibility\n    #    What it controls: Ensures same initialization each run\n    #    Change this number to get different random starting points\n    random_state=3407,\n)\n\nprint(\"âœ“ LoRA adapters applied\")\nprint(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"  Percentage trainable: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Load and Prepare Dataset\n\n**What this does:** Downloads training examples and formats them for the model.\n\nThe dataset contains ~5,500 examples of correct tool-calling behavior. Think of it like a textbook full of solved problems that the model will learn from."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nLOAD DATASET FROM HUGGINGFACE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThis downloads pre-made training examples of correct tool-calling behavior.\nEach example shows: user request â†’ tool call â†’ result â†’ assistant response\n\"\"\"\n\nprint(f\"Loading dataset: {DATASET_NAME}\")\ndataset = load_dataset(\n    DATASET_NAME,  # HuggingFace repository containing the dataset\n    data_files=DATASET_FILE,  # Specific JSONL file to use\n    split=\"train\"  # Use the training split\n)\n\nprint(f\"âœ“ Loaded {len(dataset)} examples\")\nprint(f\"\\nSample example:\")\nprint(dataset[0])\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSET CHAT TEMPLATE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThe chat template defines how conversations are formatted for the model.\nWe use ChatML format: <|im_start|>role\\ncontent<|im_end|>\n\"\"\"\n\nif tokenizer.chat_template is None:\n    print(\"\\nâš ï¸  Tokenizer has no chat template, setting ChatML template...\")\n    # ChatML format is widely used and works well for tool-calling\n    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + message['content'] + '<|im_end|>\\\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\\\n' + message['content'] + '<|im_end|>\\\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\\n' }}{% endif %}\"\n    print(\"âœ“ Chat template set to ChatML format\")\nelse:\n    print(\"\\nâœ“ Tokenizer already has chat template\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nFORMAT DATASET FOR TRAINING\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nConvert the conversation format into the exact text format the model expects.\nThis applies the chat template to each example.\n\"\"\"\n\ndef format_chat_template(example):\n    \"\"\"\n    Convert conversations to tokenizer's chat template.\n    \n    Input: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, ...]}\n    Output: {\"text\": \"<|im_start|>user\\n...<|im_end|>\\n...\"}\n    \"\"\"\n    text = tokenizer.apply_chat_template(\n        example[\"conversations\"],  # List of message dicts\n        tokenize=False,  # Return string, not token IDs\n        add_generation_prompt=False  # Don't add prompt for model to continue\n    )\n    return {\"text\": text}\n\n# Apply formatting to entire dataset\n# This creates a new \"text\" field with formatted conversations\ndataset = dataset.map(\n    format_chat_template,  # Function to apply\n    remove_columns=dataset.column_names,  # Remove original columns\n    desc=\"Formatting dataset\"  # Progress bar description\n)\n\nprint(\"âœ“ Dataset formatted for training\")\nprint(f\"\\nFormatted example (first 500 characters):\")\nprint(dataset[0][\"text\"][:500])\nprint(\"\\nğŸ’¡ The full formatted text will be used during training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Training Configuration\n\n**What this does:** Set the hyperparameters that control how the model learns.\n\nThis is the **most important section** - these settings determine how fast the model learns, how much memory it uses, and how good the final result will be. Think of it like configuring a study plan: how many hours per day, how many review sessions, how to handle difficult material, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bfloat16_supported\nfrom datetime import datetime\n\n# Create timestamped output directory\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_dir = f\"{DRIVE_OUTPUT_DIR}/{timestamp}\"\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTRAINING HYPERPARAMETERS - How the model learns\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThese control the learning process. Getting these right is crucial!\n\"\"\"\n\ntraining_args = SFTConfig(\n    output_dir=output_dir,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # BATCH SIZE & MEMORY SETTINGS\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ per_device_train_batch_size: Examples processed at once\n    #    What it controls: How many training examples fit in GPU memory at once\n    #    Turn UP (4, 8): Faster training, but uses more VRAM (might run out of memory!)\n    #    Turn DOWN (1): Slower but uses less memory (use if you get OOM errors)\n    #    Sweet spot (2): Works on most free Colab GPUs\n    per_device_train_batch_size=2,\n    \n    # ğŸ¯ gradient_accumulation_steps: Simulate larger batches\n    #    What it controls: How many mini-batches before updating weights\n    #    Turn UP (8, 16): Simulates larger batch, more stable training\n    #    Turn DOWN (1, 2): Faster updates, but noisier learning\n    #    Effective batch = per_device_batch_size Ã— gradient_accumulation_steps\n    #    Example: 2 Ã— 4 = 8 effective batch size\n    gradient_accumulation_steps=4,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LEARNING RATE & OPTIMIZATION\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ learning_rate: How fast the model learns\n    #    What it controls: Size of weight updates during training\n    #    Turn UP (5e-4): Faster learning, but might overshoot optimal values\n    #    Turn DOWN (1e-5): More careful learning, but takes longer\n    #    Sweet spot (2e-4): Standard for SFT (100x higher than preference learning!)\n    #    NOTE: This is MUCH higher than KTO learning rates (2e-7)\n    learning_rate=2e-4,\n    \n    # ğŸ¯ max_grad_norm: Gradient clipping to prevent exploding gradients\n    #    What it controls: Maximum size of weight updates\n    #    Turn UP (2.0): Allows larger updates (might cause instability)\n    #    Turn DOWN (0.5): More conservative (slower learning)\n    #    Sweet spot (1.0): Prevents extreme updates while allowing learning\n    max_grad_norm=1.0,\n    \n    # ğŸ¯ lr_scheduler_type: How learning rate changes over time\n    #    What it controls: Learning rate adjustment strategy\n    #    \"cosine\": Starts high, smoothly decreases (recommended)\n    #    \"linear\": Decreases linearly\n    #    \"constant\": Never changes (simpler but less effective)\n    lr_scheduler_type=\"cosine\",\n    \n    # ğŸ¯ warmup_ratio: Fraction of training with increasing learning rate\n    #    What it controls: How long to \"warm up\" before full learning rate\n    #    Turn UP (0.2): Longer warmup, more stable early training\n    #    Turn DOWN (0.0): No warmup, jumps straight to full LR\n    #    Sweet spot (0.1): 10% of training spent warming up\n    warmup_ratio=0.1,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # TRAINING DURATION\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ num_train_epochs: How many times to go through the entire dataset\n    #    What it controls: Total training time and learning opportunity\n    #    Turn UP (5, 10): More learning, but risk of overfitting (memorization)\n    #    Turn DOWN (1): Faster but might not learn everything\n    #    Sweet spot (3): Standard for SFT - enough to learn without memorizing\n    #    NOTE: KTO only uses 1 epoch because it's for refinement, not initial learning\n    num_train_epochs=3,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # DATA HANDLING\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ max_seq_length: Maximum tokens per example (matches earlier config)\n    max_seq_length=MAX_SEQ_LENGTH,\n    \n    # ğŸ¯ packing: Whether to combine short examples into one sequence\n    #    What it controls: Efficiency of sequence processing\n    #    True: Packs multiple short examples together (more efficient)\n    #    False: Each example is separate (simpler, safer)\n    #    Set to False for tool-calling to keep examples independent\n    packing=False,\n    \n    # ğŸ¯ dataset_text_field: Which field in dataset contains the text\n    #    This tells the trainer where to find the formatted conversations\n    dataset_text_field=\"text\",\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # PRECISION & OPTIMIZATION\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ fp16 / bf16: Mixed precision training (speed + memory optimization)\n    #    What it controls: Number precision during training\n    #    bf16 (bfloat16): Better for newer GPUs (A100, T4), more stable\n    #    fp16 (float16): For older GPUs, slightly less stable\n    #    Auto-detect: Uses bf16 if supported, fp16 otherwise\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    \n    # ğŸ¯ optim: Optimizer algorithm\n    #    What it controls: How weight updates are calculated\n    #    \"adamw_8bit\": Memory-efficient version of AdamW (recommended)\n    #    \"adamw_torch\": Standard AdamW (uses more memory)\n    #    \"sgd\": Simpler but less effective for LLMs\n    optim=\"adamw_8bit\",\n    \n    # ğŸ¯ gradient_checkpointing: Save memory by recomputing during backward pass\n    #    What it controls: Memory vs speed tradeoff\n    #    True: Uses less memory (~30% reduction), slightly slower\n    #    False: Faster but uses more memory\n    gradient_checkpointing=True,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LOGGING & CHECKPOINTING\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ logging_steps: How often to print training progress\n    #    What it controls: Frequency of console updates\n    #    Turn UP (10, 20): Less frequent updates, cleaner logs\n    #    Turn DOWN (1): See every step (very verbose!)\n    #    Sweet spot (5): Regular updates without spam\n    logging_steps=5,\n    \n    # ğŸ¯ save_steps: How often to save checkpoint\n    #    What it controls: Backup frequency during training\n    #    Turn UP (200, 500): Less frequent saves, saves disk space\n    #    Turn DOWN (50): More backups, can resume from more points\n    #    Sweet spot (100): Good balance for ~45min training\n    save_steps=100,\n    \n    # ğŸ¯ save_total_limit: Maximum number of checkpoints to keep\n    #    What it controls: Disk space used for checkpoints\n    #    Turn UP (5, 10): Keep more history, uses more space\n    #    Turn DOWN (1, 2): Only keep latest, saves space\n    #    Sweet spot (3): Last 3 checkpoints (in case latest has issues)\n    save_total_limit=3,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # REPRODUCIBILITY\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ seed: Random seed for reproducible results\n    #    What it controls: Ensures same results across runs\n    #    Change this to get different random variations\n    seed=42,\n    \n    # ğŸ¯ report_to: Where to log metrics (wandb, tensorboard, etc.)\n    #    \"none\": Don't upload metrics anywhere\n    #    \"wandb\": Log to Weights & Biases (requires setup)\n    #    \"tensorboard\": Log locally\n    report_to=\"none\",\n)\n\n# ============================================================================\n# Print configuration summary\n# ============================================================================\nprint(\"âœ“ Training configuration ready\")\nprint(f\"  Output directory: {output_dir}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Precision: {'bf16' if is_bfloat16_supported() else 'fp16'}\")\nprint(f\"\\nğŸ’¡ Training will take approximately 45 minutes for a 7B model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Initialize Trainer\n\n**What this does:** Creates the training engine that coordinates everything.\n\nThe SFTTrainer is the orchestrator - it takes the model, dataset, and configuration, then handles all the training mechanics (gradient updates, checkpointing, logging, etc.)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the SFTTrainer\n# This combines the model, dataset, and configuration into one training pipeline\n\ntrainer = SFTTrainer(\n    model=model,  # The model with LoRA adapters\n    tokenizer=tokenizer,  # For converting text to tokens\n    train_dataset=dataset,  # Our formatted training examples\n    args=training_args,  # All the hyperparameters we configured\n)\n\nprint(\"âœ“ Trainer initialized\")\nprint(\"  Ready to start training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Train!\n\n**What this does:** The actual learning happens here!\n\nThe model will:\n1. **Read examples** from the dataset\n2. **Predict** what the response should be\n3. **Compare** its prediction to the correct answer\n4. **Update weights** to get closer to the correct answer\n5. **Repeat** this process for 3 epochs (3 full passes through the data)\n\n**What to expect:**\n- Training takes ~45 minutes for 7B models on T4 GPU\n- You'll see progress updates every 5 steps\n- Loss should generally decrease over time (learning is working!)\n- Checkpoints are saved every 100 steps to Google Drive\n\n**What the metrics mean:**\n- **Loss:** How \"wrong\" the model is (lower = better, aim for <1.0)\n- **Learning Rate:** Gradually decreases as training progresses\n- **Samples/sec:** Training speed (depends on GPU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# Start training\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Upload to HuggingFace\n\n**What this does:** Share your trained model with the world!\n\nWe'll create **three versions** of your model:\n\n1. **LoRA adapters** (~320MB) - Small files that contain just the changes\n   - Fast to download\n   - Need to be combined with base model to use\n   \n2. **Merged 16-bit model** (~14GB) - Full model with adapters merged in\n   - High quality, no precision loss\n   - Large file size\n   - Best for local deployment (Ollama, LM Studio)\n   \n3. **GGUF quantizations** - Optimized versions for CPU/GPU inference\n   - Q4_K_M (~3.5GB) - Recommended for most users\n   - Q5_K_M (~4.5GB) - Better quality, larger size\n   - Q8_0 (~7GB) - Nearly full quality\n   - These work directly with Ollama and LM Studio!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload LoRA adapters\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Merged model uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GGUF quantizations\n",
    "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    "\n",
    "print(\"Creating GGUF quantizations...\")\n",
    "print(f\"This will create {len(quantization_methods)} versions\")\n",
    "print()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_methods,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"âœ“ GGUF quantizations created and uploaded!\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ‰ Done!\n\nYour model has been trained and uploaded to HuggingFace!\n\n## ğŸ“Š What You Accomplished\n\nâœ… **Fine-tuned a 7B parameter language model** to use the Claudesidian toolset  \nâœ… **Learned about SFT hyperparameters** and how they affect training  \nâœ… **Created multiple model formats** (LoRA, merged, GGUF) for different use cases  \nâœ… **Published your model** to HuggingFace for others to use  \n\n## ğŸš€ Next Steps\n\n### 1. Test Your Model\n\n**Using LM Studio:**\n1. Open LM Studio â†’ \"Discover\" tab\n2. Search for your username: `{hf_user}`\n3. Download the Q4_K_M or Q5_K_M GGUF version\n4. Load and test with tool-calling prompts!\n\n**Using Ollama:**\n```bash\n# Download your model\nollama pull {hf_user}/{OUTPUT_MODEL_NAME}\n\n# Test it\nollama run {hf_user}/{OUTPUT_MODEL_NAME}\n```\n\n### 2. Evaluate Quality\n\nRun the Evaluator to test tool-calling accuracy:\n```bash\npython -m Evaluator.cli \\\n  --model {OUTPUT_MODEL_NAME} \\\n  --prompt-set Evaluator/prompts/full_coverage.json \\\n  --output results.json\n```\n\n### 3. Refine Further (Optional)\n\nIf you want to improve your model:\n- **Collect more data**: Add examples where your model struggles\n- **Try KTO training**: Refine behavior by showing good vs bad examples\n- **Adjust hyperparameters**: Experiment with learning rate, batch size, etc.\n\n## ğŸ“ Learn More\n\n- **KTO Training**: For preference learning (good vs bad outputs)\n- **Dataset Creation**: Build custom training data for your use case\n- **Model Evaluation**: Systematic testing of tool-calling accuracy\n\n## ğŸ’¡ Tips for Better Models\n\n1. **More data is better**: 5,000+ examples produce robust models\n2. **Quality over quantity**: Clean, accurate examples matter more than volume\n3. **Test thoroughly**: Use the Evaluator to catch issues early\n4. **Iterate**: Fine-tuning is iterative - train, test, improve, repeat\n\n---\n\n**Questions or issues?** Check the documentation or open an issue on GitHub!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}