{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tool-Calling Fine-Tuning with SFT\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_tool_calling.ipynb)\n",
        "\n",
        "Train models to use the **Claudesidian-MCP toolset** for Obsidian vault operations.\n",
        "\n",
        "**Method:** SFT (Supervised Fine-Tuning) - Direct supervision for learning tool-calling behavior\n",
        "\n",
        "**Recommended GPU:**\n",
        "- 7B models: T4 (15GB) - Free Colab tier\n",
        "- 13B models: A100 (40GB) - Colab Pro\n",
        "- 70B models: A100 (80GB) - Colab Pro+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation\n",
        "\n",
        "Install Unsloth and dependencies. This takes ~2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Unsloth for faster training\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install training dependencies\n",
        "%%capture\n",
        "!pip install -U \"transformers>=4.45.0\"\n",
        "!pip install \"datasets==4.3.0\"\n",
        "!pip install -U accelerate bitsandbytes\n",
        "!pip install -U trl peft xformers triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive (Optional)\n",
        "\n",
        "Save checkpoints to Google Drive so they persist if runtime disconnects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directory\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"‚úì Google Drive mounted\")\n",
        "print(f\"‚úì Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. HuggingFace Credentials\n",
        "\n",
        "Add your HF token to Colab secrets:\n",
        "1. Click the üîë key icon in the left sidebar\n",
        "2. Add new secret: `HF_TOKEN`\n",
        "3. Get token from https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Get token from Colab secrets\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "# Get your HuggingFace username automatically\n",
        "api = HfApi()\n",
        "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
        "\n",
        "print(f\"‚úì HuggingFace token loaded\")\n",
        "print(f\"‚úì Username: {hf_user}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model options:\n",
        "#   7B:  \"unsloth/mistral-7b-v0.3-bnb-4bit\" (recommended for T4)\n",
        "#   8B:  \"unsloth/llama-3.1-8b-instruct-bnb-4bit\"\n",
        "#   13B: \"unsloth/llama-2-13b-bnb-4bit\" (requires A100)\n",
        "\n",
        "MODEL_NAME = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "# Dataset\n",
        "DATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\"\n",
        "DATASET_FILE = \"syngen_tools_sft_pingpong_11.18.25.jsonl\"\n",
        "\n",
        "# Output model name\n",
        "OUTPUT_MODEL_NAME = \"nexus-tools-sft\"\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Dataset: {DATASET_NAME}/{DATASET_FILE}\")\n",
        "print(f\"\\nOutput will be uploaded to:\")\n",
        "print(f\"  - LoRA: {hf_user}/{OUTPUT_MODEL_NAME}\")\n",
        "print(f\"  - Merged: {hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Check GPU\n",
        "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "print(\"‚úì Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Apply LoRA Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"‚úì LoRA adapters applied\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "print(f\"Loading dataset: {DATASET_NAME}\")\n",
        "dataset = load_dataset(\n",
        "    DATASET_NAME,\n",
        "    data_files=DATASET_FILE,\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(f\"‚úì Loaded {len(dataset)} examples\")\n",
        "print(f\"\\nSample:\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Set chat template if not present\n",
        "if tokenizer.chat_template is None:\n",
        "    print(\"\\n‚ö†Ô∏è  Tokenizer has no chat template, setting ChatML template...\")\n",
        "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\\\n' + message['content'] + '<|im_end|>\\\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\\\n' + message['content'] + '<|im_end|>\\\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\\\n' }}{% endif %}\"\n",
        "    print(\"‚úì Chat template set to ChatML format\")\n",
        "else:\n",
        "    print(\"\\n‚úì Tokenizer already has chat template\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format dataset for SFT training\n",
        "def format_chat_template(example):\n",
        "    \"\"\"Convert conversations to tokenizer's chat template.\"\"\"\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"conversations\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"Formatting dataset\"\n",
        ")\n",
        "\n",
        "print(\"‚úì Dataset formatted for training\")\n",
        "print(f\"\\nFormatted example (first 500 chars):\")\n",
        "print(dataset[0][\"text\"][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped output directory\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = f\"{DRIVE_OUTPUT_DIR}/{timestamp}\"\n",
        "\n",
        "# Training configuration\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    max_grad_norm=1.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    num_train_epochs=3,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    packing=False,\n",
        "    dataset_text_field=\"text\",\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    gradient_checkpointing=True,\n",
        "    logging_steps=5,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    seed=42,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"‚úì Training configuration ready\")\n",
        "print(f\"  Output directory: {output_dir}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Initialize Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"‚úì Trainer initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Train!\n",
        "\n",
        "This will take ~45 minutes for 7B models on T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU memory\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "print()\n",
        "\n",
        "# Start training\n",
        "print(\"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Upload to HuggingFace\n",
        "\n",
        "Upload LoRA adapters, merged model, and GGUF quantizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload LoRA adapters\n",
        "model.push_to_hub(\n",
        "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
        "    token=HF_TOKEN,\n",
        "    private=False\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
        "    token=HF_TOKEN,\n",
        "    private=False\n",
        ")\n",
        "\n",
        "print(f\"‚úì LoRA adapters uploaded to HuggingFace\")\n",
        "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload merged 16-bit model\n",
        "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
        "print(\"This will take ~5 minutes...\")\n",
        "\n",
        "model.push_to_hub_merged(\n",
        "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        "    token=HF_TOKEN,\n",
        "    private=False\n",
        ")\n",
        "\n",
        "print(f\"‚úì Merged model uploaded to HuggingFace\")\n",
        "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GGUF quantizations\n",
        "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
        "\n",
        "print(\"Creating GGUF quantizations...\")\n",
        "print(f\"This will create {len(quantization_methods)} versions\")\n",
        "print()\n",
        "\n",
        "model.push_to_hub_gguf(\n",
        "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
        "    tokenizer,\n",
        "    quantization_method=quantization_methods,\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "print()\n",
        "print(\"‚úì GGUF quantizations created and uploaded!\")\n",
        "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done!\n",
        "\n",
        "Your model has been trained and uploaded to HuggingFace.\n",
        "\n",
        "**Next steps:**\n",
        "1. Download GGUF files for Ollama/LM Studio\n",
        "2. Run the Evaluator to test tool-calling accuracy\n",
        "3. Deploy to production\n",
        "\n",
        "**Using your model in LM Studio:**\n",
        "1. Go to \"Discover\" tab\n",
        "2. Search for your username\n",
        "3. Download Q4_K_M or Q5_K_M version\n",
        "4. Load and test!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
