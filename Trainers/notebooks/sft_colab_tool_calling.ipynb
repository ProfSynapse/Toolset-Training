{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool-Calling Fine-Tuning with SFT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_tool_calling.ipynb)\n",
    "\n",
    "Train models to use the **Claudesidian-MCP toolset** for Obsidian vault operations.\n",
    "\n",
    "**Method:** SFT (Supervised Fine-Tuning) - Direct supervision for learning tool-calling behavior\n",
    "\n",
    "**Recommended GPU:** \n",
    "- 7B models: T4 (15GB) - Free Colab tier\n",
    "- 13B models: A100 (40GB) - Colab Pro\n",
    "- 70B models: A100 (80GB) - Colab Pro+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install training dependencies with specific versions\n%%capture\n!pip install -U \"transformers>=4.45.0\"\n!pip install \"datasets==4.3.0\"  # Specific version required by Unsloth\n!pip install -U accelerate bitsandbytes\n!pip install -U trl peft xformers triton"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Mount Google Drive (Optional but Recommended)\n\nSave checkpoints to Google Drive so they persist if runtime disconnects."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create output directory in Google Drive\nimport os\nDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\nos.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n\nprint(f\"✓ Google Drive mounted\")\nprint(f\"✓ Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "source": "# HuggingFace credentials (get token from https://huggingface.co/settings/tokens)\nimport os\nfrom google.colab import userdata\n\n# Store your HF token in Colab secrets (left sidebar → key icon)\n# Secret name: HF_TOKEN\nHF_TOKEN = userdata.get('HF_TOKEN')\nos.environ['HF_TOKEN'] = HF_TOKEN\n\n# Get HuggingFace username\nfrom huggingface_hub import HfApi\napi = HfApi()\nhf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n\nprint(f\"✓ HuggingFace token loaded\")\nprint(f\"✓ Username: {hf_user}\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Configuration\n# Options:\n#   7B:  \"unsloth/mistral-7b-v0.3-bnb-4bit\" (recommended for T4)\n#   8B:  \"unsloth/llama-3.1-8b-instruct-bnb-4bit\"\n#   13B: \"unsloth/llama-2-13b-bnb-4bit\" (requires A100)\n#   70B: \"unsloth/llama-3.1-70b-instruct-bnb-4bit\" (requires A100 80GB)\n\nMODEL_NAME = \"unsloth/mistral-7b-v0.3-bnb-4bit\"  # Change this for different models\nMAX_SEQ_LENGTH = 2048\n\n# Dataset\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\"\nDATASET_FILE = \"syngen_tools_sft_pingpong_11.18.25.jsonl\"\n\n# Output (will be uploaded to: hf_user/OUTPUT_MODEL_NAME)\nOUTPUT_MODEL_NAME = \"nexus-tools-sft\"\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Dataset: {DATASET_NAME}/{DATASET_FILE}\")\nprint(f\"Output will be uploaded to:\")\nprint(f\"  - LoRA: {hf_user}/{OUTPUT_MODEL_NAME}\")\nprint(f\"  - Merged: {hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply LoRA Adapters\n",
    "\n",
    "LoRA allows efficient fine-tuning by training only a small percentage of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # LoRA rank\n",
    "    lora_alpha=64,  # LoRA alpha scaling\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"✓ LoRA adapters applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\n# Load dataset from HuggingFace\nprint(f\"Loading dataset: {DATASET_NAME}\")\ndataset = load_dataset(\n    DATASET_NAME,\n    data_files=DATASET_FILE,\n    split=\"train\"\n)\n\nprint(f\"✓ Loaded {len(dataset)} examples\")\nprint(f\"\\nSample:\")\nprint(dataset[0])\n\n# IMPORTANT: Set chat template if not already set\nif tokenizer.chat_template is None:\n    print(\"\\n⚠️  Tokenizer has no chat template, setting ChatML template...\")\n    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n    print(\"✓ Chat template set to ChatML format\")\nelse:\n    print(\"\\n✓ Tokenizer already has chat template\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for SFT training\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Convert conversations to tokenizer's chat template.\"\"\"\n",
    "    conversations = example[\"conversations\"]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversations,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Formatting dataset\"\n",
    ")\n",
    "\n",
    "print(\"✓ Dataset formatted for training\")\n",
    "print(f\"\\nFormatted example (first 500 chars):\")\n",
    "print(dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bfloat16_supported\nfrom datetime import datetime\n\n# Create timestamped output directory\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_dir = f\"{DRIVE_OUTPUT_DIR}/{timestamp}\"\n\n# Training configuration\ntraining_args = SFTConfig(\n    # Output (saved to Google Drive)\n    output_dir=output_dir,\n    \n    # Batch configuration\n    per_device_train_batch_size=2,  # Adjust based on GPU memory\n    gradient_accumulation_steps=4,  # Effective batch = 8\n    \n    # Learning rate (MUCH higher than KTO)\n    learning_rate=2e-4,\n    max_grad_norm=1.0,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    \n    # Training schedule\n    num_train_epochs=3,\n    \n    # Sequence length\n    max_seq_length=MAX_SEQ_LENGTH,\n    \n    # SFT-specific\n    packing=False,\n    dataset_text_field=\"text\",\n    \n    # Optimizations\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    optim=\"adamw_8bit\",\n    gradient_checkpointing=True,\n    \n    # Logging & Checkpointing (saved to Google Drive)\n    logging_steps=5,\n    save_steps=100,  # Save checkpoint every 100 steps\n    save_total_limit=3,  # Keep last 3 checkpoints\n    \n    # Misc\n    seed=42,\n    report_to=\"none\",  # Disable W&B for Colab\n)\n\nprint(\"✓ Training configuration ready\")\nprint(f\"  Output directory: {output_dir}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Checkpoints every: {training_args.save_steps} steps\")\nprint(f\"  Keeping last: {training_args.save_total_limit} checkpoints\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    # Output\n",
    "    output_dir=\"./outputs\",\n",
    "    \n",
    "    # Batch configuration\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,  # Effective batch = 8\n",
    "    \n",
    "    # Learning rate (MUCH higher than KTO)\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Sequence length\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    \n",
    "    # SFT-specific\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    # Optimizations\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Disable W&B for Colab\n",
    ")\n",
    "\n",
    "print(\"✓ Training configuration ready\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train!\n",
    "\n",
    "This will take ~45 minutes for 7B models, ~1.5 hours for 13B models."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Start training\nprint(\"=\" * 60)\nprint(\"STARTING TRAINING\")\nprint(\"=\" * 60)\nprint()\n\nif RESUME_FROM_CHECKPOINT:\n    print(f\"Resuming from checkpoint: {RESUME_FROM_CHECKPOINT}\\n\")\n\ntrainer_stats = trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n\nprint()\nprint(\"=\" * 60)\nprint(\"TRAINING COMPLETED\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Resume from Checkpoint (Optional)\n\nIf training was interrupted, you can resume from the last checkpoint.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload LoRA adapters to HuggingFace\nmodel.push_to_hub(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    token=HF_TOKEN,\n    private=False\n)\ntokenizer.push_to_hub(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    token=HF_TOKEN,\n    private=False\n)\n\nprint(f\"✓ LoRA adapters uploaded to HuggingFace\")\nprint(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Upload merged 16-bit model (full quality)\nprint(\"Merging LoRA weights into base model (16-bit)...\")\nprint(\"This will take ~5 minutes...\")\n\nmodel.push_to_hub_merged(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n    tokenizer,\n    save_method=\"merged_16bit\",\n    token=HF_TOKEN,\n    private=False\n)\n\nprint(f\"✓ Merged model uploaded to HuggingFace\")\nprint(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
  },
  {
   "cell_type": "code",
   "source": "# Create GGUF quantizations\n# Quantization types:\n#   - q4_k_m: 4-bit, medium quality, ~4GB (recommended for most use cases)\n#   - q5_k_m: 5-bit, higher quality, ~5GB\n#   - q8_0: 8-bit, best quality, ~8GB\n\nquantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n\nprint(\"Creating GGUF quantizations...\")\nprint(f\"This will create {len(quantization_methods)} versions and upload them to HuggingFace\")\nprint()\n\nmodel.push_to_hub_gguf(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    tokenizer,\n    quantization_method=quantization_methods,\n    token=HF_TOKEN,\n)\n\nprint()\nprint(\"✓ GGUF quantizations created and uploaded!\")\nprint(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")\nprint()\nprint(\"GGUF files created:\")\nfor method in quantization_methods:\n    print(f\"  - {OUTPUT_MODEL_NAME}-{method.upper()}.gguf\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Create GGUF Quantizations (Optional)\n\nCreate GGUF versions for Ollama and llama.cpp. This takes ~10-15 minutes.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Done!\n\nYour model has been trained and uploaded to HuggingFace. \n\n**Next steps:**\n1. Download GGUF files for Ollama/LM Studio\n2. Run the Evaluator to test tool-calling accuracy\n3. Deploy to production\n\n**Model locations:**\n- LoRA adapters: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\n- Merged 16-bit: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\n- GGUF quantizations: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME} (Files tab)\n\n**Using the GGUF models:**\n\nOllama:\n```bash\n# Create Modelfile\nFROM {hf_user}/{OUTPUT_MODEL_NAME}:Q4_K_M\n\n# Create model\nollama create my-tool-model -f Modelfile\nollama run my-tool-model\n```\n\nLM Studio:\n1. Go to \"Discover\" tab\n2. Search for `{hf_user}/{OUTPUT_MODEL_NAME}`\n3. Download Q4_K_M or Q5_K_M version\n4. Load and test!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload LoRA adapters to HuggingFace\nmodel.push_to_hub(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    token=HF_TOKEN,\n    private=False\n)\ntokenizer.push_to_hub(\n    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n    token=HF_TOKEN,\n    private=False\n)\n\nprint(f\"✓ LoRA adapters uploaded to HuggingFace\")\nprint(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Done!\n\nYour model has been trained and uploaded to HuggingFace. \n\n**Next steps:**\n1. Download the model for local testing\n2. Create GGUF quantizations for Ollama/llama.cpp\n3. Run the Evaluator to test tool-calling accuracy\n\n**Model locations:**\n- LoRA adapters: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\n- Merged 16-bit: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Model (Optional)\n",
    "\n",
    "Quick test to see if the model learned tool-calling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a new note called 'Meeting Notes' with the content 'Discussed Q4 roadmap'\"}\n",
    "]\n",
    "\n",
    "# Format and generate\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model response:\")\n",
    "print(\"=\" * 60)\n",
    "print(response)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Your model has been trained and uploaded to HuggingFace. \n",
    "\n",
    "**Next steps:**\n",
    "1. Download the model for local testing\n",
    "2. Create GGUF quantizations for Ollama/llama.cpp\n",
    "3. Run the Evaluator to test tool-calling accuracy\n",
    "\n",
    "**Model locations:**\n",
    "- LoRA adapters: `https://huggingface.co/YOUR-USERNAME/{OUTPUT_MODEL_NAME}`\n",
    "- Merged 16-bit: `https://huggingface.co/YOUR-USERNAME/{OUTPUT_MODEL_NAME}-merged`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}