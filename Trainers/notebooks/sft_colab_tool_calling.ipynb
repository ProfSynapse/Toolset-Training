{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tool-Calling Fine-Tuning with SFT\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/sft_colab_tool_calling.ipynb)\n\n## ğŸ“ What You'll Learn\n\nThis notebook teaches you how to fine-tune a language model to use the **Claudesidian-MCP toolset** for Obsidian vault operations. By the end, you'll have:\n\n- **A custom AI model** that can call tools to manage Obsidian vaults\n- **Hands-on experience** with supervised fine-tuning (SFT)\n- **Understanding** of hyperparameters and how they affect training\n\n## ğŸ”¬ What is SFT?\n\n**SFT (Supervised Fine-Tuning)** is like teaching through examples:\n- You show the model examples of correct tool-calling behavior\n- The model learns to replicate those patterns\n- Use SFT when teaching a model a **new skill** (like using tools)\n\n**When to use SFT:**\n- âœ… Teaching tool-calling from scratch\n- âœ… Learning new task formats\n- âœ… Initial training with positive examples\n\n**Not for:**\n- âŒ Refining existing behavior (use KTO instead)\n- âŒ Teaching preferences between good/bad outputs (use preference learning)\n\n## ğŸ’» Hardware Requirements\n\n**Recommended GPU:**\n- 7B models: T4 (15GB VRAM) - âœ… **Free Colab tier works!**\n- 13B models: A100 (40GB VRAM) - Colab Pro\n- 70B models: A100 (80GB VRAM) - Colab Pro+\n\n**Training time:** ~45 minutes for a 7B model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies. This takes ~2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets==4.3.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to Google Drive so they persist if runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/SFT_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets:\n",
    "1. Click the ğŸ”‘ key icon in the left sidebar\n",
    "2. Add new secret: `HF_TOKEN`\n",
    "3. Get token from https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Get token from Colab secrets\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "# Get your HuggingFace username automatically\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"âœ“ HuggingFace token loaded\")\n",
    "print(f\"âœ“ Username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Configuration\n\n**What this does:** Choose the base model you want to fine-tune and configure basic settings.\n\nThink of this like choosing which \"brain\" you want to teach tool-calling skills to."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nBASE MODEL SELECTION - Choose Your Foundation Model\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nAll models below are 4-bit quantized to work on limited VRAM.\nLarger models = better quality but slower training and more memory needed.\n\nğŸ“š Browse all models: https://huggingface.co/unsloth\nğŸ“– Documentation: https://docs.unsloth.ai/get-started/all-our-models\n\"\"\"\n\n# ğŸ¯ MODEL_NAME: The base model to fine-tune\n#    What it is: The starting \"brain\" before we teach it tool-calling\n#\nMODEL_NAME = \"unsloth/mistral-7b-v0.3-bnb-4bit\"  # â† Default: Recommended for free Colab!\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ”¬ TINY MODELS (0.5B-1B) - Ultra-fast experimentation\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Perfect for: Rapid prototyping, testing pipelines, minimal hardware\n# VRAM: ~2-4 GB | Training time: ~10 minutes | Quality: Basic\n\n# Qwen 2.5 - Smallest viable models\n# MODEL_NAME = \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\"        # 0.5B: Ultra-tiny\n# MODEL_NAME = \"unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\"        # 1.5B: Better quality\n\n# Llama 3.2 - Meta's smallest\n# MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"        # 1B: Minimal Llama\n\n# DeepSeek R1 Distilled - Reasoning capability in tiny package\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"  # Distilled reasoning\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“¦ SMALL MODELS (3B-4B) - Fast iteration, free Colab compatible\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Perfect for: Quick experiments, limited hardware, fast training (~20-30 min)\n# VRAM: ~6-8 GB | Training time: ~25 minutes | Quality: Good for simple tasks\n\n# Llama 3.2 (Meta) - Latest small Llama, good reasoning\n# MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"        # 3B: Excellent instruction following\n# MODEL_NAME = \"unsloth/Llama-3.2-3B-bnb-4bit\"                 # 3B base (not instruct-tuned)\n\n# Qwen 2.5 (Alibaba) - Strong coding & reasoning\n# MODEL_NAME = \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\"          # 3B: Best-in-class small model\n# MODEL_NAME = \"unsloth/Qwen2.5-3B-bnb-4bit\"                   # 3B base\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit\"    # 3B: Optimized for code\n\n# Phi-3 (Microsoft) - Efficient small models\n# MODEL_NAME = \"unsloth/Phi-3.5-mini-instruct-bnb-4bit\"        # 3.8B: Microsoft's efficient model\n# MODEL_NAME = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"       # 3.8B: Original Phi-3\n\n# Gemma 2 (Google) - Memory efficient architecture\n# MODEL_NAME = \"unsloth/gemma-2-2b-it-bnb-4bit\"                # 2B: Very fast, decent quality\n# MODEL_NAME = \"unsloth/gemma-2-2b-bnb-4bit\"                   # 2B base\n\n# DeepSeek R1 Distilled - Reasoning in small package\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Qwen-3B-bnb-4bit\"  # 3B: Distilled reasoning\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸš€ MEDIUM MODELS (7B-9B) - Best balance, production ready\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Perfect for: Production use, good quality, free Colab compatible (~45 min)\n# VRAM: ~9-11 GB | Training time: ~45 minutes | Quality: Excellent â­ RECOMMENDED\n\n# Mistral 7B v0.3 (Mistral AI) - Industry standard, reliable\n# MODEL_NAME = \"unsloth/mistral-7b-v0.3-bnb-4bit\"              # â­ RECOMMENDED: Best all-around 7B\n# MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"     # Pre-instruct tuned version\n# MODEL_NAME = \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\"   # 12B: Nemo variant (larger context)\n# MODEL_NAME = \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\"       # 12B base\n\n# Llama 3.1 (Meta) - Latest from Meta, strong performance\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"   # 8B: Meta's flagship\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"            # 8B base (not instruct-tuned)\n\n# Llama 3.3 (Meta) - Newest Llama family\n# MODEL_NAME = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"       # 70B: Requires A100 80GB\n\n# Llama 3 (Meta) - Proven stable release\n# MODEL_NAME = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"          # 8B: Proven and stable\n\n# Qwen 2.5 (Alibaba) - Top-tier reasoning and coding\n# MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"          # 7B: Excellent all-around\n# MODEL_NAME = \"unsloth/Qwen2.5-7B-bnb-4bit\"                   # 7B base\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"    # 7B: Specialized for coding\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-7B-bnb-4bit\"             # 7B code base\n\n# Gemma 2 (Google) - Memory efficient\n# MODEL_NAME = \"unsloth/gemma-2-9b-it-bnb-4bit\"                # 9B: Memory-efficient architecture\n# MODEL_NAME = \"unsloth/gemma-2-9b-bnb-4bit\"                   # 9B base\n\n# DeepSeek R1 Distilled - Reasoning capability\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Qwen-7B-bnb-4bit\"  # 7B: Distilled reasoning\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\" # 8B: Llama-based reasoning\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ’ LARGE MODELS (13B-20B) - High quality, needs Colab Pro (A100 40GB)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Perfect for: Maximum quality, complex tasks, production deployment\n# VRAM: ~14-20 GB | Training time: ~90-120 minutes | Quality: Excellent\n\n# Llama 2 (Meta) - Proven, stable\n# MODEL_NAME = \"unsloth/llama-2-13b-bnb-4bit\"                  # 13B: Reliable base\n# MODEL_NAME = \"unsloth/llama-2-13b-chat-bnb-4bit\"             # 13B: Chat-optimized\n\n# Qwen 2.5 (Alibaba) - Best 14B model available\n# MODEL_NAME = \"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\"         # 14B: Top performance\n# MODEL_NAME = \"unsloth/Qwen2.5-14B-bnb-4bit\"                  # 14B base\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"   # 14B: Code specialist\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-14B-bnb-4bit\"            # 14B code base\n\n# CodeLlama (Meta) - Specialized for code generation\n# MODEL_NAME = \"unsloth/codellama-13b-bnb-4bit\"                # 13B: Code-focused\n\n# GPT-OSS (OpenAI open weights) - OpenAI architecture\n# MODEL_NAME = \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\"          # 20B: OpenAI's open model\n\n# DeepSeek R1 Distilled - Reasoning models\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\" # 14B: Strong reasoning\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ† EXTRA-LARGE MODELS (24B-32B) - Best quality, needs A100 80GB\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Perfect for: Maximum capability, specialized tasks, research\n# VRAM: ~20-30 GB | Training time: ~2-3 hours | Quality: State-of-the-art\n\n# Mistral Small (Mistral AI) - Latest from Mistral\n# MODEL_NAME = \"unsloth/Mistral-Small-24B-Instruct-2501-bnb-4bit\"  # 24B: Newest Mistral\n\n# Qwen 2.5 (Alibaba) - Massive models\n# MODEL_NAME = \"unsloth/Qwen2.5-32B-Instruct-bnb-4bit\"         # 32B: Powerhouse\n# MODEL_NAME = \"unsloth/Qwen2.5-32B-bnb-4bit\"                  # 32B base\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit\"   # 32B: Code specialist\n# MODEL_NAME = \"unsloth/Qwen2.5-Coder-32B-bnb-4bit\"            # 32B code base\n\n# Gemma 2 (Google) - Large variant\n# MODEL_NAME = \"unsloth/gemma-2-27b-it-bnb-4bit\"               # 27B: Largest Gemma\n# MODEL_NAME = \"unsloth/gemma-2-27b-bnb-4bit\"                  # 27B base\n\n# DeepSeek R1 Distilled - Large reasoning models\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit\" # 32B: Maximum reasoning\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸŒŸ MASSIVE MODELS (70B-405B) - Multi-GPU or highest-end hardware\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Perfect for: Research, benchmarking, specialized high-stakes applications\n# VRAM: ~40-80+ GB | Training time: ~4-8+ hours | Quality: Frontier\n\n# Llama 3.3 (Meta) - Latest flagship\n# MODEL_NAME = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"       # 70B: Latest Llama\n\n# Llama 3.1 (Meta) - Proven large models\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit\"  # 70B: Proven quality\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\"           # 70B base\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-405B-Instruct-bnb-4bit\" # 405B: MASSIVE (multi-GPU required)\n# MODEL_NAME = \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\"          # 405B base\n\n# Llama 3 (Meta) - Previous generation large\n# MODEL_NAME = \"unsloth/llama-3-70b-Instruct-bnb-4bit\"         # 70B: Proven stable\n\n# Llama 2 (Meta) - Earlier generation\n# MODEL_NAME = \"unsloth/llama-2-70b-bnb-4bit\"                  # 70B: Older but reliable\n# MODEL_NAME = \"unsloth/llama-2-70b-chat-bnb-4bit\"             # 70B chat\n\n# Qwen 2.5 (Alibaba) - Massive reasoning/coding\n# MODEL_NAME = \"unsloth/Qwen2.5-72B-Instruct-bnb-4bit\"         # 72B: Excellent quality\n# MODEL_NAME = \"unsloth/Qwen2.5-72B-bnb-4bit\"                  # 72B base\n\n# CodeLlama (Meta) - Large code models\n# MODEL_NAME = \"unsloth/codellama-34b-bnb-4bit\"                # 34B: Strong coding\n# MODEL_NAME = \"unsloth/codellama-70b-bnb-4bit\"                # 70B: Maximum code capability\n\n# DeepSeek V3 & R1 - Frontier reasoning models\n# MODEL_NAME = \"unsloth/DeepSeek-V3-bnb-4bit\"                  # 671B: Frontier model (multi-GPU)\n# MODEL_NAME = \"unsloth/DeepSeek-R1-bnb-4bit\"                  # 671B: Reasoning specialist\n# MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit\" # 70B: Distilled reasoning\n\n# Qwen QwQ (Reasoning specialist)\n# MODEL_NAME = \"unsloth/QwQ-32B-Preview-bnb-4bit\"              # 32B: Reasoning-focused\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ¨ SPECIALIZED MODELS - Vision, Medical, Reasoning\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Vision Models (Multimodal)\n# MODEL_NAME = \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\"         # 2B: Vision + language\n# MODEL_NAME = \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\"         # 7B: Vision + language\n# MODEL_NAME = \"unsloth/Pixtral-12B-2409-bnb-4bit\"             # 12B: Mistral's vision model\n\n# Medical Models (Healthcare specialist)\n# MODEL_NAME = \"unsloth/gemma-2-9b-medgemma-bnb-4bit\"          # 9B: Medical Gemma\n\n# Reasoning Models (Advanced problem-solving)\n# MODEL_NAME = \"unsloth/Phi-4-reasoning-bnb-4bit\"              # 14B: Microsoft's reasoning\n# MODEL_NAME = \"unsloth/Phi-4-reasoning-plus-bnb-4bit\"         # 14B+: Enhanced reasoning\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ”§ OTHER MODEL FAMILIES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Phi-4 (Microsoft) - Latest efficient models\n# MODEL_NAME = \"unsloth/Phi-4-bnb-4bit\"                        # 14B: Microsoft's latest\n\n# Phi-3 (Microsoft) - Efficient architecture\n# MODEL_NAME = \"unsloth/Phi-3-medium-4k-instruct-bnb-4bit\"     # 14B: Medium variant\n\n# Mixtral (Mistral AI) - Mixture of Experts\n# MODEL_NAME = \"unsloth/Mixtral-8x7B-Instruct-v0.1-bnb-4bit\"   # 47B params, 13B active\n\n# Gemma 1 (Google) - Earlier generation\n# MODEL_NAME = \"unsloth/gemma-7b-it-bnb-4bit\"                  # 7B: Gemma 1\n# MODEL_NAME = \"unsloth/gemma-7b-bnb-4bit\"                     # 7B base\n\n# GLM-4 (Zhipu AI)\n# MODEL_NAME = \"unsloth/glm-4-9b-chat-bnb-4bit\"                # 9B: Chinese-English bilingual\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“ MAX_SEQ_LENGTH: How much context the model can \"see\"\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ MAX_SEQ_LENGTH: Maximum conversation length in tokens\n#    What it is: How much text the model can process at once\n#    Trade-offs:\n#      - 1024: Minimal memory, fastest training, good for short interactions\n#      - 2048: Balanced, handles most conversations, recommended â­\n#      - 4096: Long context, uses more VRAM, slower training\n#      - 8192+: Very long context, needs significant VRAM\n#\nMAX_SEQ_LENGTH = 2048  # â† Recommended: Works for 95% of tool-calling examples\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“Š DATASET CONFIGURATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ DATASET: Pre-built training data for tool-calling\n#    What it is: ~5,500 high-quality examples of correct tool usage\n#    Format: User request â†’ Tool call â†’ Result â†’ Assistant response\n#\nDATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\"\nDATASET_FILE = \"syngen_tools_sft_11.22.25.jsonl\"  # Latest version (11/22/25)\n\n# Note: You can upload your own dataset to HuggingFace and use it here!\n# Just change DATASET_NAME to \"your-username/your-dataset-name\"\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ·ï¸ OUTPUT MODEL NAME\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# ğŸ¯ OUTPUT_MODEL_NAME: What to call your trained model on HuggingFace\n#    Pick something descriptive that indicates:\n#    - What it does (e.g., \"tool-calling\", \"obsidian-assistant\")\n#    - Model size (e.g., \"7b\", \"mistral-7b\")\n#    - Version (e.g., \"v1\", \"2024-11\")\n#\nOUTPUT_MODEL_NAME = \"nexus-tools-sft-7b\"  # â† Customize this!\n\n# Good naming examples:\n# - \"obsidian-tools-mistral-7b\"\n# - \"claudesidian-assistant-v2\"\n# - \"tool-calling-llama-8b\"\n\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# âœ… Print Configuration Summary\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint(\"â•”\" + \"â•\" * 78 + \"â•—\")\nprint(\"â•‘\" + \" \" * 25 + \"TRAINING CONFIGURATION\" + \" \" * 31 + \"â•‘\")\nprint(\"â• \" + \"â•\" * 78 + \"â•£\")\nprint(f\"â•‘  Model:        {MODEL_NAME:<60} â•‘\")\nprint(f\"â•‘  Max Length:   {MAX_SEQ_LENGTH:<60} tokens â•‘\")\nprint(f\"â•‘  Dataset:      {DATASET_NAME}/{DATASET_FILE:<33} â•‘\")\nprint(f\"â•‘  Output:       {hf_user}/{OUTPUT_MODEL_NAME:<49} â•‘\")\nprint(\"â• \" + \"â•\" * 78 + \"â•£\")\nprint(\"â•‘  Outputs on HuggingFace:                                                 â•‘\")\nprint(f\"â•‘    â€¢ LoRA adapters:    {hf_user}/{OUTPUT_MODEL_NAME:<40} â•‘\")\nprint(f\"â•‘    â€¢ Merged 16-bit:    {hf_user}/{OUTPUT_MODEL_NAME}-merged{' ' * (33 - len(OUTPUT_MODEL_NAME))} â•‘\")\nprint(\"â•š\" + \"â•\" * 78 + \"â•\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Load Model and Tokenizer\n\n**What this does:** Downloads the base model and prepares it for training.\n\nThe model is the \"brain\" that will learn tool-calling. The tokenizer converts text into numbers the model can process. We use 4-bit quantization to fit large models into limited GPU memory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Available VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the base model and tokenizer from HuggingFace\n# This downloads the model weights (~7GB for 7B models)\n# \n# Parameters explained:\n#   model_name: Which model to download\n#   max_seq_length: Max tokens model can process at once\n#   dtype=None: Auto-detect best precision for your GPU\n#   load_in_4bit=True: Use 4-bit quantization to save memory\n#   token: Your HF token for accessing the model\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=MODEL_NAME,\n    max_seq_length=MAX_SEQ_LENGTH,\n    dtype=None,  # Auto-detect (usually bfloat16 or float16)\n    load_in_4bit=True,  # Reduces memory usage by ~75%\n    token=HF_TOKEN,\n)\n\nprint(\"âœ“ Model loaded successfully\")\nprint(f\"  Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Apply LoRA Adapters\n\n**What this does:** Add trainable \"adapter\" layers to the model instead of training the entire thing.\n\nThink of LoRA like teaching a new skill through muscle memory - we add small specialized layers that learn the new behavior, while keeping the main \"brain\" frozen. This is way faster and uses less memory than retraining everything."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nLORA CONFIGURATION - How the model learns\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThese parameters control how we add trainable \"adapter\" layers to the model.\n\"\"\"\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    \n    # ğŸ¯ r (rank): Size of the adapter matrices\n    #    What it controls: Learning capacity vs memory usage\n    #    Turn UP (64, 128): More capacity to learn complex patterns, uses more VRAM\n    #    Turn DOWN (8, 16): Faster, less memory, but might not capture all nuances\n    #    Sweet spot (32): Good balance for most tasks\n    r=32,\n    \n    # ğŸ¯ lora_alpha: Scaling factor for LoRA updates\n    #    What it controls: How much the adapters influence the model\n    #    Turn UP (128): Stronger influence from fine-tuning (be careful of overfitting!)\n    #    Turn DOWN (16): More conservative, preserves base model behavior\n    #    Rule of thumb: Usually set to 2x the rank (if r=32, use alpha=64)\n    lora_alpha=64,\n    \n    # ğŸ¯ lora_dropout: Regularization to prevent overfitting\n    #    What it controls: Randomly drops connections during training\n    #    Turn UP (0.1): More regularization, helps if model memorizes too much\n    #    Turn DOWN (0.0): No regularization, use if you have lots of diverse data\n    #    Sweet spot (0.05): Light regularization for most cases\n    lora_dropout=0.05,\n    \n    # ğŸ¯ bias: Whether to train bias parameters\n    #    Options: \"none\" (don't train), \"all\" (train all), \"lora_only\" (train LoRA biases)\n    #    \"none\" is standard and works well for most cases\n    bias=\"none\",\n    \n    # ğŸ¯ target_modules: Which parts of the model to add adapters to\n    #    What it controls: Where learning happens in the neural network\n    #    These are the \"attention\" and \"feed-forward\" layers\n    #    More modules = more capacity but slower training\n    #    This list covers all the key layers for language understanding\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    \n    # ğŸ¯ use_gradient_checkpointing: Memory optimization technique\n    #    What it controls: Trade computation for memory\n    #    \"unsloth\": Fast checkpointing (recommended)\n    #    False: Faster but uses more VRAM\n    use_gradient_checkpointing=\"unsloth\",\n    \n    # ğŸ¯ random_state: Random seed for reproducibility\n    #    What it controls: Ensures same initialization each run\n    #    Change this number to get different random starting points\n    random_state=3407,\n)\n\nprint(\"âœ“ LoRA adapters applied\")\nprint(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"  Percentage trainable: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Load and Prepare Dataset\n\n**What this does:** Downloads training examples and formats them for the model.\n\nThe dataset contains ~5,500 examples of correct tool-calling behavior. Think of it like a textbook full of solved problems that the model will learn from."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nLOAD DATASET FROM HUGGINGFACE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThis downloads pre-made training examples of correct tool-calling behavior.\nEach example shows: user request â†’ tool call â†’ result â†’ assistant response\n\"\"\"\n\nprint(f\"Loading dataset: {DATASET_NAME}\")\ndataset = load_dataset(\n    DATASET_NAME,  # HuggingFace repository containing the dataset\n    data_files=DATASET_FILE,  # Specific JSONL file to use\n    split=\"train\"  # Use the training split\n)\n\nprint(f\"âœ“ Loaded {len(dataset)} examples\")\nprint(f\"\\nSample example:\")\nprint(dataset[0])\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSET CHAT TEMPLATE (Model-Specific)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThe chat template defines how conversations are formatted for the model.\nDifferent models use different formats - we auto-detect and use the right one!\n\"\"\"\n\n# Mistral-specific template (uses [INST] format)\nMISTRAL_CHAT_TEMPLATE = \"\"\"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'system' %}{% if loop.index == 1 %}{{ message['content'] + ' ' }}{% endif %}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n\n# Generic fallback template for other models\nDEFAULT_CHAT_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|user|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ '<|system|>\\\\n' + message['content'] + eos_token }}{% elif message['role'] == 'assistant' %}{{ '<|assistant|>\\\\n'  + message['content'] + eos_token }}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}{% endfor %}\"\"\"\n\nif tokenizer.chat_template is None:\n    # Detect model type and use appropriate template\n    is_mistral = 'mistral' in MODEL_NAME.lower()\n    \n    if is_mistral:\n        print(\"\\nâœ“ Detected Mistral model - using [INST] format\")\n        tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE\n        print(\"   Format: <s>[INST] user [/INST] assistant</s>\")\n        print(\"   This is the official Mistral chat format!\")\n    else:\n        print(\"\\nâš ï¸  No chat template found, using default format\")\n        tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n        print(\"   Format: <|user|>\\\\ncontent</s>\")\nelse:\n    print(\"\\nâœ“ Tokenizer already has chat template\")\n    if 'mistral' in MODEL_NAME.lower():\n        print(\"   Using Mistral model - template should use [INST] format\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nFORMAT DATASET FOR TRAINING\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nConvert the conversation format into the exact text format the model expects.\nThis applies the chat template to each example.\n\"\"\"\n\ndef format_chat_template(example):\n    \"\"\"\n    Convert conversations to tokenizer's chat template.\n    \n    Input: {\"conversations\": [{\"role\": \"user\", \"content\": \"...\"}, ...]}\n    Output: {\"text\": \"<|im_start|>user\\n...<|im_end|>\\n...\"}\n    \"\"\"\n    text = tokenizer.apply_chat_template(\n        example[\"conversations\"],  # List of message dicts\n        tokenize=False,  # Return string, not token IDs\n        add_generation_prompt=False  # Don't add prompt for model to continue\n    )\n    return {\"text\": text}\n\n# Apply formatting to entire dataset\n# This creates a new \"text\" field with formatted conversations\ndataset = dataset.map(\n    format_chat_template,  # Function to apply\n    remove_columns=dataset.column_names,  # Remove original columns\n    desc=\"Formatting dataset\"  # Progress bar description\n)\n\nprint(\"âœ“ Dataset formatted for training\")\nprint(f\"\\nFormatted example (first 500 characters):\")\nprint(dataset[0][\"text\"][:500])\nprint(\"\\nğŸ’¡ The full formatted text will be used during training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Training Configuration\n\n**What this does:** Set the hyperparameters that control how the model learns.\n\nThis is the **most important section** - these settings determine how fast the model learns, how much memory it uses, and how good the final result will be. Think of it like configuring a study plan: how many hours per day, how many review sessions, how to handle difficult material, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTTrainer, SFTConfig\nfrom unsloth import is_bfloat16_supported\nfrom datetime import datetime\n\n# Create timestamped output directory\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_dir = f\"{DRIVE_OUTPUT_DIR}/{timestamp}\"\n\n\"\"\"\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTRAINING HYPERPARAMETERS - How the model learns\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nThese control the learning process. Getting these right is crucial!\n\"\"\"\n\ntraining_args = SFTConfig(\n    output_dir=output_dir,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # BATCH SIZE & MEMORY SETTINGS\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ per_device_train_batch_size: Examples processed at once\n    #    What it controls: How many training examples fit in GPU memory at once\n    #    Turn UP (4, 8): Faster training, but uses more VRAM (might run out of memory!)\n    #    Turn DOWN (1): Slower but uses less memory (use if you get OOM errors)\n    #    Sweet spot (2): Works on most free Colab GPUs\n    per_device_train_batch_size=2,\n    \n    # ğŸ¯ gradient_accumulation_steps: Simulate larger batches\n    #    What it controls: How many mini-batches before updating weights\n    #    Turn UP (8, 16): Simulates larger batch, more stable training\n    #    Turn DOWN (1, 2): Faster updates, but noisier learning\n    #    Effective batch = per_device_batch_size Ã— gradient_accumulation_steps\n    #    Example: 2 Ã— 4 = 8 effective batch size\n    gradient_accumulation_steps=4,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LEARNING RATE & OPTIMIZATION\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ learning_rate: How fast the model learns\n    #    What it controls: Size of weight updates during training\n    #    Turn UP (5e-4): Faster learning, but might overshoot optimal values\n    #    Turn DOWN (1e-5): More careful learning, but takes longer\n    #    Sweet spot (2e-4): Standard for SFT (100x higher than preference learning!)\n    #    NOTE: This is MUCH higher than KTO learning rates (2e-7)\n    learning_rate=2e-4,\n    \n    # ğŸ¯ max_grad_norm: Gradient clipping to prevent exploding gradients\n    #    What it controls: Maximum size of weight updates\n    #    Turn UP (2.0): Allows larger updates (might cause instability)\n    #    Turn DOWN (0.5): More conservative (slower learning)\n    #    Sweet spot (1.0): Prevents extreme updates while allowing learning\n    max_grad_norm=1.0,\n    \n    # ğŸ¯ lr_scheduler_type: How learning rate changes over time\n    #    What it controls: Learning rate adjustment strategy\n    #    \"cosine\": Starts high, smoothly decreases (recommended)\n    #    \"linear\": Decreases linearly\n    #    \"constant\": Never changes (simpler but less effective)\n    lr_scheduler_type=\"cosine\",\n    \n    # ğŸ¯ warmup_ratio: Fraction of training with increasing learning rate\n    #    What it controls: How long to \"warm up\" before full learning rate\n    #    Turn UP (0.2): Longer warmup, more stable early training\n    #    Turn DOWN (0.0): No warmup, jumps straight to full LR\n    #    Sweet spot (0.1): 10% of training spent warming up\n    warmup_ratio=0.1,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # TRAINING DURATION\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ num_train_epochs: How many times to go through the entire dataset\n    #    What it controls: Total training time and learning opportunity\n    #    Turn UP (5, 10): More learning, but risk of overfitting (memorization)\n    #    Turn DOWN (1): Faster but might not learn everything\n    #    Sweet spot (3): Standard for SFT - enough to learn without memorizing\n    #    NOTE: KTO only uses 1 epoch because it's for refinement, not initial learning\n    num_train_epochs=3,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # DATA HANDLING\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ max_seq_length: Maximum tokens per example (matches earlier config)\n    max_seq_length=MAX_SEQ_LENGTH,\n    \n    # ğŸ¯ packing: Whether to combine short examples into one sequence\n    #    What it controls: Efficiency of sequence processing\n    #    True: Packs multiple short examples together (more efficient)\n    #    False: Each example is separate (simpler, safer)\n    #    Set to False for tool-calling to keep examples independent\n    packing=False,\n    \n    # ğŸ¯ dataset_text_field: Which field in dataset contains the text\n    #    This tells the trainer where to find the formatted conversations\n    dataset_text_field=\"text\",\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # PRECISION & OPTIMIZATION\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ fp16 / bf16: Mixed precision training (speed + memory optimization)\n    #    What it controls: Number precision during training\n    #    bf16 (bfloat16): Better for newer GPUs (A100, T4), more stable\n    #    fp16 (float16): For older GPUs, slightly less stable\n    #    Auto-detect: Uses bf16 if supported, fp16 otherwise\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    \n    # ğŸ¯ optim: Optimizer algorithm\n    #    What it controls: How weight updates are calculated\n    #    \"adamw_8bit\": Memory-efficient version of AdamW (recommended)\n    #    \"adamw_torch\": Standard AdamW (uses more memory)\n    #    \"sgd\": Simpler but less effective for LLMs\n    optim=\"adamw_8bit\",\n    \n    # ğŸ¯ gradient_checkpointing: Save memory by recomputing during backward pass\n    #    What it controls: Memory vs speed tradeoff\n    #    True: Uses less memory (~30% reduction), slightly slower\n    #    False: Faster but uses more memory\n    gradient_checkpointing=True,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LOGGING & CHECKPOINTING\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ logging_steps: How often to print training progress\n    #    What it controls: Frequency of console updates\n    #    Turn UP (10, 20): Less frequent updates, cleaner logs\n    #    Turn DOWN (1): See every step (very verbose!)\n    #    Sweet spot (5): Regular updates without spam\n    logging_steps=5,\n    \n    # ğŸ¯ save_steps: How often to save checkpoint\n    #    What it controls: Backup frequency during training\n    #    Turn UP (200, 500): Less frequent saves, saves disk space\n    #    Turn DOWN (50): More backups, can resume from more points\n    #    Sweet spot (100): Good balance for ~45min training\n    save_steps=100,\n    \n    # ğŸ¯ save_total_limit: Maximum number of checkpoints to keep\n    #    What it controls: Disk space used for checkpoints\n    #    Turn UP (5, 10): Keep more history, uses more space\n    #    Turn DOWN (1, 2): Only keep latest, saves space\n    #    Sweet spot (3): Last 3 checkpoints (in case latest has issues)\n    save_total_limit=3,\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # REPRODUCIBILITY\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    \n    # ğŸ¯ seed: Random seed for reproducible results\n    #    What it controls: Ensures same results across runs\n    #    Change this to get different random variations\n    seed=42,\n    \n    # ğŸ¯ report_to: Where to log metrics (wandb, tensorboard, etc.)\n    #    \"none\": Don't upload metrics anywhere\n    #    \"wandb\": Log to Weights & Biases (requires setup)\n    #    \"tensorboard\": Log locally\n    report_to=\"none\",\n)\n\n# ============================================================================\n# Print configuration summary\n# ============================================================================\nprint(\"âœ“ Training configuration ready\")\nprint(f\"  Output directory: {output_dir}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Precision: {'bf16' if is_bfloat16_supported() else 'fp16'}\")\nprint(f\"\\nğŸ’¡ Training will take approximately 45 minutes for a 7B model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Initialize Trainer\n\n**What this does:** Creates the training engine that coordinates everything.\n\nThe SFTTrainer is the orchestrator - it takes the model, dataset, and configuration, then handles all the training mechanics (gradient updates, checkpointing, logging, etc.)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the SFTTrainer\n# This combines the model, dataset, and configuration into one training pipeline\n\ntrainer = SFTTrainer(\n    model=model,  # The model with LoRA adapters\n    tokenizer=tokenizer,  # For converting text to tokens\n    train_dataset=dataset,  # Our formatted training examples\n    args=training_args,  # All the hyperparameters we configured\n)\n\nprint(\"âœ“ Trainer initialized\")\nprint(\"  Ready to start training!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Train!\n\n**What this does:** The actual learning happens here!\n\nThe model will:\n1. **Read examples** from the dataset\n2. **Predict** what the response should be\n3. **Compare** its prediction to the correct answer\n4. **Update weights** to get closer to the correct answer\n5. **Repeat** this process for 3 epochs (3 full passes through the data)\n\n**What to expect:**\n- Training takes ~45 minutes for 7B models on T4 GPU\n- You'll see progress updates every 5 steps\n- Loss should generally decrease over time (learning is working!)\n- Checkpoints are saved every 100 steps to Google Drive\n\n**What the metrics mean:**\n- **Loss:** How \"wrong\" the model is (lower = better, aim for <1.0)\n- **Learning Rate:** Gradually decreases as training progresses\n- **Samples/sec:** Training speed (depends on GPU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU memory\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "print()\n",
    "\n",
    "# Start training\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Upload to HuggingFace\n\n**What this does:** Share your trained model with the world!\n\nWe'll create **three versions** of your model:\n\n1. **LoRA adapters** (~320MB) - Small files that contain just the changes\n   - Fast to download\n   - Need to be combined with base model to use\n   \n2. **Merged 16-bit model** (~14GB) - Full model with adapters merged in\n   - High quality, no precision loss\n   - Large file size\n   - Best for local deployment (Ollama, LM Studio)\n   \n3. **GGUF quantizations** - Optimized versions for CPU/GPU inference\n   - Q4_K_M (~3.5GB) - Recommended for most users\n   - Q5_K_M (~4.5GB) - Better quality, larger size\n   - Q8_0 (~7GB) - Nearly full quality\n   - These work directly with Ollama and LM Studio!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload LoRA adapters\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LoRA adapters uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload merged 16-bit model\n",
    "print(\"Merging LoRA weights into base model (16-bit)...\")\n",
    "print(\"This will take ~5 minutes...\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Merged model uploaded to HuggingFace\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GGUF quantizations\n",
    "quantization_methods = [\"q4_k_m\", \"q5_k_m\", \"q8_0\"]\n",
    "\n",
    "print(\"Creating GGUF quantizations...\")\n",
    "print(f\"This will create {len(quantization_methods)} versions\")\n",
    "print()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_methods,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"âœ“ GGUF quantizations created and uploaded!\")\n",
    "print(f\"  View at: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ‰ Done!\n\nYour model has been trained and uploaded to HuggingFace!\n\n## ğŸ“Š What You Accomplished\n\nâœ… **Fine-tuned a 7B parameter language model** to use the Claudesidian toolset  \nâœ… **Learned about SFT hyperparameters** and how they affect training  \nâœ… **Created multiple model formats** (LoRA, merged, GGUF) for different use cases  \nâœ… **Published your model** to HuggingFace for others to use  \n\n## ğŸš€ Next Steps\n\n### 1. Test Your Model\n\n**Using LM Studio:**\n1. Open LM Studio â†’ \"Discover\" tab\n2. Search for your username: `{hf_user}`\n3. Download the Q4_K_M or Q5_K_M GGUF version\n4. Load and test with tool-calling prompts!\n\n**Using Ollama:**\n```bash\n# Download your model\nollama pull {hf_user}/{OUTPUT_MODEL_NAME}\n\n# Test it\nollama run {hf_user}/{OUTPUT_MODEL_NAME}\n```\n\n### 2. Evaluate Quality\n\nRun the Evaluator to test tool-calling accuracy:\n```bash\npython -m Evaluator.cli \\\n  --model {OUTPUT_MODEL_NAME} \\\n  --prompt-set Evaluator/prompts/full_coverage.json \\\n  --output results.json\n```\n\n### 3. Refine Further (Optional)\n\nIf you want to improve your model:\n- **Collect more data**: Add examples where your model struggles\n- **Try KTO training**: Refine behavior by showing good vs bad examples\n- **Adjust hyperparameters**: Experiment with learning rate, batch size, etc.\n\n## ğŸ“ Learn More\n\n- **KTO Training**: For preference learning (good vs bad outputs)\n- **Dataset Creation**: Build custom training data for your use case\n- **Model Evaluation**: Systematic testing of tool-calling accuracy\n\n## ğŸ’¡ Tips for Better Models\n\n1. **More data is better**: 5,000+ examples produce robust models\n2. **Quality over quantity**: Clean, accurate examples matter more than volume\n3. **Test thoroughly**: Use the Evaluator to catch issues early\n4. **Iterate**: Fine-tuning is iterative - train, test, improve, repeat\n\n---\n\n**Questions or issues?** Check the documentation or open an issue on GitHub!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}