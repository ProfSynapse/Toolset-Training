{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSPO Training for Tool-Calling\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/Trainers/notebooks/gspo_tool_calling_notebook.ipynb)\n",
    "\n",
    "## What is GSPO?\n",
    "\n",
    "**GSPO (Group Sequence Policy Optimization)** is a reinforcement learning technique developed by the Qwen team. Unlike SFT (which learns from examples) or KTO (which learns preferences), GSPO:\n",
    "\n",
    "- **Generates completions** during training (online RL)\n",
    "- **Scores outputs** using custom reward functions\n",
    "- **Optimizes at sequence level** (not token level like GRPO)\n",
    "\n",
    "## Why GSPO for Tool-Calling?\n",
    "\n",
    "| Method | Best For | Tool-Calling Use Case |\n",
    "|--------|----------|----------------------|\n",
    "| **SFT** | Teaching new skills | Initial tool syntax learning |\n",
    "| **KTO** | Refining preferences | Good vs bad tool choices |\n",
    "| **GSPO** | Reward-guided optimization | Complex tool selection & argument quality |\n",
    "\n",
    "GSPO shines when you have **complex evaluation criteria** that can be expressed as reward functions:\n",
    "- Did the model select the RIGHT tool?\n",
    "- Are the arguments structurally correct?\n",
    "- Is the context object complete?\n",
    "- Do argument values make semantic sense?\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "- **7B models**: T4 (15GB) - Free Colab works!\n",
    "- **13B+ models**: A100 recommended\n",
    "- **Training time**: ~60-90 minutes for 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Install Unsloth and dependencies (~2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth for faster training\n",
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install training dependencies\n",
    "%%capture\n",
    "!pip install -U \"transformers>=4.45.0\"\n",
    "!pip install \"datasets==4.3.0\"\n",
    "!pip install -U accelerate bitsandbytes\n",
    "!pip install -U trl peft xformers triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "Save checkpoints to persist across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/GSPO_Training\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoints will be saved to: {DRIVE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Credentials\n",
    "\n",
    "Add your HF token to Colab secrets (key icon in sidebar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "api = HfApi()\n",
    "hf_user = api.whoami(token=HF_TOKEN)[\"name\"]\n",
    "\n",
    "print(f\"HuggingFace username: {hf_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model & Dataset Configuration\n",
    "\n",
    "# @markdown ### Base Model\n",
    "MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\" # @param [\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", \"unsloth/Qwen2.5-3B-Instruct-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"]\n",
    "\n",
    "# @markdown ### Sequence Length\n",
    "MAX_SEQ_LENGTH = 2048 # @param [1024, 2048, 4096] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### Dataset\n",
    "DATASET_NAME = \"professorsynapse/claudesidian-synthetic-dataset\" # @param {type:\"string\"}\n",
    "DATASET_FILE = \"gspo_datasets/syngen_tools_sft_11.26.25_gspo.jsonl\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown ### Output Model Name\n",
    "OUTPUT_MODEL_NAME = \"nexus-tools-gspo-7b\" # @param {type:\"string\"}\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME}/{DATASET_FILE}\")\n",
    "print(f\"Output: {OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print()\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title LoRA Configuration\n",
    "\n",
    "r = 32 # @param [8, 16, 32, 64] {type:\"raw\"}\n",
    "lora_alpha = r * 2\n",
    "lora_dropout = 0.05 # @param {type:\"number\"}\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"LoRA applied: {trainable:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load GSPO Dataset\n",
    "\n",
    "The GSPO dataset format:\n",
    "```json\n",
    "{\n",
    "  \"prompt\": [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}],\n",
    "  \"ground_truth_tool\": \"toolName\",\n",
    "  \"ground_truth_args\": {...}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    data_files=DATASET_FILE,\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "print(f\"\\nSample prompt:\")\n",
    "print(dataset[0][\"prompt\"])\n",
    "print(f\"\\nGround truth tool: {dataset[0]['ground_truth_tool']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Reward Functions\n",
    "\n",
    "These functions evaluate the model's generated completions and return rewards.\n",
    "\n",
    "**Reward breakdown:**\n",
    "- `tool_selection_reward`: +1.0 if correct tool selected\n",
    "- `json_structure_reward`: +0.3 if valid JSON in tool call\n",
    "- `context_completeness_reward`: +0.5 if all 7 context fields present\n",
    "- `format_reward`: +0.2 if output follows expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Required context fields\n",
    "CONTEXT_FIELDS = [\n",
    "    \"sessionId\", \"workspaceId\", \"sessionDescription\",\n",
    "    \"sessionMemory\", \"toolContext\", \"primaryGoal\", \"subgoal\"\n",
    "]\n",
    "\n",
    "\n",
    "def extract_tool_call(text: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Extract tool name and arguments from model output.\n",
    "    \n",
    "    Handles multiple formats:\n",
    "    - OpenAI function calling format\n",
    "    - Text-based tool_call: format\n",
    "    - JSON tool_calls array\n",
    "    \"\"\"\n",
    "    tool_name = None\n",
    "    tool_args = None\n",
    "    \n",
    "    # Try to find function call pattern\n",
    "    # Pattern 1: tool_call: toolName\\narguments: {...}\n",
    "    tc_match = re.search(r'tool_call:\\s*(\\w+)\\s*\\narguments:\\s*({[^}]+})', text, re.DOTALL)\n",
    "    if tc_match:\n",
    "        tool_name = tc_match.group(1)\n",
    "        try:\n",
    "            tool_args = json.loads(tc_match.group(2))\n",
    "        except:\n",
    "            tool_args = {}\n",
    "        return tool_name, tool_args\n",
    "    \n",
    "    # Pattern 2: {\"name\": \"toolName\", \"arguments\": ...}\n",
    "    fn_match = re.search(r'\"name\"\\s*:\\s*\"([^\"]+)\".*?\"arguments\"\\s*:\\s*\"?({[^}]+})\"?', text, re.DOTALL)\n",
    "    if fn_match:\n",
    "        tool_name = fn_match.group(1)\n",
    "        try:\n",
    "            args_str = fn_match.group(2)\n",
    "            # Handle escaped JSON\n",
    "            if '\\\\\"' in args_str:\n",
    "                args_str = args_str.replace('\\\\\"', '\"')\n",
    "            tool_args = json.loads(args_str)\n",
    "        except:\n",
    "            tool_args = {}\n",
    "        return tool_name, tool_args\n",
    "    \n",
    "    # Pattern 3: Just look for any tool name pattern\n",
    "    name_match = re.search(r'(\\w+Manager_\\w+|\\w+Librarian_\\w+)', text)\n",
    "    if name_match:\n",
    "        tool_name = name_match.group(1)\n",
    "        # Try to find JSON after it\n",
    "        json_match = re.search(r'{[^{}]*\"context\"[^{}]*}', text)\n",
    "        if json_match:\n",
    "            try:\n",
    "                tool_args = json.loads(json_match.group(0))\n",
    "            except:\n",
    "                tool_args = {}\n",
    "    \n",
    "    return tool_name, tool_args\n",
    "\n",
    "\n",
    "def tool_selection_reward(completions, ground_truth_tool, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for selecting the correct tool.\n",
    "    +1.0 if exact match, +0.3 if same agent family, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        tool_name, _ = extract_tool_call(text)\n",
    "        \n",
    "        if tool_name == ground_truth_tool:\n",
    "            rewards.append(1.0)\n",
    "        elif tool_name and ground_truth_tool:\n",
    "            # Partial credit for same agent family (e.g., vaultManager_*)\n",
    "            pred_agent = tool_name.split('_')[0] if '_' in tool_name else ''\n",
    "            true_agent = ground_truth_tool.split('_')[0] if '_' in ground_truth_tool else ''\n",
    "            if pred_agent == true_agent:\n",
    "                rewards.append(0.3)\n",
    "            else:\n",
    "                rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def json_structure_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for valid JSON structure in tool call.\n",
    "    +0.3 if valid JSON with arguments, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        _, tool_args = extract_tool_call(text)\n",
    "        \n",
    "        if tool_args and isinstance(tool_args, dict) and len(tool_args) > 0:\n",
    "            rewards.append(0.3)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def context_completeness_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for complete context object.\n",
    "    Up to +0.5 based on how many of the 7 required fields are present.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        _, tool_args = extract_tool_call(text)\n",
    "        \n",
    "        if not tool_args or \"context\" not in tool_args:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        context = tool_args.get(\"context\", {})\n",
    "        if not isinstance(context, dict):\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Count present fields\n",
    "        present = sum(1 for field in CONTEXT_FIELDS if field in context and context[field])\n",
    "        reward = (present / len(CONTEXT_FIELDS)) * 0.5\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for proper output format.\n",
    "    +0.2 if output contains tool call structure, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        \n",
    "        # Check for tool call indicators\n",
    "        has_tool_call = (\n",
    "            'tool_call' in text.lower() or\n",
    "            'function' in text.lower() or\n",
    "            'arguments' in text.lower() or\n",
    "            re.search(r'\\w+Manager_\\w+|\\w+Librarian_\\w+', text)\n",
    "        )\n",
    "        \n",
    "        rewards.append(0.2 if has_tool_call else 0.0)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Combine all reward functions\n",
    "REWARD_FUNCTIONS = [\n",
    "    tool_selection_reward,\n",
    "    json_structure_reward,\n",
    "    context_completeness_reward,\n",
    "    format_reward,\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(REWARD_FUNCTIONS)} reward functions\")\n",
    "print(\"  - tool_selection_reward (max +1.0)\")\n",
    "print(\"  - json_structure_reward (max +0.3)\")\n",
    "print(\"  - context_completeness_reward (max +0.5)\")\n",
    "print(\"  - format_reward (max +0.2)\")\n",
    "print(f\"  Total max reward: 2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure GSPO Training\n",
    "\n",
    "GSPO uses `GRPOTrainer` with `importance_sampling_level=\"sequence\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{DRIVE_OUTPUT_DIR}/{timestamp}\"\n",
    "\n",
    "# @title GSPO Hyperparameters\n",
    "\n",
    "# @markdown ### Performance\n",
    "per_device_train_batch_size = 1 # @param [1, 2, 4] {type:\"raw\"}\n",
    "gradient_accumulation_steps = 8 # @param [4, 8, 16] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### Generation\n",
    "num_generations = 4 # @param [2, 4, 8] {type:\"raw\"}\n",
    "max_new_tokens = 512 # @param [256, 512, 1024] {type:\"raw\"}\n",
    "\n",
    "# @markdown ### Learning\n",
    "learning_rate_exponent = 6 # @param [5, 6, 7] {type:\"raw\"}\n",
    "learning_rate_multiplier = 5 # @param [1, 2, 3, 4, 5] {type:\"raw\"}\n",
    "learning_rate = learning_rate_multiplier * (10 ** -learning_rate_exponent)\n",
    "\n",
    "num_train_epochs = 1 # @param {type:\"integer\"}\n",
    "\n",
    "# @markdown ### GSPO-Specific\n",
    "# @markdown epsilon controls the clipping range for policy updates\n",
    "epsilon = 0.0003 # @param {type:\"number\"}\n",
    "epsilon_high = 0.0004 # @param {type:\"number\"}\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    \n",
    "    # Generation settings\n",
    "    num_generations=num_generations,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    \n",
    "    # GSPO: sequence-level importance sampling\n",
    "    importance_sampling_level=\"sequence\",\n",
    "    \n",
    "    # Learning settings\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    \n",
    "    # Precision\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"adamw_8bit\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"GSPO Configuration:\")\n",
    "print(f\"  Batch: {per_device_train_batch_size} x {gradient_accumulation_steps} = {per_device_train_batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"  Generations per prompt: {num_generations}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Importance sampling: sequence (GSPO)\")\n",
    "print(f\"  Output: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prepare Dataset for GRPO\n",
    "\n",
    "Format the dataset so GRPOTrainer can use the ground truth for rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral chat template\n",
    "MISTRAL_CHAT_TEMPLATE = \"\"\"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'system' %}{% if loop.index == 1 %}{{ message['content'] + ' ' }}{% endif %}{% elif message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    if 'mistral' in MODEL_NAME.lower():\n",
    "        tokenizer.chat_template = MISTRAL_CHAT_TEMPLATE\n",
    "        print(\"Applied Mistral chat template\")\n",
    "\n",
    "\n",
    "def format_for_grpo(example):\n",
    "    \"\"\"\n",
    "    Format example for GRPOTrainer.\n",
    "    \n",
    "    GRPOTrainer expects:\n",
    "    - prompt: The formatted input text\n",
    "    - Additional fields for reward computation\n",
    "    \"\"\"\n",
    "    # Format prompt using chat template\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        example[\"prompt\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True  # Add prompt for model to continue\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt_text,\n",
    "        \"ground_truth_tool\": example[\"ground_truth_tool\"],\n",
    "        \"ground_truth_args\": json.dumps(example[\"ground_truth_args\"]) if example[\"ground_truth_args\"] else \"{}\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(\n",
    "    format_for_grpo,\n",
    "    remove_columns=[\"prompt\"],  # Remove original, keep ground_truth fields\n",
    "    desc=\"Formatting for GRPO\"\n",
    ")\n",
    "\n",
    "print(f\"Dataset formatted: {len(formatted_dataset)} examples\")\n",
    "print(f\"\\nSample formatted prompt (truncated):\")\n",
    "print(formatted_dataset[0][\"prompt\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create Custom Reward Function\n",
    "\n",
    "Combine all reward functions into one that GRPOTrainer can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_reward_function(completions, prompts, ground_truth_tool, **kwargs):\n",
    "    \"\"\"\n",
    "    Combined reward function for tool-calling evaluation.\n",
    "    \n",
    "    Returns total reward (max 2.0) for each completion.\n",
    "    \"\"\"\n",
    "    batch_size = len(completions)\n",
    "    total_rewards = [0.0] * batch_size\n",
    "    \n",
    "    # Get rewards from each function\n",
    "    tool_rewards = tool_selection_reward(completions, ground_truth_tool)\n",
    "    json_rewards = json_structure_reward(completions)\n",
    "    context_rewards = context_completeness_reward(completions)\n",
    "    format_rewards = format_reward(completions)\n",
    "    \n",
    "    # Combine rewards\n",
    "    for i in range(batch_size):\n",
    "        total_rewards[i] = (\n",
    "            tool_rewards[i] +\n",
    "            json_rewards[i] +\n",
    "            context_rewards[i] +\n",
    "            format_rewards[i]\n",
    "        )\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "\n",
    "# Test the reward function\n",
    "test_completion = [[{\"content\": 'tool_call: vaultManager_listDirectory\\narguments: {\"context\": {\"sessionId\": \"test\", \"workspaceId\": \"test\", \"sessionDescription\": \"test\", \"sessionMemory\": \"test\", \"toolContext\": \"test\", \"primaryGoal\": \"test\", \"subgoal\": \"test\"}, \"path\": \"/\"}'}]]\n",
    "test_reward = combined_reward_function(\n",
    "    test_completion,\n",
    "    prompts=[\"test\"],\n",
    "    ground_truth_tool=\"vaultManager_listDirectory\"\n",
    ")\n",
    "print(f\"Test reward (perfect completion): {test_reward[0]:.2f} / 2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=combined_reward_function,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    ")\n",
    "\n",
    "print(\"GSPO Trainer initialized!\")\n",
    "print(f\"  Using sequence-level importance sampling (GSPO)\")\n",
    "print(f\"  {num_generations} generations per prompt\")\n",
    "print(f\"  Reward function: combined_reward_function (max 2.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train!\n",
    "\n",
    "GSPO training will:\n",
    "1. Generate multiple completions for each prompt\n",
    "2. Score them using the reward functions\n",
    "3. Update the model to favor higher-reward completions\n",
    "\n",
    "**Expected metrics:**\n",
    "- **reward/mean**: Average reward across generations (should increase)\n",
    "- **reward/std**: Reward variance (should decrease as model converges)\n",
    "- **loss**: Policy loss (can fluctuate, watch reward instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Memory check\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Memory reserved: {start_gpu_memory} GB / {gpu_stats.total_memory / 1024**3:.1f} GB\")\n",
    "print()\n",
    "\n",
    "# Check for checkpoints\n",
    "import glob\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "resume_from = max(checkpoint_dirs, key=lambda x: int(x.split(\"-\")[-1])) if checkpoint_dirs else None\n",
    "\n",
    "if resume_from:\n",
    "    print(f\"Resuming from: {resume_from}\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING GSPO TRAINING\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=resume_from)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save & Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "print(f\"Saved to: {output_dir}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload LoRA adapters to HuggingFace\n",
    "model.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "print(f\"Uploaded to: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload merged model + GGUF quantizations\n",
    "print(\"Creating merged model and GGUF quantizations...\")\n",
    "print(\"This takes ~10 minutes\")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=HF_TOKEN,\n",
    "    private=False\n",
    ")\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    f\"{hf_user}/{OUTPUT_MODEL_NAME}\",\n",
    "    tokenizer,\n",
    "    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\"],\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"\\nAll models uploaded!\")\n",
    "print(f\"  LoRA: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}\")\n",
    "print(f\"  Merged: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME}-merged\")\n",
    "print(f\"  GGUF: https://huggingface.co/{hf_user}/{OUTPUT_MODEL_NAME} (Files tab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Your GSPO-trained model is ready!\n",
    "\n",
    "### What You Accomplished\n",
    "\n",
    "- Trained a model using **GSPO reinforcement learning**\n",
    "- Used **custom reward functions** for tool-calling evaluation\n",
    "- Created **multiple model formats** (LoRA, merged, GGUF)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Test** with LM Studio or Ollama\n",
    "2. **Compare** GSPO vs SFT performance on tool selection accuracy\n",
    "3. **Iterate** on reward functions to improve specific behaviors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
