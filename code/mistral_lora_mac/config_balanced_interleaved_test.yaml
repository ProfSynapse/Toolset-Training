# Test Configuration - clause_balanced_interleaved dataset
model:
  name: "mistralai/Mistral-7B-Instruct-v0.3"
  cache_dir: "~/.cache/huggingface"
  dtype: "float16"
  max_seq_length: 2048
  trust_remote_code: false

lora:
  rank: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
  bias: "none"

training:
  num_epochs: 1
  per_device_batch_size: 2
  gradient_accumulation_steps: 2
  learning_rate: 1.0e-4
  warmup_steps: 2
  max_grad_norm: 1.0
  weight_decay: 0.01
  save_steps: 2
  eval_steps: 2
  logging_steps: 1
  max_steps: 5
  seed: 42

data:
  dataset_path: "../syngen_toolset_v1.0.0_claude_balanced_interleaved.jsonl"
  train_split: 0.8
  shuffle: true
  seed: 42
  max_seq_length: 2048

output:
  checkpoint_dir: "checkpoints_balanced"
  final_model_dir: "outputs/final_model_balanced"
  logs_dir: "logs"
  metrics_dir: "outputs/metrics_balanced"
  keep_last_n_checkpoints: 3

logging:
  level: "INFO"
  console: true
  file: true
  json_logs: true
  log_file: "logs/training_balanced_interleaved.log"
  json_log_file: "logs/training_balanced_interleaved.jsonl"

evaluation:
  sample_prompts:
    - "Can you help me with my vault?"
    - "What tools are available?"
    - "How do I organize my notes?"
  max_new_tokens: 100
  temperature: 0.7
  top_p: 0.9
  do_sample: true

system:
  device: "metal"
  memory_limit_gb: 16
  num_workers: 0
