# Test Configuration for KTO Training
# Lightweight settings for quick validation

model:
  name: "mistralai/Mistral-7B-Instruct-v0.3"
  cache_dir: "~/.cache/huggingface"
  dtype: "float16"
  max_seq_length: 512  # Reduced from 4096 for faster processing
  trust_remote_code: false

lora:
  rank: 8  # Reduced from 64 for faster training
  alpha: 16  # 2 * rank
  dropout: 0.05
  target_modules:
    - "q_proj"  # Only attention query (minimal but effective)
    - "v_proj"  # Only attention value
  bias: "none"

training:
  num_epochs: 1  # Just 1 epoch for testing
  per_device_batch_size: 2  # Small batch for memory efficiency
  gradient_accumulation_steps: 2  # Effective batch = 4
  learning_rate: 5.0e-5  # Slightly higher for faster convergence in testing
  warmup_steps: 5  # Minimal warmup
  max_grad_norm: 1.0
  weight_decay: 0.01
  save_steps: 20  # Save frequently for testing
  eval_steps: 10  # Evaluate frequently
  logging_steps: 1  # Log every step to see progress
  max_steps: 50  # Very short run - just 50 steps
  seed: 42

kto:
  beta: 0.1  # KTO beta parameter
  lambda_d: 1.0  # Lambda for desirable samples
  lambda_u: 1.0  # Lambda for undesirable samples

data:
  dataset_path: "test_dataset_kto.jsonl"
  train_split: 0.9  # 90 train, 10 val from 100 examples
  shuffle: true
  seed: 42
  max_seq_length: 512  # Match model.max_seq_length

output:
  checkpoint_dir: "test_checkpoints"
  final_model_dir: "test_outputs/final_model"
  logs_dir: "test_logs"
  metrics_dir: "test_outputs/metrics"
  keep_last_n_checkpoints: 2  # Keep fewer checkpoints

logging:
  level: "INFO"
  console: true
  file: true
  json_logs: true
  log_file: "test_logs/training.log"
  json_log_file: "test_logs/training.jsonl"

evaluation:
  sample_prompts:
    - "Can you help me organize my notes?"
  max_new_tokens: 50  # Shorter for faster eval
  temperature: 0.7
  top_p: 0.9
  do_sample: true

system:
  device: "metal"
  memory_limit_gb: 16
  num_workers: 0
