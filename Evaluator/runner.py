"""Evaluation orchestration logic."""
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Sequence, Union

from .lmstudio_client import LMStudioClient
from .ollama_client import OllamaClient
from .prompt_sets import PromptCase
from .schema_validator import ValidationResult, validate_assistant_response


@dataclass
class EvaluationRecord:
    case: PromptCase
    response_text: Optional[str]
    validator: Optional[ValidationResult]
    latency_s: Optional[float]
    raw_response: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

    @property
    def passed(self) -> bool:
        return self.error is None and self.validator is not None and self.validator.passed


def evaluate_cases(
    cases: Sequence[PromptCase],
    client: Union[OllamaClient, LMStudioClient],
    dry_run: bool = False,
    on_record: Callable[[EvaluationRecord], None] | None = None,
) -> List[EvaluationRecord]:
    """Run evaluation for the provided prompts."""
    records: List[EvaluationRecord] = []
    for case in cases:
        if dry_run:
            records.append(
                record := EvaluationRecord(
                    case=case,
                    response_text=None,
                    validator=None,
                    latency_s=None,
                    raw_response=None,
                    error=None,
                )
            )
            if on_record:
                on_record(record)
            continue

        try:
            response = client.chat(case.chat_messages())
        except Exception as exc:  # pylint: disable=broad-exception-caught
            records.append(
                record := EvaluationRecord(
                    case=case,
                    response_text=None,
                    validator=None,
                    latency_s=None,
                    raw_response=None,
                    error=str(exc),
                )
            )
            if on_record:
                on_record(record)
            continue

        try:
            validator_result = validate_assistant_response(response.message)
        except Exception as exc:  # pylint: disable=broad-exception-caught
            # Validation failed - record the error but continue evaluation
            records.append(
                record := EvaluationRecord(
                    case=case,
                    response_text=response.message,
                    validator=None,
                    latency_s=response.latency_s,
                    raw_response=response.raw,
                    error=f"Validation error: {exc}",
                )
            )
            if on_record:
                on_record(record)
            continue

        # Check if expected tools were actually called
        if case.expected_tools:
            called_tool_names = {tc.name for tc in validator_result.tool_calls}
            missing_tools = set(case.expected_tools) - called_tool_names

            if missing_tools:
                from .schema_validator import ValidatorIssue
                validator_result.passed = False
                validator_result.issues.append(
                    ValidatorIssue(
                        level="error",
                        message=f"Expected tools not called: {', '.join(sorted(missing_tools))}"
                    )
                )

        records.append(
            record := EvaluationRecord(
                case=case,
                response_text=response.message,
                validator=validator_result,
                latency_s=response.latency_s,
                raw_response=response.raw,
                error=None,
            )
        )
        if on_record:
            on_record(record)
    return records
