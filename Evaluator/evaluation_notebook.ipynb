{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067a035d",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Edit the variables below. For Ollama, ensure the model has been pulled (e.g., `ollama pull <model>`). For LM Studio, ensure the model is loaded and API server is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backend: 'lmstudio' or 'ollama'\n",
    "BACKEND = 'lmstudio'\n",
    "# List of local model names to evaluate\n",
    "MODEL_NAMES = [\n",
    "    'your-model-name-here'  # replace with actual model id\n",
    "]\n",
    "# Prompt set path (relative to repo root)\n",
    "PROMPT_SET = 'Evaluator/prompts/baseline.json'\n",
    "# Optional: limit number of prompts for quick smoke tests (None = all)\n",
    "LIMIT = None\n",
    "# Sampling settings (tweak as needed)\n",
    "TEMPERATURE = 0.2\n",
    "TOP_P = 0.9\n",
    "MAX_TOKENS = 768\n",
    "SEED = None  # set an int for reproducibility\n",
    "# Output directory (results will auto-nest here)\n",
    "RESULTS_DIR = 'Evaluator/results'\n",
    "# Set to True to skip actual backend calls (structure only)\n",
    "DRY_RUN = False\n",
    "# Validate context IDs embedded in prompts (requires prompts with expected_context)\n",
    "VALIDATE_CONTEXT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f055584",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "Imports core evaluator modules. If an ImportError occurs, ensure you run this notebook from the project root or add the project root to `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, datetime, pathlib\n",
    "from Evaluator.prompt_sets import load_prompt_cases, filter_prompts\n",
    "from Evaluator.config import EvaluatorConfig, PromptFilter\n",
    "from Evaluator.client_factory import create_client, create_settings\n",
    "from Evaluator.runner import evaluate_cases\n",
    "from Evaluator.reporting import (\n",
    "    build_run_payload, write_json, render_markdown, console_summary, build_metadata\n",
    ")\n",
    "print('Imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db3012",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "Utilities for timestamped paths and performing a single model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73c35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    return datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def evaluate_model(model_name: str):\n",
    "    prompt_path = pathlib.Path(PROMPT_SET)\n",
    "    cases = load_prompt_cases(prompt_path)\n",
    "    f = PromptFilter(tags=None, limit=LIMIT)\n",
    "    selected = filter_prompts(cases, f)\n",
    "    if not selected:\n",
    "        raise ValueError('No prompts selected; adjust LIMIT or tags.')\n",
    "\n",
    "    # Build settings and client\n",
    "    settings = create_settings(\n",
    "        backend=BACKEND,\n",
    "        model=model_name,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    client = create_client(\n",
    "        backend=BACKEND,\n",
    "        settings=settings,\n",
    "        timeout=60.0,\n",
    "        retries=2,\n",
    "    )\n",
    "\n",
    "    print(f'Running {len(selected)} prompts for model: {model_name}')\n",
    "    records = evaluate_cases(\n",
    "        selected,\n",
    "        client=client,\n",
    "        dry_run=DRY_RUN,\n",
    "        validate_context=VALIDATE_CONTEXT,\n",
    "        on_record=lambda r: print(f'  {r.case.case_id or \n",
    "} -> {\n",
    " if r.passed else \n",
    "}')\n",
    "    )\n",
    "\n",
    "    # Prepare output paths\n",
    "    pathlib.Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    run_tag = f'{model_name.replace('/', '_')}_{timestamp()}'\n",
    "    json_path = pathlib.Path(RESULTS_DIR) / f'notebook_run_{run_tag}.json'\n",
    "    md_path = pathlib.Path(RESULTS_DIR) / f'notebook_run_{run_tag}.md'\n",
    "\n",
    "    # Build metadata + payload\n",
    "    config = EvaluatorConfig(\n",
    "        prompts_path=prompt_path,\n",
    "        output_path=json_path,\n",
    "        save_markdown=True,\n",
    "        filter=f,\n",
    "        retries=2,\n",
    "        request_timeout=60.0,\n",
    "        dry_run=DRY_RUN,\n",
    "    )\n",
    "    metadata = build_metadata(config, settings, len(cases), len(selected), BACKEND)\n",
    "    payload = build_run_payload(records, metadata=metadata)\n",
    "    write_json(json_path, payload)\n",
    "    md_path.write_text(render_markdown(records, model_name, prompt_path.name), encoding='utf-8')\n",
    "    summary = console_summary(records)\n",
    "    print(summary)\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'json': str(json_path),\n",
    "        'markdown': str(md_path),\n",
    "        'summary': summary,\n",
    "    }\n",
    "\n",
    "print('Helper functions ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583f980",
   "metadata": {},
   "source": [
    "## 4. Run Evaluations\n",
    "Executes evaluations for each model in `MODEL_NAMES`. Skip or modify the list for single-model runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for m in MODEL_NAMES:\n",
    "    try:\n",
    "        result = evaluate_model(m)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f'Error evaluating {m}: {e}')\n",
    "\n",
    "print('Completed evaluations.')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af92f5",
   "metadata": {},
   "source": [
    "## 5. (Optional) Simple Comparison View\n",
    "Creates a lightweight comparison dictionary. For deeper analysis you can load JSON payloads into pandas or another tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {}\n",
    "for entry in results:\n",
    "    # Extract pass rate from summary string (simple parse)\n",
    "    lines = entry['summary'].splitlines()\n",
    "    pass_line = next((l for l in lines if 'Pass rate' in l), None)\n",
    "    comparison[entry['model']] = {\n",
    "        'json': entry['json'],\n",
    "        'markdown': entry['markdown'],\n",
    "        'pass_line': pass_line,\n",
    "    }\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec0869",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "- Review Markdown files in `Evaluator/results/`.\n",
    "- Open a PR with those files and a short note about qualitative behavior.\n",
    "- Suggest additional prompt tags or new prompt cases if you noticed blind spots.\n",
    "\n",
    "Thank you for helping improve model selection for the Obsidian Nexus plugin!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
