{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/ProfSynapse/Toolset-Training/blob/main/kto_tool_calling_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n# KTO Training for Tool Calling - Claudesidian Vault Tools\n\nThis notebook trains a language model using KTO (Kahneman-Tversky Optimization) to internalize tool calling for the Claudesidian vault application.\n\n**Dataset**: syngen_tools_11.14.25.jsonl (4,652 examples)\n- Desirable examples: Correct tool usage with proper parameters\n- Undesirable examples: Incorrect tool usage (wrong params, missing required fields)\n\n**Goal**: Train the model to recognize and use the correct tools with correct parameters for vault operations."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Fast installation using --no-deps to avoid dependency resolution delays (2-3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fast installation for Colab - uses --no-deps to avoid dependency resolution delays\nprint(\"Installing packages (this may take 2-3 minutes)...\")\nprint(\"=\" * 60)\n\n# Step 1: Install PyTorch 2.4.1 with CUDA 12.1\nprint(\"\\n[1/10] Installing PyTorch 2.4.1 + CUDA 12.1...\")\n!pip install -q torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\nprint(\"‚úì PyTorch 2.4.1 installed\")\n\n# Step 2: Install core dependencies without resolving conflicts (--no-deps)\nprint(\"\\n[2/10] Installing core dependencies...\")\n!pip install --no-deps bitsandbytes accelerate peft triton cut_cross_entropy unsloth_zoo\nprint(\"‚úì Core dependencies installed\")\n\n# Step 3: Install supporting libraries with version constraints\nprint(\"\\n[3/10] Installing supporting libraries...\")\n!pip install sentencepiece protobuf \"datasets>=2.14.0,<4.0.0\" \"huggingface_hub>=0.20.0\"\nprint(\"‚úì Supporting libraries installed\")\n\n# Step 4: Install specific versions of transformers and trl\nprint(\"\\n[4/10] Installing transformers and trl...\")\n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\nprint(\"‚úì Transformers and TRL installed\")\n\n# Step 5: Install tyro and msgspec (required by unsloth)\nprint(\"\\n[5/10] Installing tyro and msgspec...\")\n!pip install tyro msgspec\nprint(\"‚úì Tyro and msgspec installed\")\n\n# Step 6: Install xformers (required by unsloth for fast attention)\nprint(\"\\n[6/10] Installing xformers...\")\n!pip install --no-deps xformers\nprint(\"‚úì xformers installed\")\n\n# Step 7: Install unsloth without dependencies\nprint(\"\\n[7/10] Installing unsloth...\")\n!pip install --no-deps unsloth\nprint(\"‚úì Unsloth installed\")\n\n# Step 8: Ensure numpy compatibility\nprint(\"\\n[8/10] Ensuring numpy compatibility...\")\n!pip install \"numpy>=1.24.0,<2.0\"\nprint(\"‚úì NumPy configured\")\n\n# Step 9: Force PyTorch back to 2.4.1 (in case xformers upgraded it)\nprint(\"\\n[9/10] Re-confirming PyTorch 2.4.1 version...\")\n!pip install -q --force-reinstall torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\nimport torch\nprint(f\"‚úì PyTorch version locked at: {torch.__version__}\")\n\n# Step 10: Install Flash Attention if GPU supports it\nprint(\"\\n[10/10] Installing Flash Attention (if GPU supports it)...\")\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install ninja packaging\n    !pip install \"flash-attn>=2.5.0\" --no-build-isolation\n    print(\"‚úì Flash Attention installed\")\nelse:\n    print(\"‚ö† GPU doesn't support Flash Attention 2 (skipping)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úì INSTALLATION COMPLETE!\")\nprint(\"=\" * 60)\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Restart the runtime now (Runtime ‚Üí Restart runtime)\")\nprint(\"Then re-run this cell - it will be much faster the second time!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nprint(\"\\nImporting libraries...\")\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nimport torch\nimport os\nimport json\nfrom datasets import Dataset\nfrom trl import KTOConfig, KTOTrainer\n\nprint(\"\\n‚úì All imports successful!\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Model Loading\nLoad a pre-trained model suitable for tool calling tasks",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Model configuration\nmax_seq_length = 4096\ndtype = None  # Auto-detect: Float16 for older GPUs, Bfloat16 for Ampere+\nload_in_4bit = True  # Use 4-bit quantization for memory efficiency\n\n# Load model and tokenizer\n# Options:\n# - \"unsloth/Qwen2.5-Coder-1.5B-Instruct\" (small, fast)\n# - \"unsloth/Qwen2.5-7B-Instruct\" (medium)\n# - \"unsloth/Llama-3.2-3B-Instruct\" (small, good for tool calling)\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Qwen2.5-Coder-1.5B-Instruct\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nprint(f\"‚úì Model loaded: {model.config.model_type}\")\nprint(f\"‚úì Tokenizer vocab size: {len(tokenizer)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading and Processing\n",
    "Load the tool calling dataset and convert to KTO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from HuggingFace\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"professorsynapse/claudesidian-synthetic-dataset\",\n",
    "    data_files=\"syngen_tools_11.14.25.jsonl\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataset loaded: {len(raw_dataset['train'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ChatML format to KTO format\n",
    "def convert_to_kto_format(example):\n",
    "    \"\"\"\n",
    "    Convert tool calling conversations to KTO format.\n",
    "    \n",
    "    Input format:\n",
    "    {\n",
    "      \"conversations\": [\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"tool_call: ...\\narguments: ...\\n\\nResult: ...\\n\\n...\"}\n",
    "      ],\n",
    "      \"label\": true/false\n",
    "    }\n",
    "    \n",
    "    Output format:\n",
    "    {\n",
    "      \"prompt\": \"user message\",\n",
    "      \"completion\": \"assistant tool call and response\",\n",
    "      \"label\": true/false\n",
    "    }\n",
    "    \"\"\"\n",
    "    conversations = example[\"conversations\"]\n",
    "    \n",
    "    # Extract user and assistant messages\n",
    "    user_msg = None\n",
    "    assistant_msg = None\n",
    "    \n",
    "    for msg in conversations:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            user_msg = msg[\"content\"]\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            assistant_msg = msg[\"content\"]\n",
    "    \n",
    "    if user_msg is None or assistant_msg is None:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": user_msg,\n",
    "        \"completion\": assistant_msg,\n",
    "        \"label\": example[\"label\"]\n",
    "    }\n",
    "\n",
    "# Process the dataset\n",
    "kto_data = []\n",
    "for example in raw_dataset[\"train\"]:\n",
    "    processed = convert_to_kto_format(example)\n",
    "    if processed:\n",
    "        kto_data.append(processed)\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [ex[\"prompt\"] for ex in kto_data],\n",
    "    \"completion\": [ex[\"completion\"] for ex in kto_data],\n",
    "    \"label\": [ex[\"label\"] for ex in kto_data],\n",
    "})\n",
    "\n",
    "# Show statistics\n",
    "desirable = sum(train_dataset[\"label\"])\n",
    "undesirable = len(train_dataset) - desirable\n",
    "\n",
    "print(f\"\\n‚úì KTO Dataset prepared:\")\n",
    "print(f\"  Total examples: {len(train_dataset)}\")\n",
    "print(f\"  Desirable (correct tool use): {desirable}\")\n",
    "print(f\"  Undesirable (incorrect tool use): {undesirable}\")\n",
    "print(f\"  Ratio: {desirable/undesirable:.2f}:1\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nüìù Example (desirable):\")\n",
    "desirable_ex = [ex for ex in kto_data if ex[\"label\"]][0]\n",
    "print(f\"Prompt: {desirable_ex['prompt'][:100]}...\")\n",
    "print(f\"Completion: {desirable_ex['completion'][:150]}...\")\n",
    "\n",
    "print(f\"\\nüìù Example (undesirable):\")\n",
    "undesirable_ex = [ex for ex in kto_data if not ex[\"label\"]][0]\n",
    "print(f\"Prompt: {undesirable_ex['prompt'][:100]}...\")\n",
    "print(f\"Completion: {undesirable_ex['completion'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "Configure LoRA adapters for efficient fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank (higher = more parameters)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=128,  # LoRA scaling factor\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"‚úì LoRA adapters configured\")\n",
    "print(f\"  Rank: 64\")\n",
    "print(f\"  Alpha: 128\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTO Training Configuration\n",
    "Set up KTO trainer to learn correct vs incorrect tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KTO Training Arguments\n",
    "training_args = KTOConfig(\n",
    "    output_dir=\"./kto_claudesidian_tools\",\n",
    "    \n",
    "    # Batch size configuration\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 32\n",
    "    \n",
    "    # KTO-specific parameters\n",
    "    beta=0.1,  # KTO beta (controls strength of preference optimization)\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=5e-6,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Sequence lengths\n",
    "    max_length=4096,\n",
    "    max_prompt_length=2048,\n",
    "    \n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_8bit\",\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=250,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",  # Change to \"wandb\" for experiment tracking\n",
    ")\n",
    "\n",
    "# Initialize KTO Trainer\n",
    "kto_trainer = KTOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úì KTO trainer initialized\")\n",
    "print(f\"  Dataset: {len(train_dataset)} examples\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  KTO beta: {training_args.beta}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "Train the model to internalize Claudesidian vault tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show memory stats before training\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU: {gpu_stats.name}\")\n",
    "    print(f\"Max memory: {max_memory} GB\")\n",
    "    print(f\"Memory reserved: {start_gpu_memory} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting KTO training for tool calling...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    trainer_output = kto_trainer.train()\n",
    "    print(\"\\n‚úì Training completed successfully!\")\n",
    "    print(f\"Final loss: {trainer_output.training_loss:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Training failed: {type(e).__name__}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Save the trained model and adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(\"claudesidian_tool_lora\")\n",
    "tokenizer.save_pretrained(\"claudesidian_tool_lora\")\n",
    "\n",
    "print(\"‚úì Model saved to ./claudesidian_tool_lora\")\n",
    "\n",
    "# Optional: Upload to HuggingFace Hub\n",
    "# Uncomment and configure:\n",
    "# HF_USERNAME = \"your_username\"\n",
    "# MODEL_NAME = \"claudesidian-tool-calling-qwen-1.5b\"\n",
    "# HF_TOKEN = \"hf_...\"\n",
    "# \n",
    "# model.push_to_hub_merged(\n",
    "#     f\"{HF_USERNAME}/{MODEL_NAME}\",\n",
    "#     tokenizer,\n",
    "#     save_method=\"merged_16bit\",\n",
    "#     token=HF_TOKEN\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing\n",
    "Test the trained model with tool calling examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Set up for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    ")\n",
    "\n",
    "def test_tool_calling(user_message):\n",
    "    \"\"\"Generate tool call for a user request.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USER REQUEST:\")\n",
    "    print(\"=\"*60)\n",
    "    print(user_message)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"MODEL RESPONSE:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    text_streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        streamer=text_streamer,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"‚úì Inference setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases covering different Claudesidian tools\n",
    "test_cases = [\n",
    "    # Content reading\n",
    "    \"Show me the contents of my project roadmap file.\",\n",
    "    \n",
    "    # Content modification\n",
    "    \"Add a header to my meeting notes saying 'Q1 2025 Planning'.\",\n",
    "    \n",
    "    # File operations\n",
    "    \"Delete the old draft file called 'temp-notes.md'.\",\n",
    "    \n",
    "    # Workspace operations\n",
    "    \"Switch to my 'Personal' workspace.\",\n",
    "    \n",
    "    # Agent operations\n",
    "    \"Turn on my Research Assistant agent.\",\n",
    "    \n",
    "    # Search operations\n",
    "    \"Find all notes that mention 'product launch'.\",\n",
    "    \n",
    "    # Folder operations\n",
    "    \"Create a new folder called 'Archive-2024'.\",\n",
    "]\n",
    "\n",
    "print(\"Testing tool calling with trained model...\\n\")\n",
    "for test_case in test_cases:\n",
    "    test_tool_calling(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trained a model using KTO to internalize Claudesidian vault tools:\n",
    "\n",
    "**Tools covered:**\n",
    "- `contentManager_readContent` - Read file contents\n",
    "- `contentManager_prependContent` - Add content to file start\n",
    "- `contentManager_appendContent` - Add content to file end\n",
    "- `vaultManager_deleteNote` - Delete files\n",
    "- `workspaceManager_switchWorkspace` - Switch workspaces\n",
    "- `agentManager_toggleAgent` - Enable/disable agents\n",
    "- `searchManager_search` - Search for notes\n",
    "- `folderManager_createFolder` - Create folders\n",
    "\n",
    "**Training approach:**\n",
    "- KTO learns from desirable (correct) vs undesirable (incorrect) tool usage\n",
    "- Model learns to use correct parameter names (e.g., `filePath` not `file`)\n",
    "- Model learns to include all required parameters\n",
    "- Model learns when to use which tool\n",
    "\n",
    "**Next steps:**\n",
    "1. Test the model with your actual Claudesidian application\n",
    "2. Collect more examples of edge cases\n",
    "3. Iterate and retrain for better performance\n",
    "4. Consider larger models (7B, 14B) for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}